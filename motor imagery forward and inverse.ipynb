{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15aa7db",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from mne.datasets import sample\n",
    "from mne.minimum_norm import make_inverse_operator, apply_inverse_epochs, apply_inverse\n",
    "from mne.datasets import fetch_fsaverage\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial import Delaunay\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D, Dense, Flatten, Concatenate, BatchNormalization, Dropout, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib qt\n",
    "\n",
    "DIRECTORY_PATH = os.getcwd()\n",
    "EXTERNAL_STORAGE_PATH = \"D:\\Motor Imagery\"\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257a3fa",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45005041",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_mapping = {\n",
    "    \"EEG-Fz\": \"Fz\",\n",
    "    \"EEG-0\": \"FC3\",\n",
    "    \"EEG-1\": \"FC1\",\n",
    "    \"EEG-2\": \"FCz\",\n",
    "    \"EEG-3\": \"FC2\",\n",
    "    \"EEG-4\": \"FC4\",\n",
    "    \"EEG-5\": \"C5\",\n",
    "    \"EEG-C3\": \"C3\", \n",
    "    \"EEG-6\": \"C1\",\n",
    "    \"EEG-Cz\": \"Cz\",\n",
    "    \"EEG-7\": \"C2\",\n",
    "    \"EEG-C4\": \"C4\",\n",
    "    \"EEG-8\": \"C6\",\n",
    "    \"EEG-9\": \"CP3\",\n",
    "    \"EEG-10\": \"CP1\",\n",
    "    \"EEG-11\": \"CPz\",\n",
    "    \"EEG-12\": \"CP2\",\n",
    "    \"EEG-13\": \"CP4\",\n",
    "    \"EEG-14\": \"P1\",\n",
    "    \"EEG-Pz\": \"Pz\",\n",
    "    \"EEG-15\": \"P2\",\n",
    "    \"EEG-16\": \"POz\",\n",
    "    \"EOG-left\": \"EOG-left\",\n",
    "    \"EOG-central\": \"EOG-central\",\n",
    "    \"EOG-right\": \"EOG-right\"\n",
    "}\n",
    "\n",
    "channels_type_mapping = {\n",
    "    \"Fz\": \"eeg\",\n",
    "    \"FC3\": \"eeg\",\n",
    "    \"FC1\": \"eeg\",\n",
    "    \"FCz\": \"eeg\",\n",
    "    \"FC2\": \"eeg\",\n",
    "    \"FC4\": \"eeg\",\n",
    "    \"C5\": \"eeg\",\n",
    "    \"C3\": \"eeg\", \n",
    "    \"C1\": \"eeg\",\n",
    "    \"Cz\": \"eeg\",\n",
    "    \"C2\": \"eeg\",\n",
    "    \"C4\": \"eeg\",\n",
    "    \"C6\": \"eeg\",\n",
    "    \"CP3\": \"eeg\",\n",
    "    \"CP1\": \"eeg\",\n",
    "    \"CPz\": \"eeg\",\n",
    "    \"CP2\": \"eeg\",\n",
    "    \"CP4\": \"eeg\",\n",
    "    \"P1\": \"eeg\",\n",
    "    \"Pz\": \"eeg\",\n",
    "    \"P2\": \"eeg\",\n",
    "    \"POz\": \"eeg\",\n",
    "    \"EOG-left\": \"eog\",\n",
    "    \"EOG-central\": \"eog\",\n",
    "    \"EOG-right\": \"eog\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(\"/Users/ivanl/Downloads/MRIcron_windows/MRIcron/Resources/templates/brodmann.nii.gz\")\n",
    "\n",
    "brodmann_data = img.get_fdata()\n",
    "# Areas 3, 1 and 2 – Primary somatosensory cortex in the postcentral gyrus (frequently referred to as Areas 3, 1, 2 by convention)\n",
    "# Area 4– Primary motor cortex\n",
    "# Area 5 – Superior parietal lobule\n",
    "# Area 6 – Premotor cortex and Supplementary Motor Cortex (Secondary Motor Cortex) (Supplementary motor area)\n",
    "# Area 7 – Visuo-Motor Coordination\n",
    "brodmann_motor = None\n",
    "selected_area = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for area in selected_area:\n",
    "    if brodmann_motor is None:\n",
    "        brodmann_motor = brodmann_data.reshape(-1) == area\n",
    "    else:\n",
    "        brodmann_motor += brodmann_data.reshape(-1) == area\n",
    "#brodmann_motor = brodmann_data.reshape(-1) == 4\n",
    "print(brodmann_motor)\n",
    "print(\"brodmann template shape: \" + str(brodmann_data.shape))\n",
    "print(\"chosen points: \" + str(np.sum(brodmann_motor)))\n",
    "\n",
    "shape, affine = img.shape[:3], img.affine\n",
    "coords = np.array(np.meshgrid(*(range(i) for i in shape), indexing='ij'))\n",
    "coords = np.rollaxis(coords, 0, len(shape) + 1)\n",
    "mm_coords = nib.affines.apply_affine(affine, coords)\n",
    "\n",
    "def in_hull(p, hull):\n",
    "    \"\"\"\n",
    "    Test if points in `p` are in `hull`\n",
    "\n",
    "    `p` should be a `NxK` coordinates of `N` points in `K` dimensions\n",
    "    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the \n",
    "    coordinates of `M` points in `K`dimensions for which Delaunay triangulation\n",
    "    will be computed\n",
    "    \"\"\"\n",
    "    if not isinstance(hull,Delaunay):\n",
    "        hull = Delaunay(hull)\n",
    "\n",
    "    return hull.find_simplex(p)>=0\n",
    "\n",
    "my_left_points = None\n",
    "my_right_points = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "labels utility function\n",
    "\"\"\"\n",
    "def load_subject_labels(name=\"A01E.mat\", dir=\"drive/Shareddrives/Motor Imagery/BCI competition IV dataset/2a/2a true_labels/\"):\n",
    "  data = scipy.io.loadmat(dir + name)[\"classlabel\"].reshape(-1)\n",
    "  return data\n",
    "\n",
    "def load_all_true_labels(dataset_path):\n",
    "  data = {}\n",
    "  for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "      data[file] = load_subject_labels(name=file, dir=root) \n",
    "  return data\n",
    "\n",
    "\"\"\"\n",
    "plot graph utility function\n",
    "\"\"\"\n",
    "def plot_average_graph(subject_name=\"A01T.gdf\", Class=\"left\", filter_channels=None):\n",
    "  average = {\"left\": None, \"right\": None, \"foot\": None, \"tongue\": None, \"unknown\": None}\n",
    "  for event_class, event_data in data[subject_name][\"epoch_data\"].items():\n",
    "    if event_data != []:\n",
    "      average[event_class] = np.transpose(np.mean(event_data, axis=0))\n",
    "\n",
    "  x = average[Class]\n",
    "  if filter_channels is None:\n",
    "    fig, axs = plt.subplots(x.shape[1], gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 21)\n",
    "    for channel in range(x.shape[1]):\n",
    "      axs[channel].title.set_text(ch_names[channel])\n",
    "      axs[channel].title.set_size(20)\n",
    "      axs[channel].title.set_y(0.7)\n",
    "      axs[channel].plot(range(x.shape[0]), x[:, channel])\n",
    "      axs[channel].axvline(x=250, color=\"r\", linestyle='--')\n",
    "      #axs[channel].axvline(x=875, color=\"r\", linestyle='--')\n",
    "  else :\n",
    "    fig, axs = plt.subplots(len(filter_channels), gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 10.5)\n",
    "    for i in range(len(filter_channels)):\n",
    "      for channel in range(x.shape[1]):\n",
    "        if(filter_channels[i] == ch_names[channel]):\n",
    "          axs[i].title.set_text(ch_names[channel])\n",
    "          axs[i].title.set_size(20)\n",
    "          axs[i].title.set_y(0.7)\n",
    "          axs[i].plot(range(x.shape[0]), x[:, channel])\n",
    "          axs[i].axvline(x=250, color=\"r\", linestyle='--')\n",
    "          #axs[i].axvline(x=875, color=\"r\", linestyle='--')\n",
    "          break\n",
    "  plt.tight_layout()\n",
    "\n",
    "def plot_multiple_graph(subject_name=\"A02T.gdf\", classes=[\"left\", \"right\", \"foot\", \"tongue\"], filter_channels=None):\n",
    "  average = {\"left\": None, \"right\": None, \"foot\": None, \"tongue\": None, \"unknown\": None}\n",
    "  for event_class, event_data in data[subject_name][\"epoch_data\"].items():\n",
    "    if event_data != []:\n",
    "      average[event_class] = np.transpose(np.mean(event_data, axis=0))\n",
    "\n",
    "  color = {\"left\": \"b\", \"right\": \"g\", \"foot\": \"c\", \"tongue\": \"m\", \"tongue\": \"y\"}\n",
    "  x = []\n",
    "  for Class in classes:\n",
    "    x.append(average[Class])\n",
    "\n",
    "  if filter_channels is None:\n",
    "    fig, axs = plt.subplots(x[0].shape[1], gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 21)\n",
    "    for channel in range(x[0].shape[1]):\n",
    "      axs[channel].title.set_text(ch_names[channel])\n",
    "      axs[channel].title.set_size(20)\n",
    "      axs[channel].title.set_y(0.7)\n",
    "      axs[channel].axvline(x=250, color=\"r\", linestyle='--')\n",
    "      #axs[channel].axvline(x=875, color=\"r\", linestyle='--')\n",
    "      for i in range(len(classes)):\n",
    "        axs[channel].plot(range(x[i].shape[0]), x[i][:, channel], color=color[classes[i]])\n",
    "  else:\n",
    "    fig, axs = plt.subplots(len(filter_channels), gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 10.5)\n",
    "    for i in range(len(filter_channels)):\n",
    "      for channel in range(x[0].shape[1]):\n",
    "        if(filter_channels[i] == ch_names[channel]):\n",
    "          axs[i].title.set_text(ch_names[channel])\n",
    "          axs[i].title.set_size(20)\n",
    "          axs[i].title.set_y(0.7)\n",
    "          axs[i].axvline(x=250, color=\"r\", linestyle='--')\n",
    "          #axs[i].axvline(x=875, color=\"r\", linestyle='--')\n",
    "          for j in range(len(classes)):\n",
    "            axs[i].plot(range(x[j].shape[0]), x[j][:, channel], color=color[classes[j]])\n",
    "          break\n",
    "  plt.tight_layout()\n",
    "\n",
    "\"\"\"\n",
    "load data function\n",
    "\"\"\"\n",
    "def load_subject(name=\"A01T.gdf\", dir='drive/Shareddrives/Motor Imagery/BCI competition IV dataset/2a/BCICIV_2a_gdf/', debug=None):\n",
    "  subject_data = {}\n",
    "  # Load data\n",
    "  raw = mne.io.read_raw_gdf(dir + name)\n",
    "  # Rename channels\n",
    "  raw.rename_channels(channels_mapping)\n",
    "  # Set channels types\n",
    "  raw.set_channel_types(channels_type_mapping)\n",
    "  # Set montage\n",
    "  # Read and set the EEG electrode locations\n",
    "  ten_twenty_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "  raw.set_montage(ten_twenty_montage)\n",
    "  # Set common average reference\n",
    "  raw.set_eeg_reference('average', projection=True, verbose=False)\n",
    "  # Drop eog channels\n",
    "  raw.drop_channels([\"EOG-left\", \"EOG-central\", \"EOG-right\"])\n",
    "\n",
    "  subject_data[\"raw\"] = raw\n",
    "  subject_data[\"info\"] = raw.info\n",
    "  if debug == \"all\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for key, item in raw.info.items():\n",
    "      print(key, item)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "  \n",
    "  \"\"\"\n",
    "  '276': 'Idling EEG (eyes open)'\n",
    "  '277': 'Idling EEG (eyes closed)'\n",
    "  '768': 'Start of a trial'\n",
    "  '769': 'Cue onset left (class 1)'\n",
    "  '770': 'Cue onset right (class 2)'\n",
    "  '771': 'Cue onset foot (class 3)'\n",
    "  '772': 'Cue onset tongue (class 4)'\n",
    "  '783': 'Cue unknown'\n",
    "  '1023': 'Rejected trial'\n",
    "  '1072': 'Eye movements'\n",
    "  '32766': 'Start of a new run'\n",
    "  \"\"\"\n",
    "  custom_mapping = {'276': 276, '277': 277, '768': 768, '769': 769, '770': 770, '771': 771, '772': 772, '783': 783, '1023': 1023, '1072': 1072, '32766': 32766}\n",
    "  events_from_annot, event_dict = mne.events_from_annotations(raw, event_id=custom_mapping)\n",
    "\n",
    "  if debug == \" all\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(event_dict)\n",
    "    print(events_from_annot)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for i in range(len(raw.annotations)):\n",
    "      print(events_from_annot[i], raw.annotations[i])  \n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  class_info = \"Idling EEG (eyes open): \" + str(len(events_from_annot[events_from_annot[:, 2]==276][:, 0])) + \"\\n\" + \\\n",
    "               \"Idling EEG (eyes closed): \" + str(len(events_from_annot[events_from_annot[:, 2]==277][:, 0])) + \"\\n\" + \\\n",
    "               \"Start of a trial: \" + str(len(events_from_annot[events_from_annot[:, 2]==768][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset left (class 1): \" + str(len(events_from_annot[events_from_annot[:, 2]==769][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset right (class 2): \" + str(len(events_from_annot[events_from_annot[:, 2]==770][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset foot (class 3): \" + str(len(events_from_annot[events_from_annot[:, 2]==771][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset tongue (class 4): \" + str(len(events_from_annot[events_from_annot[:, 2]==772][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue unknown: \" + str(len(events_from_annot[events_from_annot[:, 2]==783][:, 0])) + \"\\n\" + \\\n",
    "               \"Rejected trial: \" + str(len(events_from_annot[events_from_annot[:, 2]==1023][:, 0])) + \"\\n\" + \\\n",
    "               \"Eye movements: \" + str(len(events_from_annot[events_from_annot[:, 2]==1072][:, 0])) + \"\\n\" + \\\n",
    "               \"Start of a new run: \" + str(len(events_from_annot[events_from_annot[:, 2]==32766][:, 0]))\n",
    "  subject_data[\"class_info\"] = class_info\n",
    "\n",
    "  if debug == \"all\" or debug == \"important\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(class_info)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  epoch_data = {\"left\": [], \"right\": [], \"foot\": [], \"tongue\": [], \"unknown\": []}\n",
    "  rejected_trial = events_from_annot[events_from_annot[:, 2]==1023][:, 0]\n",
    "  class_dict = {\"left\": 769, \"right\": 770, \"foot\": 771, \"tongue\": 772, \"unknown\": 783}\n",
    "  raw_data = raw.get_data()  #(22, 672528)\n",
    "  start = 10                 # cue+0.1s\n",
    "  stop = 510                 # cue+2.1s\n",
    "\n",
    "  for event_class, event_id in class_dict.items():\n",
    "    current_event = events_from_annot[events_from_annot[:, 2]==event_id][:, 0]\n",
    "    if event_class == \"unknown\":\n",
    "      subject_true_labels = true_labels[name[:4]+\".mat\"]\n",
    "      class_dict_labels = {1: \"left\", 2: \"right\", 3: \"foot\", 4: \"tongue\"}\n",
    "      for i in range(len(current_event)):\n",
    "        # exclude artifact\n",
    "        if (current_event[i] - 500 != rejected_trial).all():\n",
    "          current_event_data = np.expand_dims(np.array(raw_data[:22, current_event[i]+start:current_event[i]+stop]), axis=0)\n",
    "          if (epoch_data.get(class_dict_labels[subject_true_labels[i]]) == None).all():\n",
    "            epoch_data[class_dict_labels[subject_true_labels[i]]] = current_event_data\n",
    "          else:\n",
    "            epoch_data[class_dict_labels[subject_true_labels[i]]] = np.append(epoch_data[class_dict_labels[subject_true_labels[i]]], current_event_data, axis=0)\n",
    "    else:\n",
    "      for i in range(len(current_event)):\n",
    "        # exclude artifact\n",
    "        if((current_event[i] - 500 != rejected_trial).all()):\n",
    "          epoch_data[event_class].append(np.array(raw_data[:22, current_event[i]+start:current_event[i]+stop]))\n",
    "      epoch_data[event_class] = np.array(epoch_data[event_class])\n",
    "\n",
    "  if debug == \"all\" or debug == \"important\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for key, data in epoch_data.items():\n",
    "      print(key, len(data))\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  for event_class, event_data in epoch_data.items():\n",
    "    epoch_data[event_class] = np.array(event_data)\n",
    "\n",
    "  subject_data[\"epoch_data\"] = epoch_data\n",
    "    \n",
    "\n",
    "  return subject_data\n",
    "\n",
    "def load_all_subject(dataset_path):\n",
    "  data = {}\n",
    "  for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "      data[file] = load_subject(name=file, dir=root) \n",
    "  return data\n",
    "\n",
    "\"\"\"\n",
    "create mne epochs data structure from numpy array\n",
    "merge training and evaluation data\n",
    "\"\"\"\n",
    "def create_epochs(data):\n",
    "  subjects_data = {}\n",
    "\n",
    "  for subject in data.keys():\n",
    "    if \"E\" in subject:\n",
    "        continue\n",
    "    epochs_data = {}\n",
    "    for event in data[subject][\"epoch_data\"].keys():\n",
    "      current_event_data = None\n",
    "      \n",
    "      if data[subject][\"epoch_data\"][event].any():\n",
    "        current_event_data = data[subject][\"epoch_data\"][event]\n",
    "      if data[subject[:3]+\"E.gdf\"][\"epoch_data\"][event].any():\n",
    "        current_event_data = np.append(current_event_data, data[subject[:3]+\"E.gdf\"][\"epoch_data\"][event], axis=0)\n",
    "      if current_event_data is not None:\n",
    "          epochs_data[event] = mne.EpochsArray(current_event_data, data[subject][\"info\"], verbose=False)\n",
    "\n",
    "    subjects_data[subject[:3]] = epochs_data\n",
    "\n",
    "  return subjects_data\n",
    "\n",
    "\"\"\"\n",
    "Create source activity and reconstructed eeg respectively for each subject\n",
    "\n",
    "For each subject, there are four events in total, i.e. left, right, foot, tongue\n",
    "Split these data into train and test set using kfold\n",
    "Compute the noise covariance matrix on train set and apply it to test set\n",
    "Create source activity (only motor region) first by applying an inverse operator to the epochs \n",
    "Create reconstructed eeg by applying a forward operator to the source activity acquired earlier\n",
    "Save both these files to disk\n",
    "\"\"\"\n",
    "def apply_inverse_and_forward_kfold(epochs, n_splits=5, save_inverse=True, save_forward=True, events=[\"left\", \"right\"], subjects=None):\n",
    "    global my_left_points, my_right_points\n",
    "    \n",
    "    if subjects is None:\n",
    "        subjects = epochs.keys()\n",
    "        \n",
    "    for subject in subjects:  \n",
    "        print(subject)\n",
    "        X, Y = [], []\n",
    "        info = None\n",
    "        counter = 0\n",
    "        for event in epochs[subject].keys():\n",
    "            if info is None:\n",
    "                info = epochs[subject][event].info\n",
    "            for i in range(len(events)):\n",
    "                if event == events[i]:\n",
    "                    print(event)\n",
    "                    if len(X) == 0:\n",
    "                        X = epochs[subject][event].get_data()\n",
    "                        Y = np.zeros(len(epochs[subject][event].get_data())) + i\n",
    "                    else:\n",
    "                        X = np.append(X, epochs[subject][event].get_data(), axis=0)\n",
    "                        Y = np.append(Y, np.zeros(len(epochs[subject][event].get_data())) + i, axis=0)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "        for train_index, test_index in skf.split(X, Y):\n",
    "            counter += 1\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            X_train = mne.EpochsArray(X_train, info, verbose=False)\n",
    "            X_test = mne.EpochsArray(X_test, info, verbose=False)\n",
    "            \n",
    "            noise_cov = mne.compute_covariance(X_train, tmax=0., method=['shrunk', 'empirical'], rank=None, verbose=False)\n",
    "            fwd = mne.make_forward_solution(info, trans=trans, src=src,\n",
    "                            bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=1, verbose=False)\n",
    "            fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "                                         use_cps=True, verbose=False)\n",
    "            leadfield = fwd_fixed['sol']['data']\n",
    "            inverse_operator = make_inverse_operator(info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "            \n",
    "            method = \"sLORETA\"\n",
    "            snr = 3.\n",
    "            lambda2 = 1. / snr ** 2\n",
    "            stc_train = apply_inverse_epochs(X_train, inverse_operator, lambda2,\n",
    "                                          method=method, pick_ori=\"normal\", verbose=True)\n",
    "            \n",
    "            # get motor region points (once)\n",
    "            if my_left_points is None and my_right_points is None:\n",
    "                my_source = stc_train[0]\n",
    "                mni_lh = mne.vertex_to_mni(my_source.vertices[0], 0, mne_subject)\n",
    "                #print(mni_lh.shape)\n",
    "                mni_rh = mne.vertex_to_mni(my_source.vertices[1], 1, mne_subject)\n",
    "                #print(mni_rh.shape)\n",
    "\n",
    "                \"\"\"\n",
    "                fig = plt.figure(figsize=(8, 8))\n",
    "                ax = fig.add_subplot(projection='3d')\n",
    "                ax.scatter(mm_coords.reshape(-1, 3)[brodmann_motor][:, 0], mm_coords.reshape(-1, 3)[brodmann_motor][:, 1], mm_coords.reshape(-1, 3)[brodmann_motor][:, 2], s=15, marker='|')\n",
    "                ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='_')\n",
    "                ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "                ax.set_xlabel('X Label')\n",
    "                ax.set_ylabel('Y Label')\n",
    "                ax.set_zlabel('Z Label')\n",
    "                plt.show()\n",
    "                \"\"\"\n",
    "\n",
    "                my_left_points = in_hull(mni_lh, mm_coords.reshape(-1, 3)[brodmann_motor])\n",
    "                my_right_points = in_hull(mni_rh, mm_coords.reshape(-1, 3)[brodmann_motor])\n",
    "\n",
    "                mni_left_motor = mne.vertex_to_mni(my_source.vertices[0][my_left_points], 0, mne_subject)\n",
    "                #print(mni_left_motor.shape)\n",
    "                mni_right_motor = mne.vertex_to_mni(my_source.vertices[1][my_right_points], 1, mne_subject)\n",
    "                #print(mni_right_motor.shape)\n",
    "\n",
    "                \"\"\"\n",
    "                fig = plt.figure(figsize=(8, 8))\n",
    "                ax = fig.add_subplot(projection='3d')\n",
    "                ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='|')\n",
    "                ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "                ax.scatter(mni_left_motor[:, 0], mni_left_motor[:, 1], mni_left_motor[:, 2], s=15, marker='o')\n",
    "                ax.scatter(mni_right_motor[:, 0], mni_right_motor[:, 1], mni_right_motor[:, 2], s=15, marker='^')\n",
    "                ax.set_xlabel('X Label')\n",
    "                ax.set_ylabel('Y Label')\n",
    "                ax.set_zlabel('Z Label')\n",
    "                plt.show()\n",
    "                \"\"\"\n",
    "                \n",
    "            #print(\"Leadfield size : %d sensors x %d dipoles\" % leadfield.shape)\n",
    "            #print(stc_train[0].data.shape)\n",
    "            \n",
    "            # train set\n",
    "            # slice source activity data\n",
    "            left_hemi_data = []\n",
    "            right_hemi_data = []\n",
    "            for source in stc_train:\n",
    "                left_hemi_data.append(source.data[:len(source.vertices[0])][my_left_points])\n",
    "                right_hemi_data.append(source.data[-len(source.vertices[1]):][my_right_points])\n",
    "            left_hemi_data = np.array(left_hemi_data)\n",
    "            right_hemi_data = np.array(right_hemi_data)\n",
    "            if save_inverse:\n",
    "                source_activity_path = op.join(EXTERNAL_STORAGE_PATH, \"data\", \"source activity\", subject)\n",
    "                if not op.exists(source_activity_path):\n",
    "                    os.makedirs(source_activity_path)\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_train_X.npz\"), data=np.append(left_hemi_data, right_hemi_data, axis=1))\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_train_Y.npz\"), data=Y_train)\n",
    "            # slice reconstructed eeg data\n",
    "            reconstructed_eeg_data = []\n",
    "            for source in stc_train:\n",
    "                motor_source = np.zeros_like(source.data)\n",
    "                motor_source[:len(source.vertices[0])][my_left_points] = source.data[:len(source.vertices[0])][my_left_points]\n",
    "                motor_source[-len(source.vertices[1]):][my_right_points] = source.data[-len(source.vertices[1]):][my_right_points]\n",
    "                motor_eeg = np.dot(leadfield, motor_source)\n",
    "                reconstructed_eeg_data.append(motor_eeg)\n",
    "            if save_forward:\n",
    "                reconstructed_eeg_path = op.join(EXTERNAL_STORAGE_PATH, \"data\", \"reconstructed eeg\", subject)\n",
    "                if not op.exists(reconstructed_eeg_path):\n",
    "                    os.makedirs(reconstructed_eeg_path)\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_train_X.npz\"), data=np.array(reconstructed_eeg_data))\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_train_Y.npz\"), data=Y_train)\n",
    "            \n",
    "            del stc_train\n",
    "            gc.collect()\n",
    "            \n",
    "            stc_test = apply_inverse_epochs(X_test, inverse_operator, lambda2,\n",
    "                              method=method, pick_ori=\"normal\", verbose=True)\n",
    "            # test set\n",
    "            # slice source activity data\n",
    "            left_hemi_data = []\n",
    "            right_hemi_data = []\n",
    "            for source in stc_test:\n",
    "                left_hemi_data.append(source.data[:len(source.vertices[0])][my_left_points])\n",
    "                right_hemi_data.append(source.data[-len(source.vertices[1]):][my_right_points])\n",
    "            left_hemi_data = np.array(left_hemi_data)\n",
    "            right_hemi_data = np.array(right_hemi_data)\n",
    "            if save_inverse:\n",
    "                source_activity_path = op.join(EXTERNAL_STORAGE_PATH, \"data\", \"source activity\", subject)\n",
    "                if not op.exists(source_activity_path):\n",
    "                    os.makedirs(source_activity_path)\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_test_X.npz\"), data=np.append(left_hemi_data, right_hemi_data, axis=1))\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_test_Y.npz\"), data=Y_test)\n",
    "            # slice reconstructed eeg data\n",
    "            reconstructed_eeg_data = []\n",
    "            for source in stc_test:\n",
    "                motor_source = np.zeros_like(source.data)\n",
    "                motor_source[:len(source.vertices[0])][my_left_points] = source.data[:len(source.vertices[0])][my_left_points]\n",
    "                motor_source[-len(source.vertices[1]):][my_right_points] = source.data[-len(source.vertices[1]):][my_right_points]\n",
    "                motor_eeg = np.dot(leadfield, motor_source)\n",
    "                reconstructed_eeg_data.append(motor_eeg)\n",
    "            if save_forward:\n",
    "                reconstructed_eeg_path = op.join(EXTERNAL_STORAGE_PATH, \"data\", \"reconstructed eeg\", subject)\n",
    "                if not op.exists(reconstructed_eeg_path):\n",
    "                    os.makedirs(reconstructed_eeg_path)\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_test_X.npz\"), data=np.array(reconstructed_eeg_data))\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_test_Y.npz\"), data=Y_test)\n",
    "            \n",
    "            del X_train, X_test, Y_train, Y_test\n",
    "            del stc_test, reconstructed_eeg_data, left_hemi_data, right_hemi_data\n",
    "            gc.collect()\n",
    "\n",
    "\"\"\"\n",
    "Total params: 699,685\n",
    "Trainable params: 0\n",
    "Non-trainable params: 699,685\n",
    "\"\"\"\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\"),\n",
    "        Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\"),\n",
    "        Flatten(),\n",
    "        Dense(50, activation=\"selu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def stft_min_max(X, Y, debug=True):\n",
    "    Zxx = tf.signal.stft(X, frame_length=256, frame_step=16)\n",
    "    Zxx = tf.abs(Zxx)\n",
    "\n",
    "    if debug:\n",
    "      print(\"shape of X and Y: \" + str(X.shape) + \" \" + str(Y.shape))\n",
    "      print(\"shape of Zxx: \" + str(Zxx.shape))\n",
    "\n",
    "      # plot spectrogram\n",
    "      #samples = 0\n",
    "      #print(Y[samples])\n",
    "      #log_spec = tf.math.log(tf.transpose(Zxx[samples][0]))\n",
    "      #height = 40\n",
    "      #width = log_spec.shape[1]\n",
    "      #x_axis = tf.linspace(0, 2, num=width)\n",
    "      #y_axis = range(height)\n",
    "      #plt.pcolormesh(x_axis, y_axis, log_spec[:40, ])\n",
    "      #plt.title('STFT Magnitude')\n",
    "      #plt.ylabel('Frequency [Hz]')\n",
    "      #plt.xlabel('Time [sec]')\n",
    "      #plt.show()\n",
    "    \n",
    "    X = Zxx[:, :, :, :40]\n",
    "    X = tf.reshape(X, [X.shape[0], -1, 40])\n",
    "    X = tf.transpose(X, perm=[0, 2, 1])\n",
    "    X = tf.expand_dims(X, axis=3)\n",
    "    \n",
    "    # min max scaling (per instance)\n",
    "    original_shape = X.shape\n",
    "    X = tf.reshape(X, [original_shape[0], -1])\n",
    "    X_max = tf.math.reduce_max(X, axis=1, keepdims=True)\n",
    "    X_min = tf.math.reduce_min(X, axis=1, keepdims=True)\n",
    "    X = tf.math.divide(tf.math.subtract(X, X_min), tf.math.subtract(X_max, X_min))\n",
    "    X = tf.reshape(X, original_shape)\n",
    "    \n",
    "    if debug:\n",
    "      print(\"shape of X and Y: \" + str(X.shape) + \" \" + str(Y.shape))\n",
    "\n",
    "      # plot spectrogram\n",
    "      #samples = 0\n",
    "      #print(Y[samples])\n",
    "      #log_spec = tf.math.log(X[samples][:,:16,0])\n",
    "      #height = 40\n",
    "      #width = log_spec.shape[1]\n",
    "      #x_axis = tf.linspace(0, 2, num=width)\n",
    "      #y_axis = range(height)\n",
    "      #plt.pcolormesh(x_axis, y_axis, log_spec)\n",
    "      #plt.title('STFT Magnitude')\n",
    "      #plt.ylabel('Frequency [Hz]')\n",
    "      #plt.xlabel('Time [sec]')\n",
    "      #plt.show()\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd to google drive\n",
    "os.chdir(\"G:\")\n",
    "\n",
    "# Download fsaverage files\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = op.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "mne_subject = 'fsaverage'\n",
    "trans = 'fsaverage'  # MNE has a built-in fsaverage transformation\n",
    "src = op.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\n",
    "bem = op.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\n",
    "\n",
    "source = mne.read_source_spaces(src)\n",
    "left = source[0]\n",
    "right = source[1]\n",
    "left_pos = left[\"rr\"][left[\"inuse\"]==1]\n",
    "right_pos = right[\"rr\"][right[\"inuse\"]==1]\n",
    "                        \n",
    "transformation = mne.read_trans(op.join(fs_dir, \"bem\", \"fsaverage-trans.fif\"))\n",
    "\n",
    "save_path = op.join(os.getcwd(), \"Shared drives\", \"Motor Imagery\", \"Source Estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_path = \"Shared drives/Motor Imagery/BCI competition IV dataset/2a/2a true_labels/\"\n",
    "true_labels = load_all_true_labels(true_labels_path)\n",
    "\n",
    "dataset_path = 'Shared drives/Motor Imagery/BCI competition IV dataset/2a/BCICIV_2a_gdf/'\n",
    "data = load_all_subject(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c296e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some information to help to understand functions and data structure\n",
    "# for key, item in data.items():\n",
    "#   print(key)\n",
    "\n",
    "# ch_names = data[\"A01T.gdf\"][\"info\"][\"ch_names\"]\n",
    "# print(ch_names)\n",
    "\n",
    "# print(data[\"A01T.gdf\"][\"class_info\"])\n",
    "\n",
    "# for key, value in data.items():\n",
    "#   print(key)\n",
    "#   for event_class, event_data in value[\"epoch_data\"].items():\n",
    "#       print(event_class, len(event_data))\n",
    "#   print()\n",
    "\n",
    "# subject_name = \"A01T.gdf\"\n",
    "# Class = \"left\"\n",
    "# filter_channels = [\"C3\", \"Cz\", \"C4\"]\n",
    "# plot_average_graph(subject_name, Class, filter_channels)\n",
    "\n",
    "# subject_name = \"A02T.gdf\"\n",
    "# classes = [\"left\", \"right\"]\n",
    "# filter_channels = [\"C3\", \"Cz\", \"C4\"]\n",
    "# plot_multiple_graph(subject_name, classes, filter_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2e51d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = create_epochs(data)\n",
    "#apply_inverse_and_forward_kfold(epochs, n_splits=n_splits, subjects=[\"A05\"], events=['foot', 'tongue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f633036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some information to help to understand functions and data structure\n",
    "# my_epochs = epochs[\"A01\"][\"right\"]\n",
    "# my_evoked = my_epochs.average().pick(\"eeg\")\n",
    "\n",
    "# noise_cov = mne.compute_covariance(my_epochs, tmax=0., method=['shrunk', 'empirical'], rank=None, verbose=False)\n",
    "# fwd = mne.make_forward_solution(my_epochs.info, trans=trans, src=src,\n",
    "#                             bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=1)\n",
    "# # forward matrix\n",
    "# fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "#                                          use_cps=True)\n",
    "\n",
    "# inverse_operator = make_inverse_operator(\n",
    "#     my_epochs.info, fwd, noise_cov, loose=0.2, depth=0.8)\n",
    "\n",
    "# method = \"sLORETA\"\n",
    "# snr = 3.\n",
    "# lambda2 = 1. / snr ** 2\n",
    "# stc = mne.minimum_norm.apply_inverse(my_evoked, inverse_operator, lambda2,\n",
    "#                               method=method, pick_ori=\"normal\", verbose=True)\n",
    "\n",
    "# reconstruct_evoked = mne.apply_forward(fwd_fixed, stc, my_evoked.info)\n",
    "# my_evoked.plot_topomap()\n",
    "# reconstruct_evoked.plot_topomap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346696c",
   "metadata": {},
   "source": [
    "# CNN Classification (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "labels = {\"left\": 0, \"right\": 1}\n",
    "events = [\"left\", \"right\"]\n",
    "select_channels = [7, 9, 11]\n",
    "debug = True\n",
    "training = False\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A09\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "    \n",
    "  X, Y = [], []\n",
    "  for event in epochs[data_name].keys():\n",
    "    for i in range(len(events)):\n",
    "      if event == events[i]:\n",
    "        if len(X) == 0:\n",
    "          X = epochs[data_name][event].get_data()\n",
    "          Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "        else:\n",
    "          X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "          Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y)\n",
    "    \n",
    "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "  for train_index, test_index in skf.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # pick c3, cZ, c4 channels\n",
    "    X_train = X_train[:, select_channels, :]\n",
    "    X_test = X_test[:, select_channels, :]\n",
    "\n",
    "    print(data_name)\n",
    "    print(\"Training...\")\n",
    "    X_train, Y_train = stft_min_max(X_train, Y_train, debug)\n",
    "    print(\"Testing...\")\n",
    "    X_test, Y_test = stft_min_max(X_test, Y_test, debug)\n",
    "\n",
    "    if debug:\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    if training:\n",
    "      # create new model\n",
    "      model = create_model()\n",
    "\n",
    "      log_dir = DIRECTORY_PATH + \"/logs/\" + data_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=200, callbacks=[tensorboard_callback], verbose=0)\n",
    "\n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "    \n",
    "      # save model\n",
    "      model.save_weights(DIRECTORY_PATH + \"/models/\" + data_name + \"_\" + str(accuracy_score(Y_test, Y_hat))[:6] + \"/\")\n",
    "    else:\n",
    "      # load pretrained model\n",
    "      model = create_model()\n",
    "      model.load_weights(DIRECTORY_PATH + \"/models/\" + \"A09_0.9183/\")\n",
    "      # freeze model\n",
    "      model.trainable = False\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "  accuracy /= n_splits\n",
    "  precision /= n_splits\n",
    "  recall /= n_splits\n",
    "  f1 /= n_splits\n",
    "  kappa /= n_splits\n",
    "\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"kappa: \" + str(kappa))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0d309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "average_accuracy = 0\n",
    "average_precision = 0\n",
    "average_recall = 0\n",
    "average_f1 = 0\n",
    "average_kappa = 0\n",
    "for key, value in results.items():\n",
    "  average_accuracy += value[\"accuracy\"]\n",
    "  average_precision += value[\"precision\"]\n",
    "  average_recall += value[\"recall\"]\n",
    "  average_f1 += value[\"f1\"]\n",
    "  average_kappa += value[\"kappa\"]\n",
    "\n",
    "average_accuracy /= 9\n",
    "average_precision /= 9\n",
    "average_recall /= 9\n",
    "average_f1 /= 9\n",
    "average_kappa /= 9\n",
    "\n",
    "print(\"average accuracy: \" + str(average_accuracy))\n",
    "print(\"average precision: \" + str(average_precision))\n",
    "print(\"average recall: \" + str(average_recall))\n",
    "print(\"average f1: \" + str(average_f1))\n",
    "print(\"average kappa: \" + str(average_kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e059cb",
   "metadata": {},
   "source": [
    "# CNN Classification (reconstructed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "labels = {\"left\": 0, \"right\": 1}\n",
    "select_channels = [7, 9, 11]\n",
    "debug = True\n",
    "training = False\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A09\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  # load data from external storage\n",
    "  directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"reconstructed eeg\", data_name)\n",
    "  counter = 0\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "  while(counter < n_splits):\n",
    "    counter += 1\n",
    "    X_train = np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"]\n",
    "    X_test = np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"]\n",
    "    Y_train = np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"]\n",
    "    Y_test = np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"]\n",
    "    \n",
    "    # pick c3, cZ, c4 channels\n",
    "    X_train = X_train[:, select_channels, :]\n",
    "    X_test = X_test[:, select_channels, :]\n",
    "\n",
    "    print(data_name)\n",
    "    print(\"Training...\")\n",
    "    X_train, Y_train = stft_min_max(X_train, Y_train, debug)\n",
    "    print(\"Testing...\")\n",
    "    X_test, Y_test = stft_min_max(X_test, Y_test, debug)\n",
    "\n",
    "    if debug:\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    if training:\n",
    "      # create new model\n",
    "      model = create_model()\n",
    "      \n",
    "      log_dir = DIRECTORY_PATH + \"/logs/\" + data_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=200, callbacks=[tensorboard_callback], verbose=0)\n",
    "\n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "      # save model\n",
    "      model.save_weights(DIRECTORY_PATH + \"/models/\" + data_name + \"_\" + str(accuracy_score(Y_test, Y_hat))[:6] + \"/\")\n",
    "    else:\n",
    "      # load pretrained model\n",
    "      model = create_model()\n",
    "      model.load_weights(DIRECTORY_PATH + \"/models/\" + \"A09_0.9183/\")\n",
    "      # freeze model\n",
    "      model.trainable = False\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "  accuracy /= n_splits\n",
    "  precision /= n_splits\n",
    "  recall /= n_splits\n",
    "  f1 /= n_splits\n",
    "  kappa /= n_splits\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"kappa: \" + str(kappa))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "average_accuracy = 0\n",
    "average_precision = 0\n",
    "average_recall = 0\n",
    "average_f1 = 0\n",
    "average_kappa = 0\n",
    "for key, value in results.items():\n",
    "  average_accuracy += value[\"accuracy\"]\n",
    "  average_precision += value[\"precision\"]\n",
    "  average_recall += value[\"recall\"]\n",
    "  average_f1 += value[\"f1\"]\n",
    "  average_kappa += value[\"kappa\"]\n",
    "\n",
    "average_accuracy /= 9\n",
    "average_precision /= 9\n",
    "average_recall /= 9\n",
    "average_f1 /= 9\n",
    "average_kappa /= 9\n",
    "\n",
    "print(\"average accuracy: \" + str(average_accuracy))\n",
    "print(\"average precision: \" + str(average_precision))\n",
    "print(\"average recall: \" + str(average_recall))\n",
    "print(\"average f1: \" + str(average_f1))\n",
    "print(\"average kappa: \" + str(average_kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bc9c6",
   "metadata": {},
   "source": [
    "# CNN Classification (Auto-select Source Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03668ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_and_forward_information(epochs, events=[\"left\", \"right\"]):\n",
    "    \n",
    "    subject = \"A01\"\n",
    "    X, Y = [], []\n",
    "    info = None\n",
    "    for event in epochs[subject].keys():\n",
    "        if info is None:\n",
    "            info = epochs[subject][event].info\n",
    "        for i in range(len(events)):\n",
    "            if event == events[i]:\n",
    "                if len(X) == 0:\n",
    "                    X = epochs[subject][event].get_data()\n",
    "                    Y = np.zeros(len(epochs[subject][event].get_data())) + i\n",
    "                else:\n",
    "                    X = np.append(X, epochs[subject][event].get_data(), axis=0)\n",
    "                    Y = np.append(Y, np.zeros(len(epochs[subject][event].get_data())) + i, axis=0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    X_epochs = mne.EpochsArray(X, info, verbose=False)\n",
    "    X_evoked = X_epochs.average().pick(\"eeg\")\n",
    "\n",
    "    noise_cov = mne.compute_covariance(X_epochs, tmax=0., method=['shrunk', 'empirical'], rank=None, verbose=False)\n",
    "    fwd = mne.make_forward_solution(info, trans=trans, src=src,\n",
    "                    bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=1, verbose=False)\n",
    "    fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "                                 use_cps=True, verbose=False)\n",
    "    leadfield = fwd_fixed['sol']['data']\n",
    "    inverse_operator = make_inverse_operator(info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "\n",
    "    method = \"sLORETA\"\n",
    "    snr = 3.\n",
    "    lambda2 = 1. / snr ** 2\n",
    "    stc = apply_inverse(X_evoked, inverse_operator, lambda2, method=method, pick_ori=\"normal\", verbose=True)\n",
    "\n",
    "    # get motor region points\n",
    "    my_source = stc\n",
    "    mni_lh = mne.vertex_to_mni(my_source.vertices[0], 0, mne_subject)\n",
    "    print(mni_lh.shape)\n",
    "    mni_rh = mne.vertex_to_mni(my_source.vertices[1], 1, mne_subject)\n",
    "    print(mni_rh.shape)\n",
    "\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(mm_coords.reshape(-1, 3)[brodmann_motor][:, 0], mm_coords.reshape(-1, 3)[brodmann_motor][:, 1], mm_coords.reshape(-1, 3)[brodmann_motor][:, 2], s=15, marker='|')\n",
    "    ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='_')\n",
    "    ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    my_left_points = in_hull(mni_lh, mm_coords.reshape(-1, 3)[brodmann_motor])\n",
    "    my_right_points = in_hull(mni_rh, mm_coords.reshape(-1, 3)[brodmann_motor])\n",
    "\n",
    "    mni_left_motor = mne.vertex_to_mni(my_source.vertices[0][my_left_points], 0, mne_subject)\n",
    "    print(mni_left_motor.shape)\n",
    "    mni_right_motor = mne.vertex_to_mni(my_source.vertices[1][my_right_points], 1, mne_subject)\n",
    "    print(mni_right_motor.shape)\n",
    "\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='|')\n",
    "    ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "    ax.scatter(mni_left_motor[:, 0], mni_left_motor[:, 1], mni_left_motor[:, 2], s=15, marker='o')\n",
    "    ax.scatter(mni_right_motor[:, 0], mni_right_motor[:, 1], mni_right_motor[:, 2], s=15, marker='^')\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Leadfield size : %d sensors x %d dipoles\" % leadfield.shape)\n",
    "    print(stc.data.shape)\n",
    "\n",
    "    information = {\"my_left_points\": my_left_points, \n",
    "                   \"my_right_points\": my_right_points, \n",
    "                   \"stc_data_shape\": stc.data.shape, \n",
    "                   \"leadfield\": leadfield,\n",
    "                   \"left_vertices\": stc.vertices[0],\n",
    "                   \"right_vertices\": stc.vertices[1]}\n",
    "\n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "information = get_inverse_and_forward_information(epochs)\n",
    "print(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]].shape)\n",
    "print(information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]].shape)\n",
    "forward_matrix = np.concatenate((information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]], \n",
    "                          information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]]),\n",
    "                          axis = 1)\n",
    "print(forward_matrix.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stft_Min_Max(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Stft_Min_Max, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Zxx = tf.signal.stft(inputs, frame_length=256, frame_step=16)\n",
    "        Zxx = tf.abs(Zxx)\n",
    "        X = Zxx[:, :, :, :40]\n",
    "        X = tf.reshape(X, [tf.shape(X)[0], -1, 40])\n",
    "        X = tf.transpose(X, perm=[0, 2, 1])\n",
    "        X = tf.expand_dims(X, axis=3)\n",
    "\n",
    "        # min max scaling (per instance)\n",
    "        #original_shape = tf.shape(X)\n",
    "        original_shape = [tf.shape(X)[0], 40, 48, 1]\n",
    "        X = tf.reshape(X, [original_shape[0], -1])\n",
    "        X_max = tf.math.reduce_max(X, axis=1, keepdims=True)\n",
    "        X_min = tf.math.reduce_min(X, axis=1, keepdims=True)\n",
    "        X = tf.math.divide(tf.math.subtract(X, X_min), tf.math.subtract(X_max, X_min))\n",
    "        X = tf.reshape(X, original_shape)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class AutoSelect(tf.keras.Model):\n",
    "    def __init__(self, select_channels, forward_matrix, random_select):\n",
    "        super(AutoSelect, self).__init__()\n",
    "        # preprocessing\n",
    "        self.select_channels = select_channels\n",
    "        self.forward_matrix = tf.transpose(tf.constant(forward_matrix), perm=[1, 0])\n",
    "        self.concatenate = Concatenate(axis=1)\n",
    "        self.stft_min_max = Stft_Min_Max()\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.source_select = Dense(forward_matrix.shape[1], activation='sigmoid')\n",
    "        self.random_select = random_select\n",
    "        \n",
    "        # classifier\n",
    "        self.conv1 = Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\")\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.mp1 = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\")\n",
    "        self.conv2 = Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\")\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.mp2 = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\")\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(50, activation=\"selu\")\n",
    "        self.dense2 = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):                       # (n, 7981, 500)\n",
    "        # preprocessing\n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])  # (n, 500, 7981)\n",
    "        \n",
    "        if self.random_select:\n",
    "            x = self.dropout(x)                   # (n, 500, 7981)\n",
    "        else:\n",
    "            source_select = self.source_select(x) # (n, 500, 7981)\n",
    "            x = x * source_select                 # (n, 500, 7981)\n",
    "        \n",
    "        x = tf.matmul(x, self.forward_matrix)     # (n, 500, 22)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])       # (n, 22, 500)\n",
    "        c3 = tf.expand_dims(x[:, self.select_channels[0], :], axis=1)\n",
    "        cZ = tf.expand_dims(x[:, self.select_channels[1], :], axis=1)\n",
    "        c4 = tf.expand_dims(x[:, self.select_channels[2], :], axis=1)\n",
    "        x = self.concatenate([c3, cZ, c4])        # (n, 3, 500)\n",
    "        x = self.stft_min_max(x)                  # (n, 40, 48, 1)\n",
    "        \n",
    "        # classifier\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.mp2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cde37e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "labels = {\"left\": 0, \"right\": 1}\n",
    "select_channels = [7, 9, 11]\n",
    "debug = True\n",
    "training = True\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A09\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  # load data from external storage\n",
    "  directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"source activity\", data_name)\n",
    "  counter = 0\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "  while(counter < n_splits):\n",
    "    counter += 1\n",
    "    X_train = tf.constant(np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32))\n",
    "    X_test = tf.constant(np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32))\n",
    "    Y_train = tf.constant(np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32))\n",
    "    Y_test = tf.constant(np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32))\n",
    "    \n",
    "    if debug:\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    if training:\n",
    "      # create new model\n",
    "      model = AutoSelect(select_channels, forward_matrix, False)\n",
    "      \n",
    "      log_dir = DIRECTORY_PATH + \"/logs/\" + data_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=200, callbacks=[tensorboard_callback], verbose=1)\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "      # save model\n",
    "      model.save_weights(DIRECTORY_PATH + \"/models/\" + data_name + \"_\" + str(accuracy_score(Y_test, Y_hat))[:6] + \"/\")\n",
    "    else:\n",
    "      # load pretrained model\n",
    "      model = AutoSelect(select_channels, forward_matrix, False)\n",
    "      model.load_weights(DIRECTORY_PATH + \"/models/\" + \"A09_0.9183/\")\n",
    "      # freeze model\n",
    "      model.trainable = False\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "    \n",
    "    del X_train, Y_train, X_test, Y_test\n",
    "    gc.collect()\n",
    "\n",
    "  accuracy /= n_splits\n",
    "  precision /= n_splits\n",
    "  recall /= n_splits\n",
    "  f1 /= n_splits\n",
    "  kappa /= n_splits\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"kappa: \" + str(kappa))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "average_accuracy = 0\n",
    "average_precision = 0\n",
    "average_recall = 0\n",
    "average_f1 = 0\n",
    "average_kappa = 0\n",
    "for key, value in results.items():\n",
    "  average_accuracy += value[\"accuracy\"]\n",
    "  average_precision += value[\"precision\"]\n",
    "  average_recall += value[\"recall\"]\n",
    "  average_f1 += value[\"f1\"]\n",
    "  average_kappa += value[\"kappa\"]\n",
    "\n",
    "average_accuracy /= 9\n",
    "average_precision /= 9\n",
    "average_recall /= 9\n",
    "average_f1 /= 9\n",
    "average_kappa /= 9\n",
    "\n",
    "print(\"average accuracy: \" + str(average_accuracy))\n",
    "print(\"average precision: \" + str(average_precision))\n",
    "print(\"average recall: \" + str(average_recall))\n",
    "print(\"average f1: \" + str(average_f1))\n",
    "print(\"average kappa: \" + str(average_kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf005f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
