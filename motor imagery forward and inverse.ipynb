{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15aa7db",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from mne.datasets import sample\n",
    "from mne.minimum_norm import make_inverse_operator, apply_inverse_epochs, apply_inverse, apply_inverse_raw\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from mne.decoding import CSP, UnsupervisedSpatialFilter\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy import stats\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPool1D, AveragePooling1D, Conv2D, DepthwiseConv2D, SeparableConv2D, MaxPool2D, AveragePooling2D, GlobalAveragePooling2D, Dense, Activation, Flatten, Concatenate, BatchNormalization, LayerNormalization, Dropout, Input\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import pandas as pd\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import json\n",
    "import multiprocessing\n",
    "from scipy.spatial import KDTree\n",
    "from nibabel.nifti1 import Nifti1Image\n",
    "from nilearn import plotting\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib qt\n",
    "\n",
    "DIRECTORY_PATH = os.getcwd()\n",
    "EXTERNAL_STORAGE_PATH = \"E:\\Motor Imagery\"\n",
    "RECONSTRUCT_SAVE_FOLDER = \"all motor region\"\n",
    "n_splits = 5\n",
    "\n",
    "# force tensorflow to use cpu when facing memory issue\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)\n",
    "    \n",
    "mne.set_log_level(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257a3fa",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45005041",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_mapping = {\n",
    "    \"EEG-Fz\": \"Fz\",\n",
    "    \"EEG-0\": \"FC3\",\n",
    "    \"EEG-1\": \"FC1\",\n",
    "    \"EEG-2\": \"FCz\",\n",
    "    \"EEG-3\": \"FC2\",\n",
    "    \"EEG-4\": \"FC4\",\n",
    "    \"EEG-5\": \"C5\",\n",
    "    \"EEG-C3\": \"C3\", \n",
    "    \"EEG-6\": \"C1\",\n",
    "    \"EEG-Cz\": \"Cz\",\n",
    "    \"EEG-7\": \"C2\",\n",
    "    \"EEG-C4\": \"C4\",\n",
    "    \"EEG-8\": \"C6\",\n",
    "    \"EEG-9\": \"CP3\",\n",
    "    \"EEG-10\": \"CP1\",\n",
    "    \"EEG-11\": \"CPz\",\n",
    "    \"EEG-12\": \"CP2\",\n",
    "    \"EEG-13\": \"CP4\",\n",
    "    \"EEG-14\": \"P1\",\n",
    "    \"EEG-Pz\": \"Pz\",\n",
    "    \"EEG-15\": \"P2\",\n",
    "    \"EEG-16\": \"POz\",\n",
    "    \"EOG-left\": \"EOG-left\",\n",
    "    \"EOG-central\": \"EOG-central\",\n",
    "    \"EOG-right\": \"EOG-right\"\n",
    "}\n",
    "\n",
    "channels_type_mapping = {\n",
    "    \"Fz\": \"eeg\",\n",
    "    \"FC3\": \"eeg\",\n",
    "    \"FC1\": \"eeg\",\n",
    "    \"FCz\": \"eeg\",\n",
    "    \"FC2\": \"eeg\",\n",
    "    \"FC4\": \"eeg\",\n",
    "    \"C5\": \"eeg\",\n",
    "    \"C3\": \"eeg\", \n",
    "    \"C1\": \"eeg\",\n",
    "    \"Cz\": \"eeg\",\n",
    "    \"C2\": \"eeg\",\n",
    "    \"C4\": \"eeg\",\n",
    "    \"C6\": \"eeg\",\n",
    "    \"CP3\": \"eeg\",\n",
    "    \"CP1\": \"eeg\",\n",
    "    \"CPz\": \"eeg\",\n",
    "    \"CP2\": \"eeg\",\n",
    "    \"CP4\": \"eeg\",\n",
    "    \"P1\": \"eeg\",\n",
    "    \"Pz\": \"eeg\",\n",
    "    \"P2\": \"eeg\",\n",
    "    \"POz\": \"eeg\",\n",
    "    \"EOG-left\": \"eog\",\n",
    "    \"EOG-central\": \"eog\",\n",
    "    \"EOG-right\": \"eog\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(\"/Users/ivanlim/Downloads/MRIcron_windows/MRIcron/Resources/templates/brodmann.nii.gz\")\n",
    "ch2_img = nib.load(\"C:/Users/ivanlim/Downloads/MRIcron_windows/MRIcron/Resources/templates/ch2.nii.gz\")\n",
    "\n",
    "brodmann_data = img.get_fdata()\n",
    "# Areas 3, 1 and 2 – Primary somatosensory cortex in the postcentral gyrus (frequently referred to as Areas 3, 1, 2 by convention)\n",
    "# Area 4– Primary motor cortex\n",
    "# Area 5 – Superior parietal lobule\n",
    "# Area 6 – Premotor cortex and Supplementary Motor Cortex (Secondary Motor Cortex) (Supplementary motor area)\n",
    "# Area 7 – Visuo-Motor Coordination\n",
    "brodmann_motor = []\n",
    "selected_area = [1, 2, 3, 4, 5, 6, 7]\n",
    "#selected_area = [4]\n",
    "\n",
    "# old: compute one convex hull for all the selected regions\n",
    "# new: compute a single convex hull for one selected region then combine them together\n",
    "reconstruct_mode = \"new\"\n",
    "\n",
    "for area in selected_area:\n",
    "    if reconstruct_mode == \"old\":\n",
    "        if len(brodmann_motor) == 0:\n",
    "            brodmann_motor.append(brodmann_data.reshape(-1) == area)\n",
    "        else:\n",
    "            brodmann_motor[0] += brodmann_data.reshape(-1) == area\n",
    "    else:\n",
    "        brodmann_motor.append(brodmann_data.reshape(-1) == area)\n",
    "\n",
    "print(brodmann_motor)\n",
    "print(\"brodmann template shape: \" + str(brodmann_data.shape))\n",
    "if reconstruct_mode == \"old\":\n",
    "    print(\"chosen points: \" + str(np.sum(brodmann_motor[0])))\n",
    "else:\n",
    "    chosen_points = None\n",
    "    for selected_region in brodmann_motor:\n",
    "        print(np.sum(selected_region))\n",
    "        if chosen_points is None:\n",
    "            chosen_points = np.array(selected_region, copy=True)\n",
    "        else:\n",
    "            chosen_points += selected_region\n",
    "    print(\"chosen points: \" + str(np.sum(chosen_points)))\n",
    "\n",
    "shape, affine = img.shape[:3], img.affine\n",
    "coords = np.array(np.meshgrid(*(range(i) for i in shape), indexing='ij'))\n",
    "coords = np.rollaxis(coords, 0, len(shape) + 1)\n",
    "mm_coords = nib.affines.apply_affine(affine, coords)\n",
    "\n",
    "def in_hull(p, hull):\n",
    "    \"\"\"\n",
    "    Test if points in `p` are in `hull`\n",
    "\n",
    "    `p` should be a `NxK` coordinates of `N` points in `K` dimensions\n",
    "    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the \n",
    "    coordinates of `M` points in `K`dimensions for which Delaunay triangulation\n",
    "    will be computed\n",
    "    \"\"\"\n",
    "    if not isinstance(hull,Delaunay):\n",
    "        hull = Delaunay(hull)\n",
    "\n",
    "    return hull.find_simplex(p)>=0\n",
    "\n",
    "my_left_points = None\n",
    "my_right_points = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "labels utility function\n",
    "\"\"\"\n",
    "def load_subject_labels(name=\"A01E.mat\", dir=\"drive/Shareddrives/Motor Imagery/BCI competition IV dataset/2a/2a true_labels/\"):\n",
    "  data = scipy.io.loadmat(dir + name)[\"classlabel\"].reshape(-1)\n",
    "  return data\n",
    "\n",
    "def load_all_true_labels(dataset_path):\n",
    "  data = {}\n",
    "  for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "      data[file] = load_subject_labels(name=file, dir=root) \n",
    "  return data\n",
    "\n",
    "\"\"\"\n",
    "plot graph utility function\n",
    "\"\"\"\n",
    "def plot_average_graph(subject_name=\"A01T.gdf\", Class=\"left\", filter_channels=None):\n",
    "  average = {\"left\": None, \"right\": None, \"foot\": None, \"tongue\": None, \"unknown\": None}\n",
    "  for event_class, event_data in data[subject_name][\"epoch_data\"].items():\n",
    "    if event_data != []:\n",
    "      average[event_class] = np.transpose(np.mean(event_data, axis=0))\n",
    "\n",
    "  x = average[Class]\n",
    "  if filter_channels is None:\n",
    "    fig, axs = plt.subplots(x.shape[1], gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 21)\n",
    "    for channel in range(x.shape[1]):\n",
    "      axs[channel].title.set_text(ch_names[channel])\n",
    "      axs[channel].title.set_size(20)\n",
    "      axs[channel].title.set_y(0.7)\n",
    "      axs[channel].plot(range(x.shape[0]), x[:, channel])\n",
    "      axs[channel].axvline(x=250, color=\"r\", linestyle='--')\n",
    "      #axs[channel].axvline(x=875, color=\"r\", linestyle='--')\n",
    "  else :\n",
    "    fig, axs = plt.subplots(len(filter_channels), gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 10.5)\n",
    "    for i in range(len(filter_channels)):\n",
    "      for channel in range(x.shape[1]):\n",
    "        if(filter_channels[i] == ch_names[channel]):\n",
    "          axs[i].title.set_text(ch_names[channel])\n",
    "          axs[i].title.set_size(20)\n",
    "          axs[i].title.set_y(0.7)\n",
    "          axs[i].plot(range(x.shape[0]), x[:, channel])\n",
    "          axs[i].axvline(x=250, color=\"r\", linestyle='--')\n",
    "          #axs[i].axvline(x=875, color=\"r\", linestyle='--')\n",
    "          break\n",
    "  plt.tight_layout()\n",
    "\n",
    "def plot_multiple_graph(subject_name=\"A02T.gdf\", classes=[\"left\", \"right\", \"foot\", \"tongue\"], filter_channels=None):\n",
    "  average = {\"left\": None, \"right\": None, \"foot\": None, \"tongue\": None, \"unknown\": None}\n",
    "  for event_class, event_data in data[subject_name][\"epoch_data\"].items():\n",
    "    if event_data != []:\n",
    "      average[event_class] = np.transpose(np.mean(event_data, axis=0))\n",
    "\n",
    "  color = {\"left\": \"b\", \"right\": \"g\", \"foot\": \"c\", \"tongue\": \"m\", \"tongue\": \"y\"}\n",
    "  x = []\n",
    "  for Class in classes:\n",
    "    x.append(average[Class])\n",
    "\n",
    "  if filter_channels is None:\n",
    "    fig, axs = plt.subplots(x[0].shape[1], gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 21)\n",
    "    for channel in range(x[0].shape[1]):\n",
    "      axs[channel].title.set_text(ch_names[channel])\n",
    "      axs[channel].title.set_size(20)\n",
    "      axs[channel].title.set_y(0.7)\n",
    "      axs[channel].axvline(x=250, color=\"r\", linestyle='--')\n",
    "      #axs[channel].axvline(x=875, color=\"r\", linestyle='--')\n",
    "      for i in range(len(classes)):\n",
    "        axs[channel].plot(range(x[i].shape[0]), x[i][:, channel], color=color[classes[i]])\n",
    "  else:\n",
    "    fig, axs = plt.subplots(len(filter_channels), gridspec_kw={'hspace': 0})\n",
    "    fig.set_size_inches(37, 10.5)\n",
    "    for i in range(len(filter_channels)):\n",
    "      for channel in range(x[0].shape[1]):\n",
    "        if(filter_channels[i] == ch_names[channel]):\n",
    "          axs[i].title.set_text(ch_names[channel])\n",
    "          axs[i].title.set_size(20)\n",
    "          axs[i].title.set_y(0.7)\n",
    "          axs[i].axvline(x=250, color=\"r\", linestyle='--')\n",
    "          #axs[i].axvline(x=875, color=\"r\", linestyle='--')\n",
    "          for j in range(len(classes)):\n",
    "            axs[i].plot(range(x[j].shape[0]), x[j][:, channel], color=color[classes[j]])\n",
    "          break\n",
    "  plt.tight_layout()\n",
    "\n",
    "\"\"\"\n",
    "load data function\n",
    "\"\"\"\n",
    "def load_epoch_data(raw, name, debug=None):\n",
    "  if debug:\n",
    "    raw.plot()\n",
    "  subject_data = {}\n",
    "\n",
    "  subject_data[\"raw\"] = raw\n",
    "  subject_data[\"info\"] = raw.info\n",
    "  if debug == \"all\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for key, item in raw.info.items():\n",
    "      print(key, item)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  \"\"\"\n",
    "  '276': 'Idling EEG (eyes open)'\n",
    "  '277': 'Idling EEG (eyes closed)'\n",
    "  '768': 'Start of a trial'\n",
    "  '769': 'Cue onset left (class 1)'\n",
    "  '770': 'Cue onset right (class 2)'\n",
    "  '771': 'Cue onset foot (class 3)'\n",
    "  '772': 'Cue onset tongue (class 4)'\n",
    "  '783': 'Cue unknown'\n",
    "  '1023': 'Rejected trial'\n",
    "  '1072': 'Eye movements'\n",
    "  '32766': 'Start of a new run'\n",
    "  \"\"\"\n",
    "  custom_mapping = {'276': 276, '277': 277, '768': 768, '769': 769, '770': 770, '771': 771, '772': 772, '783': 783, '1023': 1023, '1072': 1072, '32766': 32766}\n",
    "  events_from_annot, event_dict = mne.events_from_annotations(raw, event_id=custom_mapping)\n",
    "\n",
    "  if debug == \" all\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(event_dict)\n",
    "    print(events_from_annot)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for i in range(len(raw.annotations)):\n",
    "      print(events_from_annot[i], raw.annotations[i])  \n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  class_info = \"Idling EEG (eyes open): \" + str(len(events_from_annot[events_from_annot[:, 2]==276][:, 0])) + \"\\n\" + \\\n",
    "               \"Idling EEG (eyes closed): \" + str(len(events_from_annot[events_from_annot[:, 2]==277][:, 0])) + \"\\n\" + \\\n",
    "               \"Start of a trial: \" + str(len(events_from_annot[events_from_annot[:, 2]==768][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset left (class 1): \" + str(len(events_from_annot[events_from_annot[:, 2]==769][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset right (class 2): \" + str(len(events_from_annot[events_from_annot[:, 2]==770][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset foot (class 3): \" + str(len(events_from_annot[events_from_annot[:, 2]==771][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset tongue (class 4): \" + str(len(events_from_annot[events_from_annot[:, 2]==772][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue unknown: \" + str(len(events_from_annot[events_from_annot[:, 2]==783][:, 0])) + \"\\n\" + \\\n",
    "               \"Rejected trial: \" + str(len(events_from_annot[events_from_annot[:, 2]==1023][:, 0])) + \"\\n\" + \\\n",
    "               \"Eye movements: \" + str(len(events_from_annot[events_from_annot[:, 2]==1072][:, 0])) + \"\\n\" + \\\n",
    "               \"Start of a new run: \" + str(len(events_from_annot[events_from_annot[:, 2]==32766][:, 0]))\n",
    "  subject_data[\"class_info\"] = class_info\n",
    "\n",
    "  if debug == \"all\" or debug == \"important\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(class_info)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  epoch_data = {\"left\": [], \"right\": [], \"foot\": [], \"tongue\": [], \"unknown\": []}\n",
    "  rejected_trial = events_from_annot[events_from_annot[:, 2]==1023][:, 0]\n",
    "  class_dict = {\"left\": 769, \"right\": 770, \"foot\": 771, \"tongue\": 772, \"unknown\": 783}\n",
    "  raw_data = raw.get_data()  #(22, 672528)\n",
    "  start = 10                 # cue+0.1s\n",
    "  stop = 510                 # cue+2.1s\n",
    "\n",
    "  for event_class, event_id in class_dict.items():\n",
    "    current_event = events_from_annot[events_from_annot[:, 2]==event_id][:, 0]\n",
    "    if event_class == \"unknown\":\n",
    "      subject_true_labels = true_labels[name[:4]+\".mat\"]\n",
    "      class_dict_labels = {1: \"left\", 2: \"right\", 3: \"foot\", 4: \"tongue\"}\n",
    "      for i in range(len(current_event)):\n",
    "        # exclude artifact\n",
    "        if (current_event[i] - 500 != rejected_trial).all():\n",
    "          current_event_data = np.expand_dims(np.array(raw_data[:22, current_event[i]+start:current_event[i]+stop]), axis=0)\n",
    "          if (epoch_data.get(class_dict_labels[subject_true_labels[i]]) == None).all():\n",
    "            epoch_data[class_dict_labels[subject_true_labels[i]]] = current_event_data\n",
    "          else:\n",
    "            epoch_data[class_dict_labels[subject_true_labels[i]]] = np.append(epoch_data[class_dict_labels[subject_true_labels[i]]], current_event_data, axis=0)\n",
    "    else:\n",
    "      for i in range(len(current_event)):\n",
    "        # exclude artifact\n",
    "        if((current_event[i] - 500 != rejected_trial).all()):\n",
    "          epoch_data[event_class].append(np.array(raw_data[:22, current_event[i]+start:current_event[i]+stop]))\n",
    "      epoch_data[event_class] = np.array(epoch_data[event_class])\n",
    "\n",
    "  if debug == \"all\" or debug == \"important\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for key, data in epoch_data.items():\n",
    "      print(key, len(data))\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  for event_class, event_data in epoch_data.items():\n",
    "    epoch_data[event_class] = np.array(event_data)\n",
    "\n",
    "  subject_data[\"epoch_data\"] = epoch_data\n",
    "\n",
    "  return subject_data\n",
    "\n",
    "def load_subject(name=\"A01T.gdf\", dir='drive/Shareddrives/Motor Imagery/BCI competition IV dataset/2a/BCICIV_2a_gdf/', filter_bank=None, debug=None):\n",
    "  # Load data\n",
    "  raw = mne.io.read_raw_gdf(dir + name, preload=True)\n",
    "  # Rename channels\n",
    "  raw.rename_channels(channels_mapping)\n",
    "  # Set channels types\n",
    "  raw.set_channel_types(channels_type_mapping)\n",
    "  # Set montage\n",
    "  # Read and set the EEG electrode locations\n",
    "  ten_twenty_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "  raw.set_montage(ten_twenty_montage)\n",
    "  # Drop eog channels\n",
    "  raw.drop_channels([\"EOG-left\", \"EOG-central\", \"EOG-right\"])\n",
    "  # Set common average reference\n",
    "  raw.set_eeg_reference('average', projection=True, verbose=False)\n",
    "    \n",
    "  subject_data_dict = {}\n",
    "\n",
    "  if filter_bank is None:\n",
    "    filter_bank = [-1]\n",
    "  else:\n",
    "    filter_bank = [-1] + filter_bank\n",
    "  print(\"filter bank: \", filter_bank)\n",
    "\n",
    "  for i in range(len(filter_bank)-1):\n",
    "    low = filter_bank[i]\n",
    "    high = filter_bank[i+1]\n",
    "    print(\"current frequency: \", low, \" to \", high)\n",
    "  \n",
    "    # filter frequency\n",
    "    filter_raw = raw.copy()\n",
    "    if low != -1:\n",
    "        iir_params = dict(order=5, ftype='butter')\n",
    "        filter_raw.filter(low, high, method=\"iir\", iir_params=iir_params)\n",
    "    subject_data = load_epoch_data(filter_raw, name, debug)\n",
    "    if low == -1:\n",
    "      subject_data_dict[\"original\"] = subject_data\n",
    "    else:\n",
    "      subject_data_dict[str(low)+\"-\"+str(high)] = subject_data\n",
    "\n",
    "  return subject_data_dict\n",
    "\n",
    "def load_all_subject(dataset_path, filter_bank=None):\n",
    "  data = {}\n",
    "  for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "      data[file] = load_subject(name=file, dir=root, filter_bank=filter_bank) \n",
    "  return data\n",
    "\n",
    "\"\"\"\n",
    "create mne epochs data structure from numpy array\n",
    "merge training and evaluation data\n",
    "return original and bandpass-filtered data\n",
    "\"\"\"\n",
    "def create_epochs(data):\n",
    "  subjects_data = {}\n",
    "\n",
    "  for subject in data.keys():\n",
    "    if \"E\" in subject:\n",
    "        continue\n",
    "    epochs_data = {}\n",
    "    for event in data[subject][\"original\"][\"epoch_data\"].keys():\n",
    "      current_event_data = None\n",
    "      \n",
    "      if data[subject][\"original\"][\"epoch_data\"][event].any():\n",
    "        current_event_data = data[subject][\"original\"][\"epoch_data\"][event]\n",
    "      if data[subject[:3]+\"E.gdf\"][\"original\"][\"epoch_data\"][event].any():\n",
    "        current_event_data = np.append(current_event_data, data[subject[:3]+\"E.gdf\"][\"original\"][\"epoch_data\"][event], axis=0)\n",
    "      if current_event_data is not None:\n",
    "          epochs_data[event] = mne.EpochsArray(current_event_data, data[subject][\"original\"][\"info\"], verbose=False)\n",
    "\n",
    "    subjects_data[subject[:3]] = epochs_data\n",
    "    \n",
    "  subjects_filter_data = {}\n",
    "  \n",
    "  for subject in data.keys():\n",
    "    if \"E\" in subject:\n",
    "        continue\n",
    "    epochs_data = {}\n",
    "    for freq in data[subject].keys():\n",
    "        if freq != \"original\":\n",
    "            #print(freq.split(\"-\"))\n",
    "            epochs_data[freq] = {}\n",
    "        \n",
    "            for event in data[subject][freq][\"epoch_data\"].keys():\n",
    "                current_event_data = None\n",
    "\n",
    "                if data[subject][freq][\"epoch_data\"][event].any():\n",
    "                    current_event_data = data[subject][freq][\"epoch_data\"][event]\n",
    "                if data[subject[:3]+\"E.gdf\"][freq][\"epoch_data\"][event].any():\n",
    "                    current_event_data = np.append(current_event_data, data[subject[:3]+\"E.gdf\"][freq][\"epoch_data\"][event], axis=0)\n",
    "                if current_event_data is not None:\n",
    "                    epochs_data[freq][event] = mne.EpochsArray(current_event_data, data[subject][freq][\"info\"], verbose=False)\n",
    "                \n",
    "            subjects_filter_data[subject[:3]] = epochs_data\n",
    "            \n",
    "  return subjects_data, subjects_filter_data\n",
    "\n",
    "\"\"\"\n",
    "Create source activity and reconstructed eeg respectively for each subject\n",
    "\n",
    "For each subject, there are four events in total, i.e. left, right, foot, tongue\n",
    "Split these data into train and test set using kfold\n",
    "Compute the noise covariance matrix on train set and apply it to test set\n",
    "Create source activity (only motor region) first by applying an inverse operator to the epochs \n",
    "Create reconstructed eeg by applying a forward operator to the source activity acquired earlier\n",
    "Save both these files to disk\n",
    "\"\"\"\n",
    "def apply_inverse_and_forward_kfold(epochs, n_splits=5, save_inverse=True, save_forward=True, events=[\"left\", \"right\"], subjects=None):\n",
    "    global my_left_points, my_right_points\n",
    "    \n",
    "    if subjects is None:\n",
    "        subjects = epochs.keys()\n",
    "        \n",
    "    for subject in subjects:  \n",
    "        print(subject)\n",
    "        X, Y = [], []\n",
    "        info = None\n",
    "        counter = 0\n",
    "        for event in epochs[subject].keys():\n",
    "            if info is None:\n",
    "                info = epochs[subject][event].info\n",
    "            for i in range(len(events)):\n",
    "                if event == events[i]:\n",
    "                    print(event)\n",
    "                    if len(X) == 0:\n",
    "                        X = epochs[subject][event].get_data()\n",
    "                        Y = np.zeros(len(epochs[subject][event].get_data())) + i\n",
    "                    else:\n",
    "                        X = np.append(X, epochs[subject][event].get_data(), axis=0)\n",
    "                        Y = np.append(Y, np.zeros(len(epochs[subject][event].get_data())) + i, axis=0)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "        for train_index, test_index in skf.split(X, Y):\n",
    "            counter += 1\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            X_train = mne.EpochsArray(X_train, info, verbose=False)\n",
    "            X_test = mne.EpochsArray(X_test, info, verbose=False)\n",
    "            \n",
    "            noise_cov = mne.compute_covariance(X_train, tmax=0., method=['shrunk', 'empirical'], rank=None, verbose=False)\n",
    "            fwd = mne.make_forward_solution(info, trans=trans, src=src,\n",
    "                            bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=1, verbose=False)\n",
    "            fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "                                         use_cps=True, verbose=False)\n",
    "            leadfield = fwd_fixed['sol']['data']\n",
    "            inverse_operator = make_inverse_operator(info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "            \n",
    "            method = \"sLORETA\"\n",
    "            snr = 3.\n",
    "            lambda2 = 1. / snr ** 2\n",
    "            stc_train = apply_inverse_epochs(X_train, inverse_operator, lambda2,\n",
    "                                          method=method, pick_ori=\"normal\", verbose=True)\n",
    "            \n",
    "            # get motor region points (once)\n",
    "            if my_left_points is None and my_right_points is None:\n",
    "                my_source = stc_train[0]\n",
    "                mni_lh = mne.vertex_to_mni(my_source.vertices[0], 0, mne_subject)\n",
    "                print(mni_lh.shape)\n",
    "                mni_rh = mne.vertex_to_mni(my_source.vertices[1], 1, mne_subject)\n",
    "                print(mni_rh.shape)\n",
    "\n",
    "                fig = plt.figure(figsize=(8, 8))\n",
    "                ax = fig.add_subplot(projection='3d')\n",
    "                for selected_region in brodmann_motor:\n",
    "                    ax.scatter(mm_coords.reshape(-1, 3)[selected_region][:, 0], mm_coords.reshape(-1, 3)[selected_region][:, 1], mm_coords.reshape(-1, 3)[selected_region][:, 2], s=15, marker='|')\n",
    "                ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='_')\n",
    "                ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "                ax.set_xlabel('X')\n",
    "                ax.set_ylabel('Y')\n",
    "                ax.set_zlabel('Z')\n",
    "                plt.show()\n",
    "\n",
    "                my_left_points = None\n",
    "                my_right_points = None\n",
    "                for selected_region in brodmann_motor:\n",
    "                    print(np.sum(selected_region))\n",
    "                    if my_left_points is None:\n",
    "                        my_left_points = in_hull(mni_lh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "                        my_right_points = in_hull(mni_rh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "                    else:\n",
    "                        my_left_points += in_hull(mni_lh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "                        my_right_points += in_hull(mni_rh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "\n",
    "                mni_left_motor = mne.vertex_to_mni(my_source.vertices[0][my_left_points], 0, mne_subject)\n",
    "                print(mni_left_motor.shape)\n",
    "                mni_right_motor = mne.vertex_to_mni(my_source.vertices[1][my_right_points], 1, mne_subject)\n",
    "                print(mni_right_motor.shape)\n",
    "\n",
    "                fig = plt.figure(figsize=(8, 8))\n",
    "                ax = fig.add_subplot(projection='3d')\n",
    "                ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='|')\n",
    "                ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "                ax.scatter(mni_left_motor[:, 0], mni_left_motor[:, 1], mni_left_motor[:, 2], s=15, marker='o')\n",
    "                ax.scatter(mni_right_motor[:, 0], mni_right_motor[:, 1], mni_right_motor[:, 2], s=15, marker='^')\n",
    "                ax.set_xlabel('X')\n",
    "                ax.set_ylabel('Y')\n",
    "                ax.set_zlabel('Z')\n",
    "                plt.show()\n",
    "                \n",
    "            print(\"Leadfield size : %d sensors x %d dipoles\" % leadfield.shape)\n",
    "            print(stc_train[0].data.shape)\n",
    "            \n",
    "            # train set\n",
    "            # slice source activity data\n",
    "            left_hemi_data = []\n",
    "            right_hemi_data = []\n",
    "            for source in stc_train:\n",
    "                left_hemi_data.append(source.data[:len(source.vertices[0])][my_left_points])\n",
    "                right_hemi_data.append(source.data[-len(source.vertices[1]):][my_right_points])\n",
    "            left_hemi_data = np.array(left_hemi_data)\n",
    "            right_hemi_data = np.array(right_hemi_data)\n",
    "            if save_inverse:\n",
    "                source_activity_path = op.join(EXTERNAL_STORAGE_PATH, RECONSTRUCT_SAVE_FOLDER, \"data\", \"source activity\", subject)\n",
    "                if not op.exists(source_activity_path):\n",
    "                    os.makedirs(source_activity_path)\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_train_X.npz\"), data=np.append(left_hemi_data, right_hemi_data, axis=1))\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_train_Y.npz\"), data=Y_train)\n",
    "            # slice reconstructed eeg data\n",
    "            reconstructed_eeg_data = []\n",
    "            for source in stc_train:\n",
    "                motor_source = np.zeros_like(source.data)\n",
    "                motor_source[:len(source.vertices[0])][my_left_points] = source.data[:len(source.vertices[0])][my_left_points]\n",
    "                motor_source[-len(source.vertices[1]):][my_right_points] = source.data[-len(source.vertices[1]):][my_right_points]\n",
    "                motor_eeg = np.dot(leadfield, motor_source)\n",
    "                reconstructed_eeg_data.append(motor_eeg)\n",
    "            if save_forward:\n",
    "                reconstructed_eeg_path = op.join(EXTERNAL_STORAGE_PATH, RECONSTRUCT_SAVE_FOLDER, \"data\", \"reconstructed eeg\", subject)\n",
    "                if not op.exists(reconstructed_eeg_path):\n",
    "                    os.makedirs(reconstructed_eeg_path)\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_train_X.npz\"), data=np.array(reconstructed_eeg_data))\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_train_Y.npz\"), data=Y_train)\n",
    "            \n",
    "            del stc_train\n",
    "            gc.collect()\n",
    "            \n",
    "            stc_test = apply_inverse_epochs(X_test, inverse_operator, lambda2,\n",
    "                              method=method, pick_ori=\"normal\", verbose=True)\n",
    "            # test set\n",
    "            # slice source activity data\n",
    "            left_hemi_data = []\n",
    "            right_hemi_data = []\n",
    "            for source in stc_test:\n",
    "                left_hemi_data.append(source.data[:len(source.vertices[0])][my_left_points])\n",
    "                right_hemi_data.append(source.data[-len(source.vertices[1]):][my_right_points])\n",
    "            left_hemi_data = np.array(left_hemi_data)\n",
    "            right_hemi_data = np.array(right_hemi_data)\n",
    "            if save_inverse:\n",
    "                source_activity_path = op.join(EXTERNAL_STORAGE_PATH, RECONSTRUCT_SAVE_FOLDER, \"data\", \"source activity\", subject)\n",
    "                if not op.exists(source_activity_path):\n",
    "                    os.makedirs(source_activity_path)\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_test_X.npz\"), data=np.append(left_hemi_data, right_hemi_data, axis=1))\n",
    "                np.savez_compressed(op.join(source_activity_path, str(counter)+\"_test_Y.npz\"), data=Y_test)\n",
    "            # slice reconstructed eeg data\n",
    "            reconstructed_eeg_data = []\n",
    "            for source in stc_test:\n",
    "                motor_source = np.zeros_like(source.data)\n",
    "                motor_source[:len(source.vertices[0])][my_left_points] = source.data[:len(source.vertices[0])][my_left_points]\n",
    "                motor_source[-len(source.vertices[1]):][my_right_points] = source.data[-len(source.vertices[1]):][my_right_points]\n",
    "                motor_eeg = np.dot(leadfield, motor_source)\n",
    "                reconstructed_eeg_data.append(motor_eeg)\n",
    "            if save_forward:\n",
    "                reconstructed_eeg_path = op.join(EXTERNAL_STORAGE_PATH, RECONSTRUCT_SAVE_FOLDER, \"data\", \"reconstructed eeg\", subject)\n",
    "                if not op.exists(reconstructed_eeg_path):\n",
    "                    os.makedirs(reconstructed_eeg_path)\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_test_X.npz\"), data=np.array(reconstructed_eeg_data))\n",
    "                np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\"_test_Y.npz\"), data=Y_test)\n",
    "            \n",
    "            del X_train, X_test, Y_train, Y_test\n",
    "            del stc_test, reconstructed_eeg_data, left_hemi_data, right_hemi_data\n",
    "            gc.collect()\n",
    "            \n",
    "            \n",
    "def get_inverse_and_forward_information(epochs, events=[\"left\", \"right\"]):\n",
    "    \n",
    "    subject = \"A01\"\n",
    "    X, Y = [], []\n",
    "    info = None\n",
    "    for event in epochs[subject].keys():\n",
    "        if info is None:\n",
    "            info = epochs[subject][event].info\n",
    "        for i in range(len(events)):\n",
    "            if event == events[i]:\n",
    "                if len(X) == 0:\n",
    "                    X = epochs[subject][event].get_data()\n",
    "                    Y = np.zeros(len(epochs[subject][event].get_data())) + i\n",
    "                else:\n",
    "                    X = np.append(X, epochs[subject][event].get_data(), axis=0)\n",
    "                    Y = np.append(Y, np.zeros(len(epochs[subject][event].get_data())) + i, axis=0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    X_epochs = mne.EpochsArray(X, info, verbose=False)\n",
    "    X_evoked = X_epochs.average().pick(\"eeg\")\n",
    "\n",
    "    noise_cov = mne.compute_covariance(X_epochs, tmax=0., method=['shrunk', 'empirical'], rank=None, verbose=False)\n",
    "    fwd = mne.make_forward_solution(info, trans=trans, src=src,\n",
    "                    bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=1, verbose=False)\n",
    "    fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "                                 use_cps=True, verbose=False)\n",
    "    leadfield = fwd_fixed['sol']['data']\n",
    "    inverse_operator = make_inverse_operator(info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "\n",
    "    method = \"sLORETA\"\n",
    "    snr = 3.\n",
    "    lambda2 = 1. / snr ** 2\n",
    "    stc = apply_inverse(X_evoked, inverse_operator, lambda2, method=method, pick_ori=\"normal\", verbose=True)\n",
    "\n",
    "    # get motor region points\n",
    "    my_source = stc\n",
    "    mni_lh = mne.vertex_to_mni(my_source.vertices[0], 0, mne_subject)\n",
    "    print(mni_lh.shape)\n",
    "    mni_rh = mne.vertex_to_mni(my_source.vertices[1], 1, mne_subject)\n",
    "    print(mni_rh.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='_', alpha=0.5)\n",
    "    ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_', alpha=0.5)\n",
    "    for selected_region in brodmann_motor:\n",
    "        ax.scatter(mm_coords.reshape(-1, 3)[selected_region][:, 0], mm_coords.reshape(-1, 3)[selected_region][:, 1], mm_coords.reshape(-1, 3)[selected_region][:, 2], s=15, marker='|', alpha=0.2)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    #fig.savefig(DIRECTORY_PATH+\"/brodmann template and source positions.png\", dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    my_left_points = None\n",
    "    my_right_points = None\n",
    "    for selected_region in brodmann_motor:\n",
    "        print(np.sum(selected_region))\n",
    "        if my_left_points is None:\n",
    "            my_left_points = in_hull(mni_lh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "            my_right_points = in_hull(mni_rh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "        else:\n",
    "            my_left_points += in_hull(mni_lh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "            my_right_points += in_hull(mni_rh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "\n",
    "    mni_left_motor = mne.vertex_to_mni(my_source.vertices[0][my_left_points], 0, mne_subject)\n",
    "    print(mni_left_motor.shape)\n",
    "    mni_right_motor = mne.vertex_to_mni(my_source.vertices[1][my_right_points], 1, mne_subject)\n",
    "    print(mni_right_motor.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='|', alpha=0.3)\n",
    "    ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_', alpha=0.3)\n",
    "    ax.scatter(mni_left_motor[:, 0], mni_left_motor[:, 1], mni_left_motor[:, 2], s=15, marker='o', alpha=0.5)\n",
    "    ax.scatter(mni_right_motor[:, 0], mni_right_motor[:, 1], mni_right_motor[:, 2], s=15, marker='^', alpha=0.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    #fig.savefig(DIRECTORY_PATH+\"/united_convex_hull.png\", dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Leadfield size : %d sensors x %d dipoles\" % leadfield.shape)\n",
    "    print(stc.data.shape)\n",
    "\n",
    "    information = {\"my_left_points\": my_left_points, \n",
    "                   \"my_right_points\": my_right_points, \n",
    "                   \"stc_data_shape\": stc.data.shape, \n",
    "                   \"leadfield\": leadfield,\n",
    "                   \"left_vertices\": stc.vertices[0],\n",
    "                   \"right_vertices\": stc.vertices[1],\n",
    "                   \"inverse_operator\": inverse_operator}\n",
    "\n",
    "    return information\n",
    "\n",
    "def EEGInception(num_class = 2, num_channel=3, num_samples=500):\n",
    "    Input_block = Input(shape = (num_channel, num_samples, 1))\n",
    "    drop_rate = 0.3\n",
    "    block1 = Conv2D(8, (1, 64), padding='same')(Input_block)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = Dropout(drop_rate)(block1)\n",
    "\n",
    "    block1 = DepthwiseConv2D((num_channel, 1), padding='valid', depth_multiplier = 2)(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = Dropout(drop_rate)(block1)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block2 = Conv2D(8, (1, 32), padding='same')(Input_block)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = Dropout(drop_rate)(block2)\n",
    "\n",
    "    block2 = DepthwiseConv2D((num_channel, 1), padding='valid', depth_multiplier = 2)(block2)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = Dropout(drop_rate)(block2)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block3 = Conv2D(8, (1, 16), padding='same')(Input_block)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "    block3 = Dropout(drop_rate)(block3)\n",
    "\n",
    "    block3 = DepthwiseConv2D((num_channel, 1), padding='valid', depth_multiplier = 2)(block3)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "    block3 = Dropout(drop_rate)(block3)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block = Concatenate(axis = -1)([block1, block2, block3])\n",
    "    block = AveragePooling2D((1, 4))(block)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block1_1 = Conv2D(8, (1, 16), padding='same')(block)\n",
    "    block1_1 = BatchNormalization()(block1_1)\n",
    "    block1_1 = Activation('elu')(block1_1)\n",
    "    block1_1 = Dropout(drop_rate)(block1_1)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block2_1 = Conv2D(8, (1, 8), padding='same')(block)\n",
    "    block2_1 = BatchNormalization()(block2_1)\n",
    "    block2_1 = Activation('elu')(block2_1)\n",
    "    block2_1 = Dropout(drop_rate)(block2_1)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block3_1 = Conv2D(8, (1, 4), padding='same')(block)\n",
    "    block3_1 = BatchNormalization()(block3_1)\n",
    "    block3_1 = Activation('elu')(block3_1)\n",
    "    block3_1 = Dropout(drop_rate)(block3_1)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block_new = Concatenate(axis = -1)([block1_1, block2_1, block3_1])\n",
    "    block_new = AveragePooling2D((1, 2))(block_new)\n",
    "\n",
    "    block_new = Conv2D(12, (1, 8), padding='same')(block_new)\n",
    "    block_new = BatchNormalization()(block_new)\n",
    "    block_new = Activation('elu')(block_new)\n",
    "    block_new = Dropout(drop_rate)(block_new)\n",
    "\n",
    "    block_new = AveragePooling2D((1, 2))(block_new)\n",
    "\n",
    "    block_new = Conv2D(6, (1, 4), padding='same')(block_new)\n",
    "    block_new = BatchNormalization()(block_new)\n",
    "    block_new = Activation('elu')(block_new)\n",
    "    block_new = Dropout(drop_rate)(block_new)\n",
    "\n",
    "    block_new = AveragePooling2D((1, 2))(block_new)\n",
    "\n",
    "    embedded = Flatten()(block_new)\n",
    "    \n",
    "    if num_class == 2:\n",
    "        out = Dense(1, activation = 'sigmoid')(embedded)\n",
    "    else:\n",
    "        out = Dense(num_class, activation = 'softmax')(embedded)\n",
    "        \n",
    "    return Model(inputs = Input_block, outputs = out)\n",
    "\n",
    "\"\"\"\n",
    "Total params: 699,685\n",
    "Trainable params: 0\n",
    "Non-trainable params: 699,685\n",
    "\"\"\"\n",
    "def create_model(num_class = 2, model_name=\"default\", num_channel=3):\n",
    "    if model_name == \"default\":\n",
    "        if num_class == 2:\n",
    "            model = tf.keras.models.Sequential([\n",
    "                Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\"),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\"),\n",
    "                Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\"),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\"),\n",
    "                Flatten(),\n",
    "                Dense(50, activation=\"selu\"),\n",
    "                Dense(1, activation=\"sigmoid\")\n",
    "            ])\n",
    "        else:\n",
    "            model = tf.keras.models.Sequential([\n",
    "                Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\"),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\"),\n",
    "                Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\"),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\"),\n",
    "                Flatten(),\n",
    "                Dense(50, activation=\"selu\"),\n",
    "                Dense(num_class, activation=\"softmax\")\n",
    "            ])\n",
    "    elif model_name == \"eegnet\":\n",
    "        if num_class == 2:\n",
    "            model = tf.keras.models.Sequential([\n",
    "                Conv2D(16, (1, 64), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter'),\n",
    "                BatchNormalization(),\n",
    "                DepthwiseConv2D((3, 1), use_bias = False, padding='valid', depth_multiplier = 2, activation = 'linear',\n",
    "                depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 4)),\n",
    "                Dropout(0.5),\n",
    "                SeparableConv2D(32, (1, 16), use_bias = False, activation = 'linear', padding = 'same'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 8)),\n",
    "                Dropout(0.5),\n",
    "                Flatten(),\n",
    "                Dense(1, activation = 'sigmoid', kernel_constraint = max_norm(0.25))\n",
    "            ])\n",
    "        else:\n",
    "            model = tf.keras.models.Sequential([\n",
    "                Conv2D(16, (1, 64), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter'),\n",
    "                BatchNormalization(),\n",
    "                DepthwiseConv2D((3, 1), use_bias = False, padding='valid', depth_multiplier = 2, activation = 'linear',\n",
    "                depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 4)),\n",
    "                Dropout(0.5),\n",
    "                SeparableConv2D(32, (1, 16), use_bias = False, activation = 'linear', padding = 'same'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 8)),\n",
    "                Dropout(0.5),\n",
    "                Flatten(),\n",
    "                Dense(num_class, activation = 'softmax', kernel_constraint = max_norm(0.25))\n",
    "            ])\n",
    "    elif model_name == \"eegnet_inception\":\n",
    "        model = EEGInception(num_class=num_class, num_channel=num_channel)\n",
    "\n",
    "    return model\n",
    "\n",
    "def stft_min_max(X, Y, debug=True):\n",
    "    Zxx = tf.signal.stft(X, frame_length=256, frame_step=16)\n",
    "    Zxx = tf.abs(Zxx)\n",
    "\n",
    "    if debug:\n",
    "      print(\"shape of X and Y: \" + str(X.shape) + \" \" + str(Y.shape))\n",
    "      print(\"shape of Zxx: \" + str(Zxx.shape))\n",
    "\n",
    "      # plot spectrogram\n",
    "      #samples = 0\n",
    "      #print(Y[samples])\n",
    "      #log_spec = tf.math.log(tf.transpose(Zxx[samples][0]))\n",
    "      #height = 40\n",
    "      #width = log_spec.shape[1]\n",
    "      #x_axis = tf.linspace(0, 2, num=width)\n",
    "      #y_axis = range(height)\n",
    "      #plt.pcolormesh(x_axis, y_axis, log_spec[:40, ])\n",
    "      #plt.title('Short Time Fourier Transform Magnitude')\n",
    "      #plt.ylabel('Frequency [Hz]')\n",
    "      #plt.xlabel('Time [sec]')\n",
    "      #plt.show()\n",
    "    \n",
    "    X = Zxx[:, :, :, :40]\n",
    "    X = tf.reshape(X, [X.shape[0], -1, 40])\n",
    "    X = tf.transpose(X, perm=[0, 2, 1])\n",
    "    X = tf.expand_dims(X, axis=3)\n",
    "    \n",
    "    # min max scaling (per instance)\n",
    "    original_shape = X.shape\n",
    "    X = tf.reshape(X, [original_shape[0], -1])\n",
    "    X_max = tf.math.reduce_max(X, axis=1, keepdims=True)\n",
    "    X_min = tf.math.reduce_min(X, axis=1, keepdims=True)\n",
    "    X = tf.math.divide(tf.math.subtract(X, X_min), tf.math.subtract(X_max, X_min))\n",
    "    X = tf.reshape(X, original_shape)\n",
    "    \n",
    "    if debug:\n",
    "      print(\"shape of X and Y: \" + str(X.shape) + \" \" + str(Y.shape))\n",
    "\n",
    "      # plot spectrogram\n",
    "      #samples = 0\n",
    "      #print(Y[samples])\n",
    "      #log_spec = tf.math.log(X[samples][:,:16,0])\n",
    "      #height = 40\n",
    "      #width = log_spec.shape[1]\n",
    "      #x_axis = tf.linspace(0, 2, num=width)\n",
    "      #y_axis = range(height)\n",
    "      #plt.pcolormesh(x_axis, y_axis, log_spec)\n",
    "      #plt.title('Short Time Fourier Transform Magnitude')\n",
    "      #plt.ylabel('Frequency [Hz]')\n",
    "      #plt.xlabel('Time [sec]')\n",
    "      #plt.show()\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801eb927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stft_Min_Max(keras.layers.Layer):\n",
    "    def __init__(self, num_of_channels):\n",
    "        super(Stft_Min_Max, self).__init__()\n",
    "        self.num_of_channels = num_of_channels\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Zxx = tf.signal.stft(inputs, frame_length=256, frame_step=16)\n",
    "        Zxx = tf.abs(Zxx)\n",
    "        X = Zxx[:, :, :, :40]\n",
    "        X = tf.reshape(X, [tf.shape(X)[0], -1, 40])\n",
    "        X = tf.transpose(X, perm=[0, 2, 1])\n",
    "        X = tf.expand_dims(X, axis=3)\n",
    "\n",
    "        # min max scaling (per instance)\n",
    "        #original_shape = tf.shape(X)\n",
    "        original_shape = [tf.shape(X)[0], 40, 16*self.num_of_channels, 1]\n",
    "        X = tf.reshape(X, [original_shape[0], -1])\n",
    "        X_max = tf.math.reduce_max(X, axis=1, keepdims=True)\n",
    "        X_min = tf.math.reduce_min(X, axis=1, keepdims=True)\n",
    "        X = tf.math.divide(tf.math.subtract(X, X_min), tf.math.subtract(X_max, X_min))\n",
    "        X = tf.reshape(X, original_shape)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "def my_mask(x):\n",
    "  return tf.cast(tf.greater_equal(x, 1), tf.float32)\n",
    "\n",
    "def diff_mask(mask_op):\n",
    "  @tf.custom_gradient\n",
    "  def _diff_mask(x):\n",
    "    def grad(dy):\n",
    "      return dy * tf.ones_like(x)\n",
    "    return mask_op(x), grad\n",
    "  return _diff_mask\n",
    "\n",
    "\"\"\"\n",
    "Total params: 64,404,027 / 42,089,607\n",
    "Trainable params: 64,404,003 / 42,089,607\n",
    "Non-trainable params: 24\n",
    "\"\"\"\n",
    "class AutoSelect(tf.keras.Model):\n",
    "    def __init__(self, select_channels, forward_matrix, random_select, use_mask, kqv, model_name=\"default\"):\n",
    "        super(AutoSelect, self).__init__()\n",
    "        # preprocessing\n",
    "        self.select_channels = select_channels\n",
    "        self.forward_matrix = tf.transpose(tf.constant(forward_matrix), perm=[1, 0])\n",
    "        self.concatenate = Concatenate(axis=1)\n",
    "        self.stft_min_max = Stft_Min_Max(len(select_channels))\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "        #self.tanh = tf.keras.layers.Activation('tanh')\n",
    "        #self.relu = tf.keras.layers.Activation('relu')\n",
    "        \n",
    "        # random select\n",
    "        self.random_select = random_select\n",
    "        \n",
    "        # model option\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # mask attention\n",
    "        self.use_mask = use_mask\n",
    "        self.mask = tf.Variable(np.ones((1, forward_matrix.shape[1])), dtype=tf.float32)\n",
    "        #self.mask = tf.Variable(np.random.rand(1, forward_matrix.shape[1])+0.5, dtype=tf.float32)\n",
    "        \n",
    "        # correlation attention\n",
    "        self.kqv = kqv\n",
    "        self.concatenate_1D = Concatenate(axis=1)\n",
    "        self.p1D_1 = MaxPool1D(pool_size=3, strides=1, padding='same', data_format='channels_first')\n",
    "        self.conv1D_1 = Conv1D(16, 5, padding=\"same\", activation=\"relu\", data_format='channels_first')\n",
    "        self.conv1D_2 = Conv1D(16, 10, padding=\"same\", activation=\"relu\", data_format='channels_first')\n",
    "        self.conv1D_3 = Conv1D(16, 25, padding=\"same\", activation=\"relu\", data_format='channels_first')\n",
    "        self.conv1D_4 = Conv1D(1, 5, padding=\"same\", activation=\"sigmoid\", data_format='channels_first')\n",
    "        \n",
    "        # dense attention\n",
    "        self.source_select = Dense(forward_matrix.shape[1], activation=None)\n",
    "        \n",
    "        # classifier\n",
    "        self.conv1 = Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\")\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.mp1 = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\")\n",
    "        self.conv2 = Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\")\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.mp2 = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\")\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(50, activation=\"selu\")\n",
    "        self.dense2 = Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "        # eegnet\n",
    "        if model_name == \"eegnet\":\n",
    "            self.eegnet_model = Sequential([\n",
    "                Conv2D(16, (1, 64), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter'),\n",
    "                BatchNormalization(),\n",
    "                DepthwiseConv2D((3, 1), use_bias = False, padding='valid', depth_multiplier = 2, activation = 'linear',\n",
    "                depthwise_constraint = max_norm(max_value=1), name = 'Spatial_filter'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 4)),\n",
    "                Dropout(0.5),\n",
    "                SeparableConv2D(32, (1, 16), use_bias = False, activation = 'linear', padding = 'same'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 8)),\n",
    "                Dropout(0.5),\n",
    "                Flatten(),\n",
    "                Dense(1, activation = 'sigmoid', kernel_constraint = max_norm(0.25))\n",
    "            ])\n",
    "            \n",
    "        # eeg-inception\n",
    "        if model_name == \"eegnet_inception\":\n",
    "            self.eegnet_inception_model = create_model(num_class = 2, model_name=model_name, num_channel=len(select_channels))\n",
    "\n",
    "    def call(self, inputs):                                                                         # (n, 7981/6433, 500)\n",
    "        # preprocessing\n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])                                                    # (n, 500, 7981/6433)\n",
    "            \n",
    "        if self.random_select:\n",
    "            x = self.dropout(x)                                                                     # (n, 500, 7981/6433)\n",
    "        else:\n",
    "            if self.use_mask:\n",
    "                #tf.print(self.mask)\n",
    "                mask = diff_mask(my_mask)(self.mask)                                                # (1, 7981/6433)\n",
    "                x = x * mask                                                                        # (n, 500, 7981/6433)                  \n",
    "            else:\n",
    "                if self.kqv:\n",
    "                    None\n",
    "                else:\n",
    "                    source_select = self.source_select(x)                                           # (n, 500, 7981/6433)\n",
    "                    #source_select = self.dropout(source_select)   \n",
    "                    source_select = self.sigmoid(source_select)\n",
    "                    x = x * source_select                                                           # (n, 500, 7981/6433)\n",
    "        \n",
    "        x = tf.matmul(x, self.forward_matrix)                                                       # (n, 500, 22)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])                                                         # (n, 22, 500)\n",
    "        x = tf.gather(x, indices=self.select_channels, axis=1)                                      # (n, chan_num, 500)\n",
    "        \n",
    "#         # Normalization\n",
    "#         x = tf.transpose(x, perm=[1, 0, 2])                                                         # (chan_num, n, 500)\n",
    "#         x_mean = tf.reduce_mean(x, axis=[1, 2], keepdims=True)                                      # (chan_num, 1, 1)\n",
    "#         x_std = tf.math.reduce_std(x, axis=[1, 2], keepdims=True)                                   # (chan_num, 1, 1)\n",
    "#         x = (x-x_mean)/x_std                                                                        # (chan_num, n, 500)\n",
    "#         x = tf.transpose(x, perm=[1, 0, 2])                                                         # (n, chan_num, 500)\n",
    "        \n",
    "        if self.model_name == \"default\":\n",
    "            x = self.stft_min_max(x)                                                                # (n, 40, 48, 1)\n",
    "            # classifier\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.mp1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.mp2(x)\n",
    "            x = self.flatten(x)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dense2(x)\n",
    "        elif self.model_name == \"eegnet\":\n",
    "            x = tf.expand_dims(x, axis=-1)                                                          # (n, chan_num, 500, 1)\n",
    "            x = self.eegnet_model(x)\n",
    "        elif self.model_name == \"eegnet_inception\":\n",
    "            x = tf.expand_dims(x, axis=-1)                                                          # (n, chan_num, 500, 1)\n",
    "            x = self.eegnet_inception_model(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\"\"\"\n",
    "Total params: 64,404,027 / 42,089,607\n",
    "Trainable params: 64,404,003 / 42,089,607\n",
    "Non-trainable params: 24\n",
    "\"\"\"\n",
    "class AutoSelectData(tf.keras.Model):\n",
    "    def __init__(self, select_channels, forward_matrix, random_select, use_mask, kqv, model_name=\"default\"):\n",
    "        super(AutoSelectData, self).__init__()\n",
    "        # preprocessing\n",
    "        self.select_channels = select_channels\n",
    "        self.forward_matrix = tf.transpose(tf.constant(forward_matrix), perm=[1, 0])\n",
    "        self.concatenate = Concatenate(axis=1)\n",
    "        self.stft_min_max = Stft_Min_Max(len(select_channels))\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "        #self.tanh = tf.keras.layers.Activation('tanh')\n",
    "        #self.relu = tf.keras.layers.Activation('relu')\n",
    "        \n",
    "        # random select\n",
    "        self.random_select = random_select\n",
    "        \n",
    "        # model option\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # mask attention\n",
    "        self.use_mask = use_mask\n",
    "        self.mask = tf.Variable(np.ones((1, forward_matrix.shape[1])), dtype=tf.float32)\n",
    "        #self.mask = tf.Variable(np.random.rand(1, forward_matrix.shape[1])+0.5, dtype=tf.float32)\n",
    "        \n",
    "        # correlation attention\n",
    "        self.kqv = kqv\n",
    "        self.concatenate_1D = Concatenate(axis=1)\n",
    "        self.p1D_1 = MaxPool1D(pool_size=3, strides=1, padding='same', data_format='channels_first')\n",
    "        self.conv1D_1 = Conv1D(16, 5, padding=\"same\", activation=\"relu\", data_format='channels_first')\n",
    "        self.conv1D_2 = Conv1D(16, 10, padding=\"same\", activation=\"relu\", data_format='channels_first')\n",
    "        self.conv1D_3 = Conv1D(16, 25, padding=\"same\", activation=\"relu\", data_format='channels_first')\n",
    "        self.conv1D_4 = Conv1D(1, 5, padding=\"same\", activation=\"sigmoid\", data_format='channels_first')\n",
    "        \n",
    "        # dense attention\n",
    "        self.source_select = Dense(forward_matrix.shape[1], activation=None)\n",
    "        \n",
    "        # classifier\n",
    "        self.conv1 = Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\")\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.mp1 = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\")\n",
    "        self.conv2 = Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=\"selu\")\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.mp2 = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"valid\")\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(50, activation=\"selu\")\n",
    "        self.dense2 = Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "        # eegnet\n",
    "        if model_name == \"eegnet\":\n",
    "            self.eegnet_model = Sequential([\n",
    "                Conv2D(16, (1, 64), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter'),\n",
    "                BatchNormalization(),\n",
    "                DepthwiseConv2D((3, 1), use_bias = False, padding='valid', depth_multiplier = 2, activation = 'linear',\n",
    "                depthwise_constraint = max_norm(max_value=1), name = 'Spatial_filter'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 4)),\n",
    "                Dropout(0.5),\n",
    "                SeparableConv2D(32, (1, 16), use_bias = False, activation = 'linear', padding = 'same'),\n",
    "                BatchNormalization(),\n",
    "                Activation('elu'),\n",
    "                AveragePooling2D((1, 8)),\n",
    "                Dropout(0.5),\n",
    "                Flatten(),\n",
    "                Dense(1, activation = 'sigmoid', kernel_constraint = max_norm(0.25))\n",
    "            ])\n",
    "\n",
    "    def call(self, inputs):                                                                         # (n, 7981/6433, 500)\n",
    "        # preprocessing\n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])                                                    # (n, 500, 7981/6433)\n",
    "            \n",
    "        if self.random_select:\n",
    "            x = self.dropout(x)                                                                     # (n, 500, 7981/6433)\n",
    "        else:\n",
    "            if self.use_mask:\n",
    "                #tf.print(self.mask)\n",
    "                mask = diff_mask(my_mask)(self.mask)                                                # (1, 7981/6433)\n",
    "                x = x * mask                                                                        # (n, 500, 7981/6433)                  \n",
    "            else:\n",
    "                if self.kqv:\n",
    "                    None\n",
    "                else:\n",
    "                    source_select = self.source_select(x)                                           # (n, 500, 7981/6433)\n",
    "                    #source_select = self.dropout(source_select)   \n",
    "                    source_select = self.sigmoid(source_select)\n",
    "                    x = x * source_select                                                           # (n, 500, 7981/6433)\n",
    "        \n",
    "        x = tf.matmul(x, self.forward_matrix)                                                       # (n, 500, 22)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])                                                         # (n, 22, 500)\n",
    "        x = tf.gather(x, indices=self.select_channels, axis=1)                                      # (n, chan_num, 500)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1131dd",
   "metadata": {},
   "source": [
    "## Masked Filter Bank Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create source activity from raw EEG signals for each subject\n",
    "Select the most relevant sources from the source space by applying the trainable layer\n",
    "Apply the forward matrix to obtain the EEG signals in channels space\n",
    "Apply bandpass filtering in specified frequency ranges and epoching the raw EEG signals\n",
    "\n",
    "Save the bandpass filtered files to disk\n",
    "\"\"\"\n",
    "def load_raw_subject(name=\"A01T.gdf\", dir='drive/Shareddrives/Motor Imagery/BCI competition IV dataset/2a/BCICIV_2a_gdf/', filter_bank=None, debug=None):\n",
    "  # Load data\n",
    "  raw = mne.io.read_raw_gdf(dir + name, preload=True)\n",
    "  # Rename channels\n",
    "  raw.rename_channels(channels_mapping)\n",
    "  # Set channels types\n",
    "  raw.set_channel_types(channels_type_mapping)\n",
    "  # Set montage\n",
    "  # Read and set the EEG electrode locations\n",
    "  ten_twenty_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "  raw.set_montage(ten_twenty_montage)\n",
    "  # Drop eog channels\n",
    "  raw.drop_channels([\"EOG-left\", \"EOG-central\", \"EOG-right\"])\n",
    "  # Set common average reference\n",
    "  raw.set_eeg_reference('average', projection=True, verbose=False)\n",
    "\n",
    "  return raw\n",
    "\n",
    "\n",
    "def apply_all_inverse_raw_bandpass_filter(dataset_path, epochs, subjects, filter_bank=None, save_filter=True):\n",
    "    information = get_inverse_and_forward_information(epochs)\n",
    "    print(information)\n",
    "\n",
    "    print(information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]].shape)\n",
    "    print(information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]].shape)\n",
    "    forward_matrix = np.concatenate((information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]], \n",
    "                              information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]]),\n",
    "                              axis = 1)\n",
    "    print(forward_matrix.shape)  \n",
    "    \n",
    "    del information\n",
    "    gc.collect()\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "      for file in files:\n",
    "        if file.split(\".\")[0][:3] in subjects:\n",
    "            print(\"current file:\", file)\n",
    "            raw = load_raw_subject(name=file, dir=root, filter_bank=filter_bank)\n",
    "            raw_list = apply_inverse_raw_bandpass_filter(raw, epochs[file.split(\".\")[0][:3]], forward_matrix, file.split(\".\")[0], save_filter=save_filter, events=[\"left\", \"right\"])\n",
    "\n",
    "def apply_inverse_raw_bandpass_filter(raw, epoch, forward_matrix, file_name, save_filter=True, events=[\"left\", \"right\"]):\n",
    "    global my_left_points, my_right_points\n",
    "    \n",
    "    data_name = file_name[:3]\n",
    "    \n",
    "    use_csp = True\n",
    "    if use_csp:\n",
    "        select_channels = list(range(22))\n",
    "    else:\n",
    "        select_channels = [7, 9, 11]\n",
    "    model_name = \"eegnet\"\n",
    "    random_select = False\n",
    "    use_mask = True\n",
    "    kqv = False\n",
    "\n",
    "    X, Y = [], []\n",
    "    info = None\n",
    "    counter = 0\n",
    "    for event in epoch.keys():\n",
    "        if info is None:\n",
    "            info = epoch[event].info\n",
    "        for i in range(len(events)):\n",
    "            if event == events[i]:\n",
    "                print(event)\n",
    "                if len(X) == 0:\n",
    "                    X = epoch[event].get_data()\n",
    "                    Y = np.zeros(len(epoch[event].get_data())) + i\n",
    "                else:\n",
    "                    X = np.append(X, epoch[event].get_data(), axis=0)\n",
    "                    Y = np.append(Y, np.zeros(len(epoch[event].get_data())) + i, axis=0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        counter += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        X_train = mne.EpochsArray(X_train, info, verbose=False)\n",
    "        X_test = mne.EpochsArray(X_test, info, verbose=False)\n",
    "\n",
    "        noise_cov = mne.compute_covariance(X_train, tmax=0., method=['shrunk', 'empirical'], rank=None, verbose=False)\n",
    "        fwd = mne.make_forward_solution(info, trans=trans, src=src,\n",
    "                        bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=1, verbose=False)\n",
    "        fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "                                     use_cps=True, verbose=False)\n",
    "        leadfield = fwd_fixed['sol']['data']\n",
    "        inverse_operator = make_inverse_operator(info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "\n",
    "        method = \"sLORETA\"\n",
    "        snr = 3.\n",
    "        lambda2 = 1. / snr ** 2\n",
    "        stc_raw = apply_inverse_raw(raw, inverse_operator, lambda2,\n",
    "                                      method=method, pick_ori=\"normal\", verbose=True)\n",
    "        stc_train = apply_inverse_epochs(X_train, inverse_operator, lambda2,\n",
    "                                      method=method, pick_ori=\"normal\", verbose=True)\n",
    "\n",
    "        # get motor region points (once)\n",
    "        if my_left_points is None and my_right_points is None:\n",
    "            my_source = stc_train[0]\n",
    "            mni_lh = mne.vertex_to_mni(my_source.vertices[0], 0, mne_subject)\n",
    "            print(mni_lh.shape)\n",
    "            mni_rh = mne.vertex_to_mni(my_source.vertices[1], 1, mne_subject)\n",
    "            print(mni_rh.shape)\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "            for selected_region in brodmann_motor:\n",
    "                ax.scatter(mm_coords.reshape(-1, 3)[selected_region][:, 0], mm_coords.reshape(-1, 3)[selected_region][:, 1], mm_coords.reshape(-1, 3)[selected_region][:, 2], s=15, marker='|')\n",
    "            ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='_')\n",
    "            ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "            plt.show()\n",
    "\n",
    "            my_left_points = None\n",
    "            my_right_points = None\n",
    "            for selected_region in brodmann_motor:\n",
    "                print(np.sum(selected_region))\n",
    "                if my_left_points is None:\n",
    "                    my_left_points = in_hull(mni_lh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "                    my_right_points = in_hull(mni_rh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "                else:\n",
    "                    my_left_points += in_hull(mni_lh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "                    my_right_points += in_hull(mni_rh, mm_coords.reshape(-1, 3)[selected_region])\n",
    "\n",
    "            mni_left_motor = mne.vertex_to_mni(my_source.vertices[0][my_left_points], 0, mne_subject)\n",
    "            print(mni_left_motor.shape)\n",
    "            mni_right_motor = mne.vertex_to_mni(my_source.vertices[1][my_right_points], 1, mne_subject)\n",
    "            print(mni_right_motor.shape)\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "            ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='|')\n",
    "            ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "            ax.scatter(mni_left_motor[:, 0], mni_left_motor[:, 1], mni_left_motor[:, 2], s=15, marker='o')\n",
    "            ax.scatter(mni_right_motor[:, 0], mni_right_motor[:, 1], mni_right_motor[:, 2], s=15, marker='^')\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "            plt.show()\n",
    "\n",
    "        print(\"Leadfield size : %d sensors x %d dipoles\" % leadfield.shape)\n",
    "        print(\"stc_train[0] shape: \", stc_train[0].data.shape)\n",
    "        \n",
    "        del stc_train\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"stc raw shape: \", stc_raw.data.shape)\n",
    "\n",
    "        # load pretrained model\n",
    "        model = AutoSelectData(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "        weight_path = \"D:/forward and inverse results (new)/motor/ml motor/eegnet 22 channels/models/\" + data_name + \"/\"\n",
    "\n",
    "        for weight_file in os.listdir(weight_path):\n",
    "          if weight_file.split(\"_\")[0] == str(counter):\n",
    "              break\n",
    "        load_weights_file = os.path.join(weight_path, weight_file) + \"/\"\n",
    "        model.load_weights(load_weights_file)\n",
    "        #model.build((None, 6433, 500))\n",
    "        #print(model.summary())\n",
    "        model.trainable = False\n",
    "        \n",
    "        mask_weight = np.moveaxis(model.mask.numpy(), [0, 1], [1, 0])\n",
    "        print(\"mask weight shape: \", mask_weight.shape)        \n",
    "        \n",
    "        left_hemi_data = stc_raw.data[:len(stc_raw.vertices[0])][my_left_points]\n",
    "        right_hemi_data = stc_raw.data[-len(stc_raw.vertices[1]):][my_right_points]\n",
    "        left_hemi_data = np.array(left_hemi_data)\n",
    "        right_hemi_data = np.array(right_hemi_data)\n",
    "        \n",
    "        motor_source = np.append(left_hemi_data, right_hemi_data, axis=0)\n",
    "        if save_filter:\n",
    "#             source_activity_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region filter\", \"raw\", \"source activity\", file_name)\n",
    "#             if not op.exists(source_activity_path):\n",
    "#                 os.makedirs(source_activity_path)\n",
    "#             np.savez_compressed(op.join(source_activity_path, str(counter)+\".npz\"), data=np.array(motor_source), info=raw.info)\n",
    "            \n",
    "            reconstructed_eeg_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region filter\", \"raw\", \"reconstructed eeg\", file_name)\n",
    "            if not op.exists(reconstructed_eeg_path):\n",
    "                os.makedirs(reconstructed_eeg_path)\n",
    "            motor_eeg = np.dot(forward_matrix, motor_source)\n",
    "            np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\".npz\"), data=np.array(motor_eeg), info=raw.info)\n",
    "        \n",
    "        motor_source = motor_source * mask_weight\n",
    "        if save_filter:\n",
    "#             source_activity_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region filter\", \"masked raw\", \"source activity\", file_name)\n",
    "#             if not op.exists(source_activity_path):\n",
    "#                 os.makedirs(source_activity_path)\n",
    "#             np.savez_compressed(op.join(source_activity_path, str(counter)+\".npz\"), data=np.array(motor_source), info=raw.info)\n",
    "            \n",
    "            reconstructed_eeg_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region filter\", \"masked raw\", \"reconstructed eeg\", file_name)\n",
    "            if not op.exists(reconstructed_eeg_path):\n",
    "                os.makedirs(reconstructed_eeg_path)\n",
    "            motor_eeg = np.dot(forward_matrix, motor_source)\n",
    "            print(\"motor eeg shape: \", motor_eeg.shape)\n",
    "            np.savez_compressed(op.join(reconstructed_eeg_path, str(counter)+\".npz\"), data=np.array(motor_eeg), info=raw.info)\n",
    "        \n",
    "        del stc_raw\n",
    "        del motor_source\n",
    "        gc.collect()\n",
    "        \n",
    "def create_epochs_filter(data):\n",
    "  subjects_data = {}\n",
    "\n",
    "  for subject in data.keys():\n",
    "    if \"E\" in subject:\n",
    "        continue\n",
    "    epochs_data = {}\n",
    "    for counter in data[subject].keys():\n",
    "      epochs_data[counter] = {}\n",
    "      for event in data[subject][counter][\"original\"][\"epoch_data\"].keys():\n",
    "        current_event_data = None\n",
    "      \n",
    "        if data[subject][counter][\"original\"][\"epoch_data\"][event].any():\n",
    "          current_event_data = data[subject][counter][\"original\"][\"epoch_data\"][event]\n",
    "        if data[subject[:3]+\"E.gdf\"][counter][\"original\"][\"epoch_data\"][event].any():\n",
    "          current_event_data = np.append(current_event_data, data[subject[:3]+\"E.gdf\"][counter][\"original\"][\"epoch_data\"][event], axis=0)\n",
    "        if current_event_data is not None:\n",
    "          epochs_data[counter][event] = mne.EpochsArray(current_event_data, data[subject][counter][\"original\"][\"info\"], verbose=False)\n",
    "\n",
    "    subjects_data[subject[:3]] = epochs_data\n",
    "    \n",
    "  subjects_filter_data = {}\n",
    "  \n",
    "  for subject in data.keys():\n",
    "    if \"E\" in subject:\n",
    "        continue\n",
    "    epochs_data = {}\n",
    "    for counter in data[subject].keys():\n",
    "      epochs_data[counter] = {}\n",
    "      for freq in data[subject][counter].keys():\n",
    "          if freq != \"original\":\n",
    "              #print(freq.split(\"-\"))\n",
    "              epochs_data[counter][freq] = {}\n",
    "        \n",
    "              for event in data[subject][counter][freq][\"epoch_data\"].keys():\n",
    "                  current_event_data = None\n",
    "\n",
    "                  if data[subject][counter][freq][\"epoch_data\"][event].any():\n",
    "                      current_event_data = data[subject][counter][freq][\"epoch_data\"][event]\n",
    "                  if data[subject[:3]+\"E.gdf\"][counter][freq][\"epoch_data\"][event].any():\n",
    "                      current_event_data = np.append(current_event_data, data[subject[:3]+\"E.gdf\"][counter][freq][\"epoch_data\"][event], axis=0)\n",
    "                  if current_event_data is not None:\n",
    "                      epochs_data[counter][freq][event] = mne.EpochsArray(current_event_data, data[subject][counter][freq][\"info\"], verbose=False)\n",
    "                \n",
    "              subjects_filter_data[subject[:3]] = epochs_data\n",
    "            \n",
    "  return subjects_data, subjects_filter_data\n",
    "    \n",
    "def load_epoch_data_filter(raw, masked_raw, name, debug=None):\n",
    "  if debug:\n",
    "    raw.plot()\n",
    "  subject_data = {}\n",
    "\n",
    "  subject_data[\"raw\"] = raw\n",
    "  subject_data[\"info\"] = raw.info\n",
    "  if debug == \"all\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for key, item in raw.info.items():\n",
    "      print(key, item)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  \"\"\"\n",
    "  '276': 'Idling EEG (eyes open)'\n",
    "  '277': 'Idling EEG (eyes closed)'\n",
    "  '768': 'Start of a trial'\n",
    "  '769': 'Cue onset left (class 1)'\n",
    "  '770': 'Cue onset right (class 2)'\n",
    "  '771': 'Cue onset foot (class 3)'\n",
    "  '772': 'Cue onset tongue (class 4)'\n",
    "  '783': 'Cue unknown'\n",
    "  '1023': 'Rejected trial'\n",
    "  '1072': 'Eye movements'\n",
    "  '32766': 'Start of a new run'\n",
    "  \"\"\"\n",
    "  custom_mapping = {'276': 276, '277': 277, '768': 768, '769': 769, '770': 770, '771': 771, '772': 772, '783': 783, '1023': 1023, '1072': 1072, '32766': 32766}\n",
    "  events_from_annot, event_dict = mne.events_from_annotations(raw, event_id=custom_mapping)\n",
    "\n",
    "  if debug == \" all\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(event_dict)\n",
    "    print(events_from_annot)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for i in range(len(raw.annotations)):\n",
    "      print(events_from_annot[i], raw.annotations[i])  \n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  class_info = \"Idling EEG (eyes open): \" + str(len(events_from_annot[events_from_annot[:, 2]==276][:, 0])) + \"\\n\" + \\\n",
    "               \"Idling EEG (eyes closed): \" + str(len(events_from_annot[events_from_annot[:, 2]==277][:, 0])) + \"\\n\" + \\\n",
    "               \"Start of a trial: \" + str(len(events_from_annot[events_from_annot[:, 2]==768][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset left (class 1): \" + str(len(events_from_annot[events_from_annot[:, 2]==769][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset right (class 2): \" + str(len(events_from_annot[events_from_annot[:, 2]==770][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset foot (class 3): \" + str(len(events_from_annot[events_from_annot[:, 2]==771][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue onset tongue (class 4): \" + str(len(events_from_annot[events_from_annot[:, 2]==772][:, 0])) + \"\\n\" + \\\n",
    "               \"Cue unknown: \" + str(len(events_from_annot[events_from_annot[:, 2]==783][:, 0])) + \"\\n\" + \\\n",
    "               \"Rejected trial: \" + str(len(events_from_annot[events_from_annot[:, 2]==1023][:, 0])) + \"\\n\" + \\\n",
    "               \"Eye movements: \" + str(len(events_from_annot[events_from_annot[:, 2]==1072][:, 0])) + \"\\n\" + \\\n",
    "               \"Start of a new run: \" + str(len(events_from_annot[events_from_annot[:, 2]==32766][:, 0]))\n",
    "  subject_data[\"class_info\"] = class_info\n",
    "\n",
    "  if debug == \"all\" or debug == \"important\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(class_info)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  epoch_data = {\"left\": [], \"right\": [], \"foot\": [], \"tongue\": [], \"unknown\": []}\n",
    "  rejected_trial = events_from_annot[events_from_annot[:, 2]==1023][:, 0]\n",
    "  class_dict = {\"left\": 769, \"right\": 770, \"foot\": 771, \"tongue\": 772, \"unknown\": 783}\n",
    "  raw_data = masked_raw.get_data()  #(22, 672528)\n",
    "  start = 10                 # cue+0.1s\n",
    "  stop = 510                 # cue+2.1s\n",
    "\n",
    "  for event_class, event_id in class_dict.items():\n",
    "    current_event = events_from_annot[events_from_annot[:, 2]==event_id][:, 0]\n",
    "    if event_class == \"unknown\":\n",
    "      subject_true_labels = true_labels[name[:4]+\".mat\"]\n",
    "      class_dict_labels = {1: \"left\", 2: \"right\", 3: \"foot\", 4: \"tongue\"}\n",
    "      for i in range(len(current_event)):\n",
    "        # exclude artifact\n",
    "        if (current_event[i] - 500 != rejected_trial).all():\n",
    "          current_event_data = np.expand_dims(np.array(raw_data[:22, current_event[i]+start:current_event[i]+stop]), axis=0)\n",
    "          if (epoch_data.get(class_dict_labels[subject_true_labels[i]]) == None).all():\n",
    "            epoch_data[class_dict_labels[subject_true_labels[i]]] = current_event_data\n",
    "          else:\n",
    "            epoch_data[class_dict_labels[subject_true_labels[i]]] = np.append(epoch_data[class_dict_labels[subject_true_labels[i]]], current_event_data, axis=0)\n",
    "    else:\n",
    "      for i in range(len(current_event)):\n",
    "        # exclude artifact\n",
    "        if((current_event[i] - 500 != rejected_trial).all()):\n",
    "          epoch_data[event_class].append(np.array(raw_data[:22, current_event[i]+start:current_event[i]+stop]))\n",
    "      epoch_data[event_class] = np.array(epoch_data[event_class])\n",
    "\n",
    "  if debug == \"all\" or debug == \"important\":\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    for key, data in epoch_data.items():\n",
    "      print(key, len(data))\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "  for event_class, event_data in epoch_data.items():\n",
    "    epoch_data[event_class] = np.array(event_data)\n",
    "\n",
    "  subject_data[\"epoch_data\"] = epoch_data\n",
    "\n",
    "  return subject_data\n",
    "    \n",
    "def load_subject_filter(name=\"A01T.gdf\", dir='drive/Shareddrives/Motor Imagery/BCI competition IV dataset/2a/BCICIV_2a_gdf/', filter_bank=None, debug=None):    \n",
    "  raw = load_raw_subject(name, dir, filter_bank, debug)\n",
    "  \n",
    "  directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region filter\", \"masked raw\", \"reconstructed eeg\")\n",
    "\n",
    "  masked_raw = {}\n",
    "\n",
    "  for subject_folder in os.listdir(directory_path):\n",
    "    if name.split(\".\")[0] == subject_folder:\n",
    "        print(name, subject_folder)\n",
    "        \n",
    "        for subject_file in os.listdir(op.join(directory_path, subject_folder)):\n",
    "            masked_eeg_data = np.load(op.join(directory_path, subject_folder, subject_file), allow_pickle=True)[\"data\"]\n",
    "            masked_raw[subject_file.split(\".\")[0]] = mne.io.RawArray(masked_eeg_data, raw.info)\n",
    "        break\n",
    "    \n",
    "  subject_data_dict = {}\n",
    "\n",
    "  if filter_bank is None:\n",
    "    filter_bank = [-1]\n",
    "  else:\n",
    "    filter_bank = [-1] + filter_bank\n",
    "  print(\"filter bank: \", filter_bank)\n",
    "  \n",
    "  for counter, masked_raw_i in masked_raw.items():\n",
    "    print(counter)\n",
    "    subject_data_dict[counter] = {}\n",
    "      \n",
    "    for i in range(len(filter_bank)-1):\n",
    "        low = filter_bank[i]\n",
    "        high = filter_bank[i+1]\n",
    "        print(\"current frequency: \", low, \" to \", high)\n",
    "        # filter frequency\n",
    "        masked_filter_raw = masked_raw_i.copy()\n",
    "        if low != -1:\n",
    "          iir_params = dict(order=5, ftype='butter')\n",
    "          masked_filter_raw.filter(low, high, method=\"iir\", iir_params=iir_params)\n",
    "        subject_data = load_epoch_data_filter(raw, masked_filter_raw, name, debug)\n",
    "        if low == -1:\n",
    "          subject_data_dict[counter][\"original\"] = subject_data\n",
    "        else:\n",
    "          subject_data_dict[counter][str(low)+\"-\"+str(high)] = subject_data\n",
    "\n",
    "  return subject_data_dict\n",
    "    \n",
    "def load_all_subject_filter(dataset_path, filter_bank=None):\n",
    "  data = {}\n",
    "  for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "      data[file] = load_subject_filter(name=file, dir=root, filter_bank=filter_bank) \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd to google drive\n",
    "os.chdir(\"G:\")\n",
    "\n",
    "# Download fsaverage files\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = op.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "mne_subject = 'fsaverage'\n",
    "trans = 'fsaverage'  # MNE has a built-in fsaverage transformation\n",
    "src = op.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\n",
    "bem = op.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\n",
    "\n",
    "source = mne.read_source_spaces(src)\n",
    "left = source[0]\n",
    "right = source[1]\n",
    "left_pos = left[\"rr\"][left[\"inuse\"]==1]\n",
    "right_pos = right[\"rr\"][right[\"inuse\"]==1]\n",
    "                        \n",
    "transformation = mne.read_trans(op.join(fs_dir, \"bem\", \"fsaverage-trans.fif\"))\n",
    "\n",
    "save_path = op.join(os.getcwd(), \"Shared drives\", \"Motor Imagery\", \"Source Estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut = 4\n",
    "highcut = 40\n",
    "interval = 4\n",
    "filter_bank = list(np.arange(lowcut, highcut+interval, step=interval))\n",
    "print(\"filter bank: \", filter_bank)\n",
    "\n",
    "true_labels_path = \"Shared drives/Motor Imagery/BCI competition IV dataset/2a/2a true_labels/\"\n",
    "true_labels = load_all_true_labels(true_labels_path)\n",
    "\n",
    "dataset_path = 'Shared drives/Motor Imagery/BCI competition IV dataset/2a/BCICIV_2a_gdf/'\n",
    "data = load_all_subject(dataset_path, filter_bank=filter_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c296e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some information to help to understand functions and data structure\n",
    "# for key, item in data.items():\n",
    "#   print(key)\n",
    "\n",
    "# ch_names = data[\"A01T.gdf\"][\"info\"][\"ch_names\"]\n",
    "# print(ch_names)\n",
    "\n",
    "# print(data[\"A01T.gdf\"][\"class_info\"])\n",
    "\n",
    "# for key, value in data.items():\n",
    "#   print(key)\n",
    "#   for event_class, event_data in value[\"epoch_data\"].items():\n",
    "#       print(event_class, len(event_data))\n",
    "#   print()\n",
    "\n",
    "# subject_name = \"A01T.gdf\"\n",
    "# Class = \"left\"\n",
    "# filter_channels = [\"C3\", \"Cz\", \"C4\"]\n",
    "# plot_average_graph(subject_name, Class, filter_channels)\n",
    "\n",
    "# subject_name = \"A02T.gdf\"\n",
    "# classes = [\"left\", \"right\"]\n",
    "# filter_channels = [\"C3\", \"Cz\", \"C4\"]\n",
    "# plot_multiple_graph(subject_name, classes, filter_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2e51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs, epochs_filter_bank = create_epochs(data)\n",
    "#apply_inverse_and_forward_kfold(epochs, n_splits=n_splits, save_inverse=True, save_forward=True)\n",
    "print(epochs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_all_inverse_raw_bandpass_filter(dataset_path, epochs, [\"A10\"], filter_bank=filter_bank, save_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16154b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut = 4\n",
    "highcut = 40\n",
    "interval = 4\n",
    "filter_bank = list(np.arange(lowcut, highcut+interval, step=interval))\n",
    "print(\"filter bank: \", filter_bank)\n",
    "\n",
    "true_labels_path = \"Shared drives/Motor Imagery/BCI competition IV dataset/2a/2a true_labels/\"\n",
    "true_labels = load_all_true_labels(true_labels_path)\n",
    "\n",
    "dataset_path = 'Shared drives/Motor Imagery/BCI competition IV dataset/2a/BCICIV_2a_gdf/'\n",
    "data = load_all_subject_filter(dataset_path, filter_bank=filter_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, epochs_filter_bank = create_epochs_filter(data)\n",
    "#apply_inverse_and_forward_kfold(epochs, n_splits=n_splits, save_inverse=True, save_forward=True)\n",
    "print(epochs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f633036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # some information to help to understand functions and data structure\n",
    "# my_epochs = epochs[\"A01\"][\"right\"]\n",
    "# my_evoked = my_epochs.average().pick(\"eeg\")\n",
    "\n",
    "# noise_cov = mne.compute_covariance(my_epochs, tmax=0., method=['shrunk', 'empirical'], rank=None, verbose=False)\n",
    "# fwd = mne.make_forward_solution(my_epochs.info, trans=trans, src=src,\n",
    "#                             bem=bem, eeg=True, meg=False, mindist=5.0, n_jobs=1)\n",
    "# # forward matrix\n",
    "# fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "#                                          use_cps=True)\n",
    "\n",
    "# inverse_operator = make_inverse_operator(\n",
    "#     my_epochs.info, fwd, noise_cov, loose=0.2, depth=0.8)\n",
    "\n",
    "# method = \"sLORETA\"\n",
    "# snr = 3.\n",
    "# lambda2 = 1. / snr ** 2\n",
    "# stc = mne.minimum_norm.apply_inverse(my_evoked, inverse_operator, lambda2,\n",
    "#                               method=method, pick_ori=\"normal\", verbose=True)\n",
    "# stc.plot(hemi='both', background='white')\n",
    "\n",
    "# reconstruct_evoked = mne.apply_forward(fwd_fixed, stc, my_evoked.info)\n",
    "# my_evoked.plot_topomap()\n",
    "# reconstruct_evoked.plot_topomap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54e8bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load gpt2 generated data\n",
    "# fake_data_directory_path = '/standard_generate_dataset/finetune/gpt2xcnn/var_5.0_random_size_0.4.json'\n",
    "# f = open(DIRECTORY_PATH + fake_data_directory_path)\n",
    "# generate_data = json.load(f)\n",
    "# X_fake = []\n",
    "# Y_fake = []\n",
    " \n",
    "# # Iterating through the json\n",
    "# # list\n",
    "# for key, value in generate_data.items():\n",
    "#     X_fake.append(np.array(value))\n",
    "#     if int(key) < 125:\n",
    "#         Y_fake.append(np.zeros(1))\n",
    "#     else:\n",
    "#         Y_fake.append(np.ones(1))\n",
    "# f.close()\n",
    "\n",
    "# del generate_data\n",
    "# gc.collect()\n",
    "\n",
    "# X_fake = np.array(X_fake)\n",
    "# Y_fake = np.array(Y_fake).reshape(-1)\n",
    "\n",
    "# print(X_fake.shape)\n",
    "# print(Y_fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca89bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize fake data and compare with real data\n",
    "# times = np.linspace(0, 1.75, 10)\n",
    "# info = epochs[\"A05\"][\"left\"].info\n",
    "# print(fake_data_directory_path)\n",
    "# print(\"fake left\")\n",
    "# fake_left_evoked = mne.EpochsArray(X_fake[Y_fake==0], info, verbose=False).average().pick(\"eeg\")\n",
    "# #fake_left_evoked = mne.EpochsArray(np.expand_dims(X_fake[Y_fake==0][0], axis=0), info, verbose=False).average().pick(\"eeg\")\n",
    "# fake_left_evoked.plot_topomap(times=times)\n",
    "# #fake_left_evoked.plot()\n",
    "# print(\"fake right\")\n",
    "# fake_right_evoked = mne.EpochsArray(X_fake[Y_fake==1], info, verbose=False).average().pick(\"eeg\")\n",
    "# #fake_right_evoked = mne.EpochsArray(np.expand_dims(X_fake[Y_fake==1][0], axis=0), info, verbose=False).average().pick(\"eeg\")\n",
    "# fake_right_evoked.plot_topomap(times=times)\n",
    "# #fake_right_evoked.plot()\n",
    "\n",
    "# real_train_X = np.load(op.join(op.join(EXTERNAL_STORAGE_PATH, \"primary motor region\", \"data\", \"reconstructed eeg\", \"A05\"), \"1_train_X.npz\"), allow_pickle=True)[\"data\"]\n",
    "# real_train_Y = np.load(op.join(op.join(EXTERNAL_STORAGE_PATH, \"primary motor region\", \"data\", \"reconstructed eeg\", \"A05\"), \"1_train_Y.npz\"), allow_pickle=True)[\"data\"]\n",
    "# print(\"real left\")\n",
    "# real_left_evoked = mne.EpochsArray(real_train_X[real_train_Y==0], info, verbose=False).average().pick(\"eeg\")\n",
    "# real_left_evoked.plot_topomap(times=times)\n",
    "# #real_left_evoked.plot()\n",
    "# print(\"real right\")\n",
    "# real_right_evoked = mne.EpochsArray(real_train_X[real_train_Y==1], info, verbose=False).average().pick(\"eeg\")\n",
    "# real_right_evoked.plot_topomap(times=times)\n",
    "# #real_right_evoked.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346696c",
   "metadata": {},
   "source": [
    "# CNN Classification (original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "events = [\"left\", \"right\"]\n",
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "classes = 2\n",
    "debug = True\n",
    "training = True\n",
    "model_name = \"eegnet_inception\"\n",
    "num_epochs = 200\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A05\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "    \n",
    "  X, Y = [], []\n",
    "  for event in epochs[data_name].keys():\n",
    "    for i in range(len(events)):\n",
    "      if event == events[i]:\n",
    "        if len(X) == 0:\n",
    "          X = epochs[data_name][event].get_data()\n",
    "          Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "        else:\n",
    "          X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "          Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y)\n",
    "    \n",
    "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "  for train_index, test_index in skf.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # add fake data\n",
    "    # (1) real + fake\n",
    "#     X_train = np.append(X_train, X_fake, axis=0)\n",
    "#     Y_train = np.append(Y_train, Y_fake, axis=0)\n",
    "    # (2) fake\n",
    "#     X_train = X_fake\n",
    "#     Y_train = Y_fake\n",
    "    \n",
    "    # pick c3, cZ, c4 channels\n",
    "    X_train = X_train[:, select_channels, :]\n",
    "    X_test = X_test[:, select_channels, :]\n",
    "    \n",
    "#     # Normalization\n",
    "#     X_train_temp = np.zeros(X_train.shape)\n",
    "#     for i in range(X_train.shape[1]):\n",
    "#         temp = X_train[:, i, :]\n",
    "#         X_train_temp[:, i, :] = (temp-np.mean(temp))/np.std(temp)\n",
    "#     X_test_temp = np.zeros(X_test.shape)\n",
    "#     for i in range(X_test.shape[1]):\n",
    "#         temp = X_test[:, i, :]\n",
    "#         X_test_temp[:, i, :] = (temp-np.mean(temp))/np.std(temp)\n",
    "#     X_train = np.array(X_train_temp)\n",
    "#     X_test = np.array(X_test_temp)\n",
    "\n",
    "    print(data_name)\n",
    "    if model_name == \"default\":\n",
    "        X_train, Y_train = stft_min_max(X_train, Y_train, debug)\n",
    "        X_test, Y_test = stft_min_max(X_test, Y_test, debug)\n",
    "    elif model_name == \"eegnet\":\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "    elif model_name == \"eegnet_inception\":\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "    \n",
    "    if debug:\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    if training:\n",
    "      # create new model\n",
    "      model = create_model(num_class=classes, model_name=model_name, num_channel=len(select_channels))\n",
    "        \n",
    "      log_dir = DIRECTORY_PATH + \"/motor/logs/\" + data_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      if classes == 2:\n",
    "          model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      else:\n",
    "          model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "      model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=num_epochs, callbacks=[tensorboard_callback], verbose=0)\n",
    "\n",
    "      Y_hat = model.predict(X_test)\n",
    "      if classes == 2:\n",
    "          Y_hat = (Y_hat >= 0.5)\n",
    "      else:\n",
    "          Y_hat = np.argmax(Y_hat, axis=1)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "    \n",
    "      # save model\n",
    "      model.save_weights(DIRECTORY_PATH + \"/motor/models/\" + data_name + \"_\" + str(accuracy_score(Y_test, Y_hat))[:6] + \"/\")\n",
    "    else:\n",
    "      # load pretrained model\n",
    "      model = create_model(num_class=classes, model_name=model_name, num_channel=len(select_channels))\n",
    "      model.load_weights(DIRECTORY_PATH + \"/motor/models/\" + \"A05_0.7547/\")\n",
    "      # freeze model\n",
    "      model.trainable = False\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      if classes == 2:\n",
    "          model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      else:\n",
    "          model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      if classes == 2:\n",
    "          Y_hat = (Y_hat >= 0.5)\n",
    "      else:\n",
    "          Y_hat = np.argmax(Y_hat, axis=1)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "    \n",
    "  accuracy /= n_splits\n",
    "  precision /= n_splits\n",
    "  recall /= n_splits\n",
    "  f1 /= n_splits\n",
    "  kappa /= n_splits\n",
    "\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"kappa: \" + str(kappa))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0d309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "result_accuracy = []\n",
    "result_precision = []\n",
    "result_recall = []\n",
    "result_f1 = []\n",
    "result_kappa = []\n",
    "for key, value in results.items():\n",
    "  result_accuracy += [value[\"accuracy\"]]\n",
    "  result_precision += [value[\"precision\"]]\n",
    "  result_recall += [value[\"recall\"]]\n",
    "  result_f1 += [value[\"f1\"]]\n",
    "  result_kappa += [value[\"kappa\"]]\n",
    "\n",
    "print(\"accuracy: (mean) \" + str(np.mean(result_accuracy)) + \" (std) \" + str(np.std(result_accuracy)))\n",
    "print(\"precision: (mean) \" + str(np.mean(result_precision)) + \" (std) \" + str(np.std(result_precision)))\n",
    "print(\"recall: (mean) \" + str(np.mean(result_recall)) + \" (std) \" + str(np.std(result_recall)))\n",
    "print(\"f1: (mean) \" + str(np.mean(result_f1)) + \" (std) \" + str(np.std(result_f1)))\n",
    "print(\"kappa: (mean) \" + str(np.mean(result_kappa)) + \" (std) \" + str(np.std(result_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec04023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time computation\n",
    "events = [\"left\", \"right\"]\n",
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "classes = 2\n",
    "debug = False\n",
    "model_name = \"default\"\n",
    "warm_up = 10 # initializing memory allocators, and GPU-related initializations \n",
    "\n",
    "data_list = []\n",
    "data_list.append(\"A01\")\n",
    "\n",
    "for data_name in data_list:\n",
    "    \n",
    "  X, Y = [], []\n",
    "  for event in epochs[data_name].keys():\n",
    "    for i in range(len(events)):\n",
    "      if event == events[i]:\n",
    "        if len(X) == 0:\n",
    "          X = epochs[data_name][event].get_data()\n",
    "          Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "        else:\n",
    "          X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "          Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y) \n",
    "  X = X[:, select_channels, :]\n",
    "\n",
    "#   # Normalization\n",
    "#   X_temp = np.zeros(X.shape)\n",
    "#   for i in range(X.shape[1]):\n",
    "#     temp = X[:, i, :]\n",
    "#     X_temp[:, i, :] = (temp-np.mean(temp))/np.std(temp)\n",
    "#   X = np.array(X_temp)\n",
    "    \n",
    "  X_test = np.expand_dims(X[0], axis=0)\n",
    "  Y_test = np.expand_dims(Y[0], axis=0)\n",
    "    \n",
    "  # load pretrained model\n",
    "  model = create_model(num_class=classes, model_name=model_name, num_channel=len(select_channels))\n",
    "  model.load_weights(\"D:/forward and inverse results (new)/motor/default 22 channels/original EEG/models/A01_0.8571/\")\n",
    "  # freeze model\n",
    "  model.trainable = False\n",
    "  optimizer = Adam(learning_rate=1e-5)\n",
    "  if classes == 2:\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "  else:\n",
    "      model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "  if model_name == \"default\":\n",
    "      model.build(input_shape=(None, 40, 16*len(select_channels), 1))\n",
    "  elif model_name == \"eegnet\":\n",
    "      model.build(input_shape=(None, len(select_channels), 500, 1))\n",
    "  elif model_name == \"eegnet_inception\":\n",
    "      model.build(input_shape=(None, len(select_channels), 500, 1))\n",
    "  print(model.summary())\n",
    "\n",
    "  for i in range(warm_up):\n",
    "    if i == warm_up-1:\n",
    "        start = time.time()\n",
    "    if model_name == \"default\":\n",
    "      X_time, Y_time = stft_min_max(X_test, Y_test, debug)\n",
    "    elif model_name == \"eegnet\":\n",
    "      X_time = np.expand_dims(X_test, axis=-1)\n",
    "    elif model_name == \"eegnet_inception\":\n",
    "      X_time = np.expand_dims(X_test, axis=-1)\n",
    "    print(X_time.shape)\n",
    "    \n",
    "    Y_hat = model.predict(X_time)\n",
    "\n",
    "    if i == warm_up-1:\n",
    "        end = time.time()\n",
    "        print(\"time used: \", (end - start)*1000, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e059cb",
   "metadata": {},
   "source": [
    "# CNN Classification (reconstructed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8b544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "classes = 2\n",
    "debug = True\n",
    "training = True\n",
    "model_name = \"eegnet\"\n",
    "num_epochs = 200\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A02\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  # load data from external storage\n",
    "  directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"reconstructed eeg\", data_name)\n",
    "  counter = 0\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "  while(counter < n_splits):\n",
    "    counter += 1\n",
    "    X_train = np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"]\n",
    "    X_test = np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"]\n",
    "    Y_train = np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"]\n",
    "    Y_test = np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"]\n",
    "    \n",
    "    #X_train, _, Y_train, _ = train_test_split(X_fake, Y_fake, test_size=0.8, random_state=456, stratify=Y_fake)\n",
    "    \n",
    "    # pick c3, cZ, c4 channels\n",
    "    X_train = X_train[:, select_channels, :]\n",
    "    X_test = X_test[:, select_channels, :]\n",
    "    \n",
    "#     # Normalization\n",
    "#     X_train_temp = np.zeros(X_train.shape)\n",
    "#     for i in range(X_train.shape[1]):\n",
    "#         temp = X_train[:, i, :]\n",
    "#         X_train_temp[:, i, :] = (temp-np.mean(temp))/np.std(temp)\n",
    "#     X_test_temp = np.zeros(X_test.shape)\n",
    "#     for i in range(X_test.shape[1]):\n",
    "#         temp = X_test[:, i, :]\n",
    "#         X_test_temp[:, i, :] = (temp-np.mean(temp))/np.std(temp)\n",
    "#     X_train = np.array(X_train_temp)\n",
    "#     X_test = np.array(X_test_temp)\n",
    "\n",
    "    print(data_name)\n",
    "    if model_name == \"default\":\n",
    "        X_train, Y_train = stft_min_max(X_train, Y_train, debug)\n",
    "        X_test, Y_test = stft_min_max(X_test, Y_test, debug)\n",
    "    elif model_name == \"eegnet\":\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "    elif model_name == \"eegnet_inception\":\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    if debug:\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    if training:\n",
    "      # create new model\n",
    "      model = create_model(num_class=classes, model_name=model_name, num_channel=len(select_channels))\n",
    "      \n",
    "      log_dir = DIRECTORY_PATH + \"/motor/logs/\" + data_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      if classes == 2:\n",
    "          model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      else:\n",
    "          model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "      model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=num_epochs, callbacks=[tensorboard_callback], verbose=0)\n",
    "\n",
    "      Y_hat = model.predict(X_test)\n",
    "      if classes == 2:\n",
    "          Y_hat = (Y_hat >= 0.5)\n",
    "      else:\n",
    "          Y_hat = np.argmax(Y_hat, axis=1)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "      # save model\n",
    "      model.save_weights(DIRECTORY_PATH + \"/motor/models/\" + data_name + \"_\" + str(accuracy_score(Y_test, Y_hat))[:6] + \"/\")\n",
    "    else:\n",
    "      # load pretrained model\n",
    "      model = create_model(num_class=classes, model_name=model_name, num_channel=len(select_channels))\n",
    "      model.load_weights(DIRECTORY_PATH + \"/motor/models/\" + \"A09_0.9183/\")\n",
    "      # freeze model\n",
    "      model.trainable = False\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      if classes == 2:\n",
    "          model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      else:\n",
    "          model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      if classes == 2:\n",
    "          Y_hat = (Y_hat >= 0.5)\n",
    "      else:\n",
    "          Y_hat = np.argmax(Y_hat, axis=1)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "  accuracy /= n_splits\n",
    "  precision /= n_splits\n",
    "  recall /= n_splits\n",
    "  f1 /= n_splits\n",
    "  kappa /= n_splits\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"kappa: \" + str(kappa))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "result_accuracy = []\n",
    "result_precision = []\n",
    "result_recall = []\n",
    "result_f1 = []\n",
    "result_kappa = []\n",
    "for key, value in results.items():\n",
    "  result_accuracy += [value[\"accuracy\"]]\n",
    "  result_precision += [value[\"precision\"]]\n",
    "  result_recall += [value[\"recall\"]]\n",
    "  result_f1 += [value[\"f1\"]]\n",
    "  result_kappa += [value[\"kappa\"]]\n",
    "\n",
    "print(\"accuracy: (mean) \" + str(np.mean(result_accuracy)) + \" (std) \" + str(np.std(result_accuracy)))\n",
    "print(\"precision: (mean) \" + str(np.mean(result_precision)) + \" (std) \" + str(np.std(result_precision)))\n",
    "print(\"recall: (mean) \" + str(np.mean(result_recall)) + \" (std) \" + str(np.std(result_recall)))\n",
    "print(\"f1: (mean) \" + str(np.mean(result_f1)) + \" (std) \" + str(np.std(result_f1)))\n",
    "print(\"kappa: (mean) \" + str(np.mean(result_kappa)) + \" (std) \" + str(np.std(result_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1372fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time computation\n",
    "events = [\"left\", \"right\"]\n",
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "classes = 2\n",
    "debug = False\n",
    "model_name = \"eegnet\"\n",
    "warm_up = 10 # initializing memory allocators, and GPU-related initializations \n",
    "\n",
    "data_list = []\n",
    "data_list.append(\"A01\")\n",
    "\n",
    "information = get_inverse_and_forward_information(epochs)\n",
    "print(information)\n",
    "leadfield = information[\"leadfield\"]\n",
    "inverse_operator = information[\"inverse_operator\"]\n",
    "my_left_points = information[\"my_left_points\"]\n",
    "my_right_points = information[\"my_right_points\"]\n",
    "info = epochs[\"A01\"][\"left\"].info\n",
    "\n",
    "for data_name in data_list:\n",
    "    \n",
    "  X, Y = [], []\n",
    "  for event in epochs[data_name].keys():\n",
    "    for i in range(len(events)):\n",
    "      if event == events[i]:\n",
    "        if len(X) == 0:\n",
    "          X = epochs[data_name][event].get_data()\n",
    "          Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "        else:\n",
    "          X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "          Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y) \n",
    "\n",
    "  print(\"shape of X and Y: \" + str(X.shape) + \" \" + str(Y.shape))\n",
    "\n",
    "  X_test = np.expand_dims(X[0], axis=0)\n",
    "  Y_test = np.expand_dims(Y[0], axis=0)\n",
    "    \n",
    "  # load pretrained model\n",
    "  model = create_model(num_class=classes, model_name=model_name, num_channel=len(select_channels))\n",
    "  model.load_weights(\"D:/forward and inverse results (new)/motor/eegnet 22 channels (channel-wise normalization)/all motor/models/A01_0.8214/\")\n",
    "  # freeze model\n",
    "  model.trainable = False\n",
    "  optimizer = Adam(learning_rate=1e-5)\n",
    "  if classes == 2:\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "  else:\n",
    "      model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])      \n",
    "  if model_name == \"default\":\n",
    "      model.build(input_shape=(None, 40, 16*len(select_channels), 1))\n",
    "  elif model_name == \"eegnet\":\n",
    "      model.build(input_shape=(None, len(select_channels), 500, 1))\n",
    "  elif model_name == \"eegnet_inception\":\n",
    "      model.build(input_shape=(None, len(select_channels), 500, 1))\n",
    "\n",
    "  for i in range(warm_up):\n",
    "    if i == warm_up-1:\n",
    "        start = time.time()\n",
    "    \n",
    "    X_epochs = mne.EpochsArray(X_test, info, verbose=False)\n",
    "    method = \"sLORETA\"\n",
    "    snr = 3.\n",
    "    lambda2 = 1. / snr ** 2\n",
    "    stc_test = apply_inverse_epochs(X_epochs, inverse_operator, lambda2,\n",
    "                                  method=method, pick_ori=\"normal\", verbose=debug)\n",
    "    # slice reconstructed eeg data\n",
    "    reconstructed_eeg_data = []\n",
    "    for source in stc_test:\n",
    "        motor_source = np.zeros_like(source.data)\n",
    "        motor_source[:len(source.vertices[0])][my_left_points] = source.data[:len(source.vertices[0])][my_left_points]\n",
    "        motor_source[-len(source.vertices[1]):][my_right_points] = source.data[-len(source.vertices[1]):][my_right_points]\n",
    "        motor_eeg = np.dot(leadfield, motor_source)\n",
    "        reconstructed_eeg_data.append(motor_eeg)\n",
    "    reconstructed_eeg_data = np.array(reconstructed_eeg_data)    \n",
    "    reconstructed_eeg_data = reconstructed_eeg_data[:, select_channels, :]\n",
    "    print(reconstructed_eeg_data.shape)\n",
    "    \n",
    "#     # Normalization\n",
    "#     reconstructed_eeg_data_temp = np.zeros(reconstructed_eeg_data.shape)\n",
    "#     for j in range(reconstructed_eeg_data.shape[1]):\n",
    "#       temp = reconstructed_eeg_data[:, j, :]\n",
    "#       reconstructed_eeg_data_temp[:, j, :] = (temp-np.mean(temp))/np.std(temp)\n",
    "#     reconstructed_eeg_data = np.array(reconstructed_eeg_data_temp)\n",
    "    \n",
    "    if model_name == \"default\":\n",
    "      X_time, Y_time = stft_min_max(reconstructed_eeg_data, Y_test, debug)\n",
    "    elif model_name == \"eegnet\":\n",
    "      X_time = np.expand_dims(reconstructed_eeg_data, axis=-1) \n",
    "    elif model_name == \"eegnet_inception\":\n",
    "      X_time = np.expand_dims(reconstructed_eeg_data, axis=-1)\n",
    "    \n",
    "    Y_hat = model.predict(X_time)\n",
    "\n",
    "    if i == warm_up-1:\n",
    "        end = time.time()\n",
    "        print(\"time used: \", (end - start)*1000, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bc9c6",
   "metadata": {},
   "source": [
    "# CNN Classification (Auto-select Source Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "information = get_inverse_and_forward_information(epochs)\n",
    "print(information)\n",
    "\n",
    "print(information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]].shape)\n",
    "print(information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]].shape)\n",
    "forward_matrix = np.concatenate((information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]], \n",
    "                          information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]]),\n",
    "                          axis = 1)\n",
    "print(forward_matrix.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cde37e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "debug = True\n",
    "training = True\n",
    "random_select = False\n",
    "use_mask = True\n",
    "kqv = False\n",
    "model_name = \"eegnet\"\n",
    "num_epochs = 200\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A02\")\n",
    "# data_list.append(\"A05\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  # load data from external storage\n",
    "  directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"source activity\", data_name)\n",
    "  counter = 0\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "    \n",
    "  while(counter < n_splits):\n",
    "    counter += 1\n",
    "    X_train = np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    X_test = np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_train = np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_test = np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    \n",
    "    if debug:\n",
    "      print(data_name)\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    if training:\n",
    "      # create new model\n",
    "      model = AutoSelect(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "      #model.build(input_shape=(None, forward_matrix.shape[1], 500))\n",
    "      #print(model.summary())\n",
    "      \n",
    "      log_dir = DIRECTORY_PATH + \"/motor/logs/\" + data_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=num_epochs, callbacks=[tensorboard_callback], verbose=0)\n",
    "        \n",
    "      Y_hat = model.predict(X_test, batch_size=8)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "      # save model\n",
    "      model.save_weights(DIRECTORY_PATH + \"/motor/models/\" + data_name + \"_\" + str(accuracy_score(Y_test, Y_hat))[:6] + \"/\")\n",
    "    else:\n",
    "      # load pretrained model\n",
    "      model = AutoSelect(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "      model.load_weights(DIRECTORY_PATH + \"/motor/models/\" + \"A09_0.9183/\")\n",
    "      # freeze model\n",
    "      model.trainable = False\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "    \n",
    "    del X_train, Y_train, X_test, Y_test\n",
    "    gc.collect()\n",
    "\n",
    "  accuracy /= n_splits\n",
    "  precision /= n_splits\n",
    "  recall /= n_splits\n",
    "  f1 /= n_splits\n",
    "  kappa /= n_splits\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"kappa: \" + str(kappa))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "result_accuracy = []\n",
    "result_precision = []\n",
    "result_recall = []\n",
    "result_f1 = []\n",
    "result_kappa = []\n",
    "for key, value in results.items():\n",
    "  result_accuracy += [value[\"accuracy\"]]\n",
    "  result_precision += [value[\"precision\"]]\n",
    "  result_recall += [value[\"recall\"]]\n",
    "  result_f1 += [value[\"f1\"]]\n",
    "  result_kappa += [value[\"kappa\"]]\n",
    "\n",
    "print(\"accuracy: (mean) \" + str(np.mean(result_accuracy)) + \" (std) \" + str(np.std(result_accuracy)))\n",
    "print(\"precision: (mean) \" + str(np.mean(result_precision)) + \" (std) \" + str(np.std(result_precision)))\n",
    "print(\"recall: (mean) \" + str(np.mean(result_recall)) + \" (std) \" + str(np.std(result_recall)))\n",
    "print(\"f1: (mean) \" + str(np.mean(result_f1)) + \" (std) \" + str(np.std(result_f1)))\n",
    "print(\"kappa: (mean) \" + str(np.mean(result_kappa)) + \" (std) \" + str(np.std(result_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time computation\n",
    "events = [\"left\", \"right\"]\n",
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "debug = False\n",
    "random_select = False\n",
    "use_mask = False\n",
    "kqv = False\n",
    "model_name = \"eegnet\"\n",
    "warm_up = 10 # initializing memory allocators, and GPU-related initializations \n",
    "\n",
    "data_list = []\n",
    "data_list.append(\"A01\")\n",
    "\n",
    "information = get_inverse_and_forward_information(epochs)\n",
    "print(information)\n",
    "print(information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]].shape)\n",
    "print(information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]].shape)\n",
    "forward_matrix = np.concatenate((information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]], \n",
    "                          information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]]),\n",
    "                          axis = 1)\n",
    "print(forward_matrix.shape)\n",
    "inverse_operator = information[\"inverse_operator\"]\n",
    "my_left_points = information[\"my_left_points\"]\n",
    "my_right_points = information[\"my_right_points\"]\n",
    "info = epochs[\"A01\"][\"left\"].info\n",
    "\n",
    "for data_name in data_list:\n",
    "    \n",
    "  X, Y = [], []\n",
    "  for event in epochs[data_name].keys():\n",
    "    for i in range(len(events)):\n",
    "      if event == events[i]:\n",
    "        if len(X) == 0:\n",
    "          X = epochs[data_name][event].get_data()\n",
    "          Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "        else:\n",
    "          X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "          Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y) \n",
    "  X_test = np.expand_dims(X[0], axis=0)\n",
    "  Y_test = np.expand_dims(Y[0], axis=0)\n",
    "    \n",
    "  # load pretrained model\n",
    "  model = AutoSelect(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "  model.load_weights(\"D:/forward and inverse results (new)/motor/eegnet 22 channels (channel-wise normalization)/all motor attention/models/A01_0.8181/\")\n",
    "  # freeze model\n",
    "  model.trainable = False\n",
    "  optimizer = Adam(learning_rate=1e-5)\n",
    "  model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])      \n",
    "  model.build(input_shape=(None, forward_matrix.shape[1], 500))\n",
    "\n",
    "  for i in range(warm_up):\n",
    "    if i == warm_up-1:\n",
    "        start = time.time()\n",
    "    \n",
    "    X_epochs = mne.EpochsArray(X_test, info, verbose=False)\n",
    "    method = \"sLORETA\"\n",
    "    snr = 3.\n",
    "    lambda2 = 1. / snr ** 2\n",
    "    stc_test = apply_inverse_epochs(X_epochs, inverse_operator, lambda2,\n",
    "                                  method=method, pick_ori=\"normal\", verbose=debug)\n",
    "    # slice source activity data\n",
    "    left_hemi_data = []\n",
    "    right_hemi_data = []\n",
    "    for source in stc_test:\n",
    "        left_hemi_data.append(source.data[:len(source.vertices[0])][my_left_points])\n",
    "        right_hemi_data.append(source.data[-len(source.vertices[1]):][my_right_points])\n",
    "    left_hemi_data = np.array(left_hemi_data)\n",
    "    right_hemi_data = np.array(right_hemi_data)\n",
    "    \n",
    "    X_time = np.append(left_hemi_data, right_hemi_data, axis=1)\n",
    "    print(X_time.shape)\n",
    "    Y_hat = model.predict(X_time)\n",
    "\n",
    "    if i == warm_up-1:\n",
    "        end = time.time()\n",
    "        print(\"time used: \", (end - start)*1000, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e32b19",
   "metadata": {},
   "source": [
    "# Visualizing Enhanced EEG Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086571a",
   "metadata": {},
   "source": [
    "## Save Enhanced Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = [\"A01\", \"A02\", \"A03\", \"A04\", \"A05\", \"A06\", \"A07\", \"A08\", \"A09\"]\n",
    "# select_channels = list(range(22))\n",
    "# #select_channels = [7, 9, 11]\n",
    "# debug = True\n",
    "# model_name = \"eegnet\"\n",
    "# random_select = False\n",
    "# use_mask = True\n",
    "# kqv = False\n",
    "\n",
    "# for data_name in data_list:\n",
    "#   # load data from external storage\n",
    "#   directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"source activity\", data_name)\n",
    "#   counter = 0\n",
    "#   accuracy = 0\n",
    "#   precision = 0\n",
    "#   recall = 0\n",
    "#   f1 = 0\n",
    "#   kappa = 0\n",
    "#   print(data_name)\n",
    "    \n",
    "#   while(counter < n_splits):\n",
    "#     counter += 1\n",
    "#     X_train = np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "#     X_test = np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "#     Y_train = np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "#     Y_test = np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    \n",
    "#     print(\"before...\")\n",
    "#     print(\"X_train shape: {}, X_test shape: {}\".format(X_train.shape, X_test.shape))    \n",
    "    \n",
    "#     model = AutoSelectData(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "#     #weight_path = \"D:/forward and inverse results (new)/motor/ml motor/\"+ model_name + \"/models/\" + data_name + \"/\"\n",
    "#     weight_path = \"D:/forward and inverse results (new)/motor/ml motor/eegnet 22 channels/models/\" + data_name + \"/\"\n",
    "    \n",
    "#     for weight_file in os.listdir(weight_path):\n",
    "#         if weight_file.split(\"_\")[0] == str(counter):\n",
    "#             break\n",
    "#     load_weights_file = os.path.join(weight_path, weight_file) + \"/\"\n",
    "#     model.load_weights(load_weights_file)\n",
    "#     #model.build((None, X_train.shape[1], X_train.shape[2]))\n",
    "#     #print(model.summary())\n",
    "#     model.trainable = False\n",
    "#     X_train = model(X_train, training=False).numpy()\n",
    "#     X_test = model(X_test, training=False).numpy()\n",
    "    \n",
    "#     print(\"after...\")\n",
    "#     print(\"X_train shape: {}, X_test shape: {}\".format(X_train.shape, X_test.shape))\n",
    "    \n",
    "#     enhanced_eeg_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"enhanced eeg\", data_name)\n",
    "#     if not op.exists(enhanced_eeg_path):\n",
    "#         os.makedirs(enhanced_eeg_path)\n",
    "#     np.savez_compressed(op.join(enhanced_eeg_path, str(counter)+\"_train_X.npz\"), data=X_train)\n",
    "#     np.savez_compressed(op.join(enhanced_eeg_path, str(counter)+\"_train_Y.npz\"), data=Y_train)\n",
    "#     np.savez_compressed(op.join(enhanced_eeg_path, str(counter)+\"_test_X.npz\"), data=X_test)\n",
    "#     np.savez_compressed(op.join(enhanced_eeg_path, str(counter)+\"_test_Y.npz\"), data=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9539620",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\"A01\", \"A02\", \"A03\", \"A04\", \"A05\", \"A06\", \"A07\", \"A08\", \"A09\"]\n",
    "events = [\"left\", \"right\"]\n",
    "epochs_visualize = {}\n",
    "\n",
    "for data_name in data_list:\n",
    "  epochs_visualize[data_name] = {}\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "    \n",
    "  X, Y = [], []\n",
    "  for event in epochs[data_name].keys():\n",
    "    for i in range(len(events)):\n",
    "      if event == events[i]:\n",
    "        if len(X) == 0:\n",
    "          X = epochs[data_name][event].get_data()\n",
    "          Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "        else:\n",
    "          X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "          Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y)\n",
    "    \n",
    "  print(data_name)\n",
    "    \n",
    "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "  counter = 0\n",
    "  for train_index, test_index in skf.split(X, Y):\n",
    "    counter += 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"enhanced eeg\", data_name)\n",
    "    X_train_enhanced = np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    X_test_enhanced = np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_train_enhanced = np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_test_enhanced = np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    \n",
    "    assert(len(Y_train) == np.sum(Y_train == Y_train_enhanced))\n",
    "    assert(len(Y_test) == np.sum(Y_test == Y_test_enhanced))\n",
    "    \n",
    "    # visualize eeg signals in time series\n",
    "    info = epochs[\"A01\"][\"left\"].info\n",
    "    X_train_0 = X_train[np.where(Y_train == 0)[0]]\n",
    "    X_train_1 = X_train[np.where(Y_train == 1)[0]]\n",
    "    X_test_0 = X_test[np.where(Y_test == 0)[0]]\n",
    "    X_test_1 = X_test[np.where(Y_test == 1)[0]]\n",
    "    \n",
    "    X_train_enhanced_0 = X_train_enhanced[np.where(Y_train == 0)[0]]\n",
    "    X_train_enhanced_1 = X_train_enhanced[np.where(Y_train == 1)[0]]\n",
    "    X_test_enhanced_0 = X_test_enhanced[np.where(Y_test == 0)[0]]\n",
    "    X_test_enhanced_1 = X_test_enhanced[np.where(Y_test == 1)[0]]\n",
    "    \n",
    "    print(counter)\n",
    "    print(len(X_train_0), len(X_train_1), len(X_test_0), len(X_test_1))\n",
    "    print(len(X_train_enhanced_0), len(X_train_enhanced_1), len(X_test_enhanced_0), len(X_test_enhanced_1))\n",
    "    \n",
    "    epochs_visualize[data_name][\"original\"] = {}\n",
    "    epochs_visualize[data_name][\"enhanced\"] = {}\n",
    "    \n",
    "    epochs_visualize[data_name][\"original\"][\"X_train_0\"] = mne.EpochsArray(X_train_0, info, verbose=False)\n",
    "    epochs_visualize[data_name][\"original\"][\"X_train_1\"] = mne.EpochsArray(X_train_1, info, verbose=False)\n",
    "    epochs_visualize[data_name][\"original\"][\"X_test_0\"] = mne.EpochsArray(X_test_0, info, verbose=False)\n",
    "    epochs_visualize[data_name][\"original\"][\"X_test_1\"] = mne.EpochsArray(X_test_1, info, verbose=False)\n",
    "    \n",
    "    epochs_visualize[data_name][\"enhanced\"][\"X_train_0\"] = mne.EpochsArray(X_train_enhanced_0, info, verbose=False)\n",
    "    epochs_visualize[data_name][\"enhanced\"][\"X_train_1\"] = mne.EpochsArray(X_train_enhanced_1, info, verbose=False)\n",
    "    epochs_visualize[data_name][\"enhanced\"][\"X_test_0\"] = mne.EpochsArray(X_test_enhanced_0, info, verbose=False)\n",
    "    epochs_visualize[data_name][\"enhanced\"][\"X_test_1\"] = mne.EpochsArray(X_test_enhanced_1, info, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1177236",
   "metadata": {},
   "source": [
    "## Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "picked_time = [0.75, 1, 1.25, 1.5]\n",
    "topomap_subject = \"A09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"original\"][\"X_train_0\"].average().plot_topomap(res=256, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"original\"][\"X_train_1\"].average().plot_topomap(res=256, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d017e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"enhanced\"][\"X_train_0\"].average().plot_topomap(res=256, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250aeb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"enhanced\"][\"X_train_1\"].average().plot_topomap(res=256, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"original\"][\"X_train_0\"].average().plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33466c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"original\"][\"X_train_1\"].average().plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"enhanced\"][\"X_train_0\"].average().plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e8516",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_visualize[topomap_subject][\"enhanced\"][\"X_train_1\"].average().plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5dc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subject_spectrogram(evoked_data, plot_channel=[7,9,11]):\n",
    "    ch_num_to_ch_name = {7:\"C3\", 9:\"Cz\", 11:\"C4\"}\n",
    "    Zxx = tf.signal.stft(evoked_data, frame_length=128, frame_step=16)\n",
    "    Zxx = tf.abs(Zxx)\n",
    "    \n",
    "    print(\"shape of evoked_data: \" + str(evoked_data.shape))\n",
    "    print(\"shape of Zxx: \" + str(Zxx.shape))\n",
    "    \n",
    "    # plot spectrogram\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    fig.suptitle('Short Time Fourier Transform Magnitude')\n",
    "    fig.supxlabel('frequency (Hz)')\n",
    "    fig.supylabel('Time [sec]')\n",
    "    for i, channel_i in enumerate(plot_channel):    \n",
    "        log_spec = tf.math.log(tf.transpose(Zxx[channel_i]))\n",
    "        height = 50\n",
    "        width = log_spec.shape[1]\n",
    "        x_axis = tf.linspace(0, 2, num=width)\n",
    "        y_axis = range(height)\n",
    "        ax[i].title.set_text(ch_num_to_ch_name[channel_i])\n",
    "        im = ax[i].pcolormesh(x_axis, y_axis, log_spec[:50, ])\n",
    "        fig.colorbar(mappable=im, ax=ax[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_subject_spectrogram(epochs_visualize[topomap_subject][\"original\"][\"X_train_0\"].average().get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd871cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_subject_spectrogram(epochs_visualize[topomap_subject][\"original\"][\"X_train_1\"].average().get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d243d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_subject_spectrogram(epochs_visualize[topomap_subject][\"enhanced\"][\"X_train_0\"].average().get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_subject_spectrogram(epochs_visualize[topomap_subject][\"enhanced\"][\"X_train_1\"].average().get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6ff64",
   "metadata": {},
   "source": [
    "# Visualizing Mask and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a1b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "model = AutoSelect(select_channels, forward_matrix, False, True, False, \"eegnet\")\n",
    "model.build(input_shape=(None, 6433, 500))\n",
    "#print(model.summary())\n",
    "load_model_directory = \"all motor mask initialize 1/models/\"\n",
    "load_model_subject = \"A01_0.8928/\"\n",
    "\n",
    "model.load_weights(\"D:/forward and inverse results (new)/motor/eegnet 22 channels/\" + load_model_directory + load_model_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.get_weights():\n",
    "    print(weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cc76a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mni_lh = mne.vertex_to_mni(information[\"left_vertices\"], 0, mne_subject)\n",
    "print(mni_lh.shape)\n",
    "mni_rh = mne.vertex_to_mni(information[\"right_vertices\"], 1, mne_subject)\n",
    "print(mni_rh.shape)\n",
    "\n",
    "mni_left_points = mni_lh[information[\"my_left_points\"]]\n",
    "print(mni_left_points.shape)\n",
    "mni_right_points = mni_rh[information[\"my_right_points\"]]\n",
    "print(mni_right_points.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(mni_lh[:, 0], mni_lh[:, 1], mni_lh[:, 2], s=15, marker='|')\n",
    "ax.scatter(mni_rh[:, 0], mni_rh[:, 1], mni_rh[:, 2], s=15, marker='_')\n",
    "ax.scatter(mni_left_points[:, 0], mni_left_points[:, 1], mni_left_points[:, 2], s=15, marker='o')\n",
    "ax.scatter(mni_right_points[:, 0], mni_right_points[:, 1], mni_right_points[:, 2], s=15, marker='^')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004667d2",
   "metadata": {},
   "source": [
    "## Mask Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614338d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mask\n",
    "#np.sum(model.get_weights()[-1] >= 1)\n",
    "\n",
    "for pts in model.get_weights()[-1]:\n",
    "    print(pts)\n",
    "print(np.max(model.get_weights()[-1]))\n",
    "print(np.min(model.get_weights()[-1]))\n",
    "print(np.mean(model.get_weights()[-1]))\n",
    "print(np.std(model.get_weights()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_weight = np.zeros((information[\"my_left_points\"].shape))\n",
    "right_weight =  np.zeros((information[\"my_right_points\"].shape))\n",
    "left_weight[information[\"my_left_points\"]] = model.get_weights()[-1].reshape(-1)[:np.sum(information[\"my_left_points\"])]\n",
    "right_weight[information[\"my_right_points\"]] = model.get_weights()[-1].reshape(-1)[np.sum(information[\"my_left_points\"]):]\n",
    "\n",
    "total_weight = np.append(left_weight, right_weight, axis=0)\n",
    "print(total_weight)\n",
    "print(total_weight.shape)\n",
    "\n",
    "# binary weights\n",
    "total_weight = (total_weight >= 1).astype(int)\n",
    "print(total_weight)\n",
    "print(\"original: \", np.sum(information[\"my_left_points\"])+np.sum(np.sum(information[\"my_right_points\"])))\n",
    "print(\"mask: \", np.sum(total_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a93ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_tree = KDTree(mm_coords.reshape(-1, 3))\n",
    "final_weight = np.zeros(mm_coords.reshape(-1, 3).shape)\n",
    "\n",
    "max_points = 0\n",
    "for i, vertex_i in enumerate(vertices_tree.query_ball_point(np.append(mni_lh, mni_rh, axis=0), 2)):\n",
    "    if vertex_i != []:\n",
    "        if np.max(vertex_i) > max_points:\n",
    "            max_points = np.max(vertex_i)\n",
    "        #print(mm_coords.reshape(-1, 3)[np.max(vertex_i)])\n",
    "        #print(np.append(mni_lh, mni_rh, axis=0)[i])\n",
    "        \n",
    "        # fill all\n",
    "        for vertex_j in vertex_i:\n",
    "            final_weight[vertex_j] = total_weight[i]\n",
    "        # fill biggest index\n",
    "        #final_weight[np.max(vertex_i)] = total_weight[i]\n",
    "        #print(vertex_i)\n",
    "print(max_points)\n",
    "\n",
    "final_weight = final_weight.reshape(mm_coords.shape)[:, :, :, 0]\n",
    "\n",
    "visualization_mask = Nifti1Image(final_weight, ch2_img.affine, ch2_img.header)\n",
    "nib.save(visualization_mask, os.path.join('C:/Users/ivanlim/Desktop/EEG-forward-and-inverse', 'visualization_mask.nii.gz'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_mask_with_shape = Nifti1Image(np.asanyarray(final_weight), ch2_img.affine, ch2_img.header)\n",
    "nib.save(visualization_mask_with_shape, os.path.join('C:/Users/ivanlim/Desktop/EEG-forward-and-inverse', 'visualization_mask_with_shape.nii.gz')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725ec2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_glass_brain(\"C:/Users/ivanlim/Desktop/EEG-forward-and-inverse/visualization_mask_with_shape.nii.gz\", display_mode='xz', threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d9503",
   "metadata": {},
   "source": [
    "# Statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tensorboard(path, scalars):\n",
    "    \"\"\"returns a dictionary of pandas dataframes for each requested scalar\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        path,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    _absorb_print = ea.Reload()\n",
    "    #print(ea.Tags())\n",
    "    # make sure the scalars are in the event accumulator tags\n",
    "    assert all(\n",
    "        s in ea.Tags()['tensors'] for s in scalars\n",
    "    ), \"some scalars were not found in the event accumulator\"\n",
    "    return {k: pd.DataFrame(ea.Tensors(k)) for k in scalars}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280b85f",
   "metadata": {},
   "source": [
    "## Right-tailed test (paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79d2c2",
   "metadata": {},
   "source": [
    "### subject-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4a25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "method_path = \"D:/forward and inverse results (new)/motor/eeg-inception 22 channels (channel-wise normalization)\"\n",
    "accuracy_dict = {}\n",
    "\n",
    "for method in os.listdir(method_path):\n",
    "    logs_path = os.path.join(method_path, method, \"logs\")\n",
    "    if not accuracy_dict.get(method):\n",
    "        accuracy_dict[method] = {}\n",
    "    for subject in os.listdir(logs_path):\n",
    "        if not accuracy_dict[method].get(subject):\n",
    "            accuracy_dict[method][subject] = {}\n",
    "        subject_path = os.path.join(logs_path, subject)\n",
    "        for time in os.listdir(subject_path):\n",
    "            validation_path = os.path.join(subject_path, time, \"validation\")\n",
    "            tensorboard_logs = os.path.join(validation_path, os.listdir(validation_path)[0])\n",
    "            df = parse_tensorboard(tensorboard_logs, [\"epoch_loss\", \"epoch_accuracy\"])\n",
    "            #print(subject, time)\n",
    "            last_test_accuracy = tf.constant(tf.make_ndarray(df[\"epoch_accuracy\"][\"tensor_proto\"][len(df[\"epoch_accuracy\"][\"tensor_proto\"])-1]))\n",
    "            accuracy_dict[method][subject][time] = last_test_accuracy.numpy()\n",
    "            \n",
    "accuracy_mean_std = {}\n",
    "for method in accuracy_dict.keys():\n",
    "    if method == \"original EEG\":\n",
    "        continue\n",
    "    accuracy_mean_std[method] = {}\n",
    "    for subject in accuracy_dict[method].keys():\n",
    "        accuracy_mean_std[method][subject] = {}\n",
    "        accuracy_mean_std[method][subject][\"accuracy\"] = []\n",
    "        for i in range(len(list(accuracy_dict[method][subject].values()))):\n",
    "            accuracy_method = list(accuracy_dict[method][subject].values())[i]\n",
    "            accuracy_control = list(accuracy_dict[\"original EEG\"][subject].values())[i]\n",
    "            accuracy_mean_std[method][subject][\"accuracy\"].append(accuracy_method - accuracy_control)\n",
    "        mean = np.mean(accuracy_mean_std[method][subject][\"accuracy\"])\n",
    "        std = np.std(accuracy_mean_std[method][subject][\"accuracy\"])\n",
    "        accuracy_mean_std[method][subject][\"mean\"] = mean\n",
    "        accuracy_mean_std[method][subject][\"std\"] = std\n",
    "        #print(method, subject, mean, std)\n",
    "        \n",
    "subject_list = [\"A01\", \"A02\", \"A03\", \"A04\", \"A05\", \"A06\", \"A07\", \"A08\", \"A09\"]\n",
    "t_test_results = {}\n",
    "\n",
    "for i, subject in enumerate(subject_list):\n",
    "    t_test_results[subject] = {}\n",
    "    count = 0\n",
    "    for method in accuracy_mean_std.keys():\n",
    "#         if method == \"all motor attention with dropout 0.5\":\n",
    "#             continue\n",
    "            \n",
    "        mean = accuracy_mean_std[method][subject][\"mean\"]\n",
    "        std = accuracy_mean_std[method][subject][\"std\"]\n",
    "        n = 5\n",
    "        t_test = mean / np.sqrt((std**2)/n)\n",
    "        df = n - 1\n",
    "        if t_test > 0:\n",
    "            p_value = stats.t.sf(np.abs(t_test), df)\n",
    "            if p_value < 0.05:\n",
    "                count += 1\n",
    "                print(subject, method, mean)\n",
    "        #print(t_test, df, p_value)\n",
    "        \n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd25cbf",
   "metadata": {},
   "source": [
    "### method-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af86670",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_path = \"D:/forward and inverse results (new)/motor/eeg-inception 22 channels (channel-wise normalization)\"\n",
    "accuracy_dict = {}\n",
    "\n",
    "for method in os.listdir(method_path):\n",
    "    logs_path = os.path.join(method_path, method, \"logs\")\n",
    "    if not accuracy_dict.get(method):\n",
    "        accuracy_dict[method] = {}\n",
    "    for subject in os.listdir(logs_path):\n",
    "        if not accuracy_dict[method].get(subject):\n",
    "            accuracy_dict[method][subject] = {}\n",
    "        subject_path = os.path.join(logs_path, subject)\n",
    "        for time in os.listdir(subject_path):\n",
    "            validation_path = os.path.join(subject_path, time, \"validation\")\n",
    "            tensorboard_logs = os.path.join(validation_path, os.listdir(validation_path)[0])\n",
    "            df = parse_tensorboard(tensorboard_logs, [\"epoch_loss\", \"epoch_accuracy\"])\n",
    "            #print(subject, time)\n",
    "            last_test_accuracy = tf.constant(tf.make_ndarray(df[\"epoch_accuracy\"][\"tensor_proto\"][len(df[\"epoch_accuracy\"][\"tensor_proto\"])-1]))\n",
    "            accuracy_dict[method][subject][time] = last_test_accuracy.numpy()\n",
    "            \n",
    "accuracy_mean_std = {}\n",
    "for method in accuracy_dict.keys():\n",
    "    accuracy_mean_std[method] = {}\n",
    "    for subject in accuracy_dict[method].keys():\n",
    "        accuracy_time = list(accuracy_dict[method][subject].values())\n",
    "        mean = np.mean(accuracy_time)\n",
    "        std = np.std(accuracy_time)\n",
    "        accuracy_mean_std[method][subject] = {}\n",
    "        accuracy_mean_std[method][subject][\"mean\"] = mean\n",
    "        accuracy_mean_std[method][subject][\"std\"] = std\n",
    "        #print(method, subject, mean)\n",
    "        \n",
    "subject_list = [\"A01\", \"A02\", \"A03\", \"A04\", \"A05\", \"A06\", \"A07\", \"A08\", \"A09\"]\n",
    "t_test_results = {}\n",
    "\n",
    "for method in accuracy_mean_std.keys():\n",
    "#     if method == \"original EEG\" or method == \"all motor attention with dropout 0.5\":\n",
    "#         continue\n",
    "    if method == \"original EEG\":\n",
    "        continue\n",
    "    t_test_results[method] = []\n",
    "    for subject in subject_list:\n",
    "        accuracy_1 = accuracy_mean_std[method][subject][\"mean\"]\n",
    "        accuracy_2 = accuracy_mean_std[\"original EEG\"][subject][\"mean\"]\n",
    "        mean = accuracy_1 - accuracy_2\n",
    "        t_test_results[method].append(mean)\n",
    "        #print(mean)\n",
    "\n",
    "for method in accuracy_mean_std.keys():\n",
    "#     if method == \"original EEG\" or method == \"all motor attention with dropout 0.5\":\n",
    "#         continue\n",
    "    if method == \"original EEG\":\n",
    "        continue\n",
    "    mean = np.mean(t_test_results[method])\n",
    "    std = np.std(t_test_results[method])\n",
    "    n = 9\n",
    "    t_test = mean / np.sqrt((std**2)/n)\n",
    "    df = n - 1\n",
    "    if t_test > 0:\n",
    "        p_value = stats.t.sf(np.abs(t_test), df)\n",
    "        if p_value < 0.05:\n",
    "            print(method, mean)\n",
    "    #print(t_test, df, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40633dbb",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcc199",
   "metadata": {},
   "source": [
    "## Traditional machine learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1e14f",
   "metadata": {},
   "source": [
    "### Generate trainable attention model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6436a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "information = get_inverse_and_forward_information(epochs)\n",
    "print(information)\n",
    "\n",
    "print(information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]].shape)\n",
    "print(information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]].shape)\n",
    "forward_matrix = np.concatenate((information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]], \n",
    "                          information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]]),\n",
    "                          axis = 1)\n",
    "print(forward_matrix.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "#select_channels = [7, 9, 11]\n",
    "select_channels = list(range(22))\n",
    "debug = True\n",
    "training = True\n",
    "random_select = False\n",
    "use_mask = True\n",
    "kqv = False\n",
    "model_name = \"eegnet\"\n",
    "num_epochs = 200\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A02\")\n",
    "# data_list.append(\"A05\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  # load data from external storage\n",
    "  directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"source activity\", data_name)\n",
    "  counter = 0\n",
    "  accuracy = 0\n",
    "  precision = 0\n",
    "  recall = 0\n",
    "  f1 = 0\n",
    "  kappa = 0\n",
    "    \n",
    "  while(counter < n_splits):\n",
    "    counter += 1\n",
    "    X_train = np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    X_test = np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_train = np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_test = np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    \n",
    "    if debug:\n",
    "      print(data_name)\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    if training:\n",
    "      # create new model\n",
    "      model = AutoSelect(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "      #model.build(input_shape=(None, forward_matrix.shape[1], 500))\n",
    "      #print(model.summary())\n",
    "      \n",
    "      log_dir = DIRECTORY_PATH + \"/\" + model_name + \"/ml motor/logs/\" + data_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "      model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=num_epochs, callbacks=[tensorboard_callback], verbose=0)\n",
    "        \n",
    "      Y_hat = model.predict(X_test, batch_size=8)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "\n",
    "      # save model\n",
    "      model.save_weights(DIRECTORY_PATH + \"/\" + model_name + \"/ml motor/models/\" + data_name + \"/\" + str(counter) + \"_\" + str(accuracy_score(Y_test, Y_hat))[:6] + \"/\")\n",
    "    else:\n",
    "      # load pretrained model\n",
    "      model = AutoSelect(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "      model.load_weights(DIRECTORY_PATH + \"/default/ml motor/models/A09/\" + \"1_0.9183/\")\n",
    "      # freeze model\n",
    "      model.trainable = False\n",
    "      optimizer = Adam(learning_rate=1e-5)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "      Y_hat = model.predict(X_test)\n",
    "      Y_hat = (Y_hat >= 0.5)\n",
    "      accuracy += accuracy_score(Y_test, Y_hat)\n",
    "      precision += precision_score(Y_test, Y_hat, average=\"macro\")\n",
    "      recall += recall_score(Y_test, Y_hat, average=\"macro\")\n",
    "      f1 += f1_score(Y_test, Y_hat, average=\"macro\")\n",
    "      kappa += cohen_kappa_score(Y_test, Y_hat)\n",
    "    \n",
    "    del X_train, Y_train, X_test, Y_test\n",
    "    gc.collect()\n",
    "\n",
    "  accuracy /= n_splits\n",
    "  precision /= n_splits\n",
    "  recall /= n_splits\n",
    "  f1 /= n_splits\n",
    "  kappa /= n_splits\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"f1: \" + str(f1))\n",
    "    print(\"kappa: \" + str(kappa))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "result_accuracy = []\n",
    "result_precision = []\n",
    "result_recall = []\n",
    "result_f1 = []\n",
    "result_kappa = []\n",
    "for key, value in results.items():\n",
    "  result_accuracy += [value[\"accuracy\"]]\n",
    "  result_precision += [value[\"precision\"]]\n",
    "  result_recall += [value[\"recall\"]]\n",
    "  result_f1 += [value[\"f1\"]]\n",
    "  result_kappa += [value[\"kappa\"]]\n",
    "\n",
    "print(\"accuracy: (mean) \" + str(np.mean(result_accuracy)) + \" (std) \" + str(np.std(result_accuracy)))\n",
    "print(\"precision: (mean) \" + str(np.mean(result_precision)) + \" (std) \" + str(np.std(result_precision)))\n",
    "print(\"recall: (mean) \" + str(np.mean(result_recall)) + \" (std) \" + str(np.std(result_recall)))\n",
    "print(\"f1: (mean) \" + str(np.mean(result_f1)) + \" (std) \" + str(np.std(result_f1)))\n",
    "print(\"kappa: (mean) \" + str(np.mean(result_kappa)) + \" (std) \" + str(np.std(result_kappa)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabfe6d",
   "metadata": {},
   "source": [
    "### Lda, SVM, Random Forest, Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3582f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "events = [\"left\", \"right\"]\n",
    "select_channels = list(range(22))\n",
    "#select_channels = [7, 9, 11]\n",
    "classes = 2\n",
    "debug = True\n",
    "use_csp = True\n",
    "\n",
    "warm_up = 10 # initializing memory allocators, and GPU-related initializations \n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A05\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  accuracy = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  f1 = []\n",
    "  kappa = []\n",
    "    \n",
    "  X, Y = [], []\n",
    "  if use_csp: \n",
    "    for freq in epochs_filter_bank[data_name].keys():\n",
    "        #print(freq)\n",
    "        X_freq, Y_freq = [], []\n",
    "        \n",
    "        for event in epochs_filter_bank[data_name][freq].keys():\n",
    "            for i in range(len(events)):\n",
    "                if event == events[i]:\n",
    "                    if len(X_freq) == 0:\n",
    "                        X_freq = epochs_filter_bank[data_name][freq][event].get_data()\n",
    "                        Y_freq = np.zeros(len(epochs_filter_bank[data_name][freq][event].get_data())) + i\n",
    "                    else:\n",
    "                        X_freq = np.append(X_freq, epochs_filter_bank[data_name][freq][event].get_data(), axis=0)\n",
    "                        Y_freq = np.append(Y_freq, np.zeros(len(epochs_filter_bank[data_name][freq][event].get_data())) + i, axis=0)\n",
    "        if len(X) == 0:\n",
    "            X = np.expand_dims(X_freq, axis=0)\n",
    "            Y = Y_freq\n",
    "        else:\n",
    "            X = np.append(X, np.expand_dims(X_freq, axis=0), axis=0)\n",
    "  else:\n",
    "    for event in epochs[data_name].keys():\n",
    "      for i in range(len(events)):\n",
    "        if event == events[i]:\n",
    "          if len(X) == 0:\n",
    "            X = epochs[data_name][event].get_data()\n",
    "            Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "          else:\n",
    "            X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "            Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "\n",
    "  count = 0\n",
    "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "  for train_index, test_index in skf.split(X[0], Y) if len(X.shape) == 4 else skf.split(X, Y):\n",
    "    count += 1\n",
    "    \n",
    "    if use_csp:\n",
    "        X_train_csp, Y_train_csp = [], []\n",
    "        X_test_csp, Y_test_csp = [], []\n",
    "        for i in range(len(X)):\n",
    "            #print(\"current freq: \", i)\n",
    "            X_train, X_test = X[i][train_index], X[i][test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            csp_pipeline = make_pipeline(\n",
    "                CSP(n_components=4, reg='diagonal_fixed', log=True, norm_trace=False, rank='full')\n",
    "            )\n",
    "            \n",
    "            X_train = csp_pipeline.fit_transform(X_train, Y_train)\n",
    "            X_test = csp_pipeline.transform(X_test)\n",
    "            \n",
    "            if len(X_train_csp) == 0:\n",
    "                X_train_csp = X_train\n",
    "                X_test_csp = X_test\n",
    "                Y_train_csp = Y_train\n",
    "                Y_test_csp = Y_test\n",
    "            else:\n",
    "                X_train_csp = np.append(X_train_csp, X_train, axis=1)\n",
    "                X_test_csp = np.append(X_test_csp, X_test, axis=1)\n",
    "                \n",
    "        X_train = np.array(X_train_csp, copy=True)\n",
    "        X_test = np.array(X_test_csp, copy=True)\n",
    "        \n",
    "    else:\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    print(X_train.shape, Y_train.shape)\n",
    "    print(X_test.shape, Y_test.shape)\n",
    "    \n",
    "    # add fake data\n",
    "    # (1) real + fake\n",
    "#     X_train = np.append(X_train, X_fake, axis=0)\n",
    "#     Y_train = np.append(Y_train, Y_fake, axis=0)\n",
    "    # (2) fake\n",
    "#     X_train = X_fake\n",
    "#     Y_train = Y_fake\n",
    "    \n",
    "    # pick c3, cZ, c4 channels\n",
    "    if not use_csp:\n",
    "        X_train = X_train[:, select_channels, :]\n",
    "        X_test = X_test[:, select_channels, :]\n",
    "\n",
    "    print(data_name)\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    if debug:\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    #model = make_pipeline(StandardScaler(), SVC(gamma='auto', random_state=0))\n",
    "    #model = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "    #model = make_pipeline(StandardScaler(), AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "    model = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis())\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_hat = model.predict(X_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(Y_test, Y_hat))\n",
    "    precision.append(precision_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    recall.append(recall_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    f1.append(f1_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    kappa.append(cohen_kappa_score(Y_test, Y_hat))\n",
    "    \n",
    "    # time computation\n",
    "    if data_name != \"A01\" or count != 1:\n",
    "        continue\n",
    "    for i in range(warm_up):\n",
    "        if i == warm_up-1:\n",
    "            start = time.time()\n",
    "        X_time = np.expand_dims(X_test[0], 0)\n",
    "        #print(X_time.shape)\n",
    "\n",
    "        Y_hat = model.predict(X_time)\n",
    "\n",
    "        if i == warm_up-1:\n",
    "            end = time.time()\n",
    "            print(\"time used: \", (end - start)*1000, \"ms\")\n",
    "\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(np.mean(accuracy)))\n",
    "    print(\"precision: \" + str(np.mean(precision)))\n",
    "    print(\"recall: \" + str(np.mean(recall)))\n",
    "    print(\"f1: \" + str(np.mean(f1)))\n",
    "    print(\"kappa: \" + str(np.mean(kappa)))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "result_accuracy = []\n",
    "result_precision = []\n",
    "result_recall = []\n",
    "result_f1 = []\n",
    "result_kappa = []\n",
    "for key, value in results.items():\n",
    "  result_accuracy += [value[\"accuracy\"]]\n",
    "  result_precision += [value[\"precision\"]]\n",
    "  result_recall += [value[\"recall\"]]\n",
    "  result_f1 += [value[\"f1\"]]\n",
    "  result_kappa += [value[\"kappa\"]]\n",
    "\n",
    "print(\"accuracy: (mean) \" + str(np.mean(result_accuracy)) + \" (std) \" + str(np.std(result_accuracy)))\n",
    "print(\"precision: (mean) \" + str(np.mean(result_precision)) + \" (std) \" + str(np.std(result_precision)))\n",
    "print(\"recall: (mean) \" + str(np.mean(result_recall)) + \" (std) \" + str(np.std(result_recall)))\n",
    "print(\"f1: (mean) \" + str(np.mean(result_f1)) + \" (std) \" + str(np.std(result_f1)))\n",
    "print(\"kappa: (mean) \" + str(np.mean(result_kappa)) + \" (std) \" + str(np.std(result_kappa)))\n",
    "\n",
    "#np.savez(os.path.join(DIRECTORY_PATH, \"lda.npz\"), results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f031305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = np.load(os.path.join(DIRECTORY_PATH, \"svm.npz\"), allow_pickle=True)\n",
    "# print(svm[\"results\"].item()[\"A01\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48da16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "events = [\"left\", \"right\"]\n",
    "use_csp = False\n",
    "select_channels = list(range(22))\n",
    "#select_channels = [7, 9, 11]\n",
    "classes = 2\n",
    "debug = True\n",
    "model_name = \"eegnet\"\n",
    "random_select = False\n",
    "use_mask = True\n",
    "kqv = False\n",
    "\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A05\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  # load data from external storage\n",
    "  directory_path = op.join(EXTERNAL_STORAGE_PATH, \"all motor region\", \"data\", \"source activity\", data_name)\n",
    "  counter = 0\n",
    "  accuracy = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  f1 = []\n",
    "  kappa = []\n",
    "    \n",
    "  while(counter < n_splits):\n",
    "    counter += 1\n",
    "    X_train = np.load(op.join(directory_path, str(counter)+\"_train_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    X_test = np.load(op.join(directory_path, str(counter)+\"_test_X.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_train = np.load(op.join(directory_path, str(counter)+\"_train_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    Y_test = np.load(op.join(directory_path, str(counter)+\"_test_Y.npz\"), allow_pickle=True)[\"data\"].astype(np.float32)\n",
    "    \n",
    "    if debug:\n",
    "      print(data_name)\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    model = AutoSelectData(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "    #weight_path = \"D:/forward and inverse results (new)/motor/ml motor/\"+ model_name + \"/models/\" + data_name + \"/\"\n",
    "    weight_path = \"D:/forward and inverse results (new)/motor/ml motor/eegnet 22 channels/models/\" + data_name + \"/\"\n",
    "    \n",
    "    for weight_file in os.listdir(weight_path):\n",
    "        if weight_file.split(\"_\")[0] == str(counter):\n",
    "            break\n",
    "    load_weights_file = os.path.join(weight_path, weight_file) + \"/\"\n",
    "    model.load_weights(load_weights_file)\n",
    "    #model.build((None, X_train.shape[1], X_train.shape[2]))\n",
    "    #print(model.summary())\n",
    "    model.trainable = False\n",
    "    X_train = model(X_train, training=False).numpy()\n",
    "    X_test = model(X_test, training=False).numpy()\n",
    "    \n",
    "    if not use_csp:\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    if use_csp:\n",
    "        # old version\n",
    "        lowcut = 4\n",
    "        highcut = 40\n",
    "        interval = 4\n",
    "        filter_bank = list(np.arange(lowcut, highcut+interval, step=interval))\n",
    "        info = epochs[data_name][\"left\"].info\n",
    "        \n",
    "        X_train = mne.EpochsArray(X_train, info, verbose=False)\n",
    "        X_test = mne.EpochsArray(X_test, info, verbose=False)\n",
    "        \n",
    "        X_train_csp = []\n",
    "        X_test_csp = []\n",
    "        \n",
    "        for i in range(len(filter_bank)-1):\n",
    "            low = filter_bank[i]\n",
    "            high = filter_bank[i+1]\n",
    "            print(\"current frequency: \", low, \" to \", high)\n",
    "\n",
    "            # filter frequency\n",
    "            iir_params = dict(order=5, ftype='butter')\n",
    "            X_train_filter = X_train.copy()\n",
    "            X_train_filter.filter(low, high, method=\"iir\", iir_params=iir_params)\n",
    "            \n",
    "            X_test_filter = X_test.copy()\n",
    "            X_test_filter.filter(low, high, method=\"iir\", iir_params=iir_params)\n",
    "            \n",
    "            csp_pipeline = make_pipeline(\n",
    "                CSP(n_components=4, reg='diagonal_fixed', log=True, norm_trace=False, rank='full')\n",
    "            )\n",
    "            \n",
    "            X_train_csp_feature = csp_pipeline.fit_transform(X_train_filter.get_data(), Y_train)\n",
    "            X_test_csp_feature = csp_pipeline.transform(X_test_filter.get_data())\n",
    "            \n",
    "            if len(X_train_csp) == 0:\n",
    "                X_train_csp = X_train_csp_feature\n",
    "                X_test_csp = X_test_csp_feature\n",
    "            else:\n",
    "                X_train_csp = np.append(X_train_csp, X_train_csp_feature, axis=1)\n",
    "                X_test_csp = np.append(X_test_csp, X_test_csp_feature, axis=1)\n",
    "        \n",
    "        X_train = np.array(X_train_csp, copy=True)\n",
    "        X_test = np.array(X_test_csp, copy=True)\n",
    "        \n",
    "    print(\"X train: \", X_train.shape, \" X test: \", X_test.shape)\n",
    "\n",
    "    #model = make_pipeline(StandardScaler(), SVC(gamma='auto', random_state=0))\n",
    "    #model = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "    model = make_pipeline(StandardScaler(), AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "    #model = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis())\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_hat = model.predict(X_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(Y_test, Y_hat))\n",
    "    precision.append(precision_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    recall.append(recall_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    f1.append(f1_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    kappa.append(cohen_kappa_score(Y_test, Y_hat))\n",
    "\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(np.mean(accuracy)))\n",
    "    print(\"precision: \" + str(np.mean(precision)))\n",
    "    print(\"recall: \" + str(np.mean(recall)))\n",
    "    print(\"f1: \" + str(np.mean(f1)))\n",
    "    print(\"kappa: \" + str(np.mean(kappa)))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe4137",
   "metadata": {},
   "source": [
    "### Masked raw -> bandpass filter -> CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbb2e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labels\n",
    "left (class 0) right (class 1) foot (class 2) tongue (class 3)\n",
    "\n",
    "channels\n",
    "c3(7) cz(9) c4(11)\n",
    "\"\"\"\n",
    "results = {\"A01\": {}, \"A02\": {}, \"A03\": {}, \"A04\": {}, \"A05\": {}, \"A06\": {}, \"A07\": {}, \"A08\": {}, \"A09\": {}}\n",
    "events = [\"left\", \"right\"]\n",
    "select_channels = list(range(22))\n",
    "#select_channels = [7, 9, 11]\n",
    "classes = 2\n",
    "debug = True\n",
    "\n",
    "# train model on each subject individually\n",
    "data_list = []\n",
    "for subject in results.keys():\n",
    "  data_list.append(subject)\n",
    "\n",
    "# train model on individual subject\n",
    "# data_list = []\n",
    "# data_list.append(\"A01\")\n",
    "\n",
    "for data_name in data_list:\n",
    "  counter = 0\n",
    "  accuracy = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  f1 = []\n",
    "  kappa = []\n",
    "    \n",
    "  skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    \n",
    "  while(counter < n_splits):\n",
    "    counter += 1\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for freq in epochs_filter_bank[data_name][str(counter)].keys():\n",
    "        #print(freq)\n",
    "        X_freq, Y_freq = [], []\n",
    "\n",
    "        for event in epochs_filter_bank[data_name][str(counter)][freq].keys():\n",
    "            for i in range(len(events)):\n",
    "                if event == events[i]:\n",
    "                    if len(X_freq) == 0:\n",
    "                        X_freq = epochs_filter_bank[data_name][str(counter)][freq][event].get_data()\n",
    "                        Y_freq = np.zeros(len(epochs_filter_bank[data_name][str(counter)][freq][event].get_data())) + i\n",
    "                    else:\n",
    "                        X_freq = np.append(X_freq, epochs_filter_bank[data_name][str(counter)][freq][event].get_data(), axis=0)\n",
    "                        Y_freq = np.append(Y_freq, np.zeros(len(epochs_filter_bank[data_name][str(counter)][freq][event].get_data())) + i, axis=0)\n",
    "        if len(X) == 0:\n",
    "            X = np.expand_dims(X_freq, axis=0)\n",
    "            Y = Y_freq\n",
    "        else:\n",
    "            X = np.append(X, np.expand_dims(X_freq, axis=0), axis=0)\n",
    "    if debug:\n",
    "        print(\"shape of X and Y: \" + str(X.shape) + \" \" + str(Y.shape))\n",
    "    split_index = list(skf.split(X[0], Y))\n",
    "    train_index, test_index = split_index[counter-1]\n",
    "    \n",
    "    X_train_csp, Y_train_csp = [], []\n",
    "    X_test_csp, Y_test_csp = [], []\n",
    "    for i in range(len(X)):\n",
    "        #print(\"current freq: \", i)\n",
    "        X_train, X_test = X[i][train_index], X[i][test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        csp_pipeline = make_pipeline(\n",
    "            CSP(n_components=4, reg='diagonal_fixed', log=True, norm_trace=False, rank='full')\n",
    "        )\n",
    "            \n",
    "        X_train = csp_pipeline.fit_transform(X_train, Y_train)\n",
    "        X_test = csp_pipeline.transform(X_test)\n",
    "            \n",
    "        if len(X_train_csp) == 0:\n",
    "            X_train_csp = X_train\n",
    "            X_test_csp = X_test\n",
    "            Y_train_csp = Y_train\n",
    "            Y_test_csp = Y_test\n",
    "        else:\n",
    "            X_train_csp = np.append(X_train_csp, X_train, axis=1)\n",
    "            X_test_csp = np.append(X_test_csp, X_test, axis=1)\n",
    "                \n",
    "        X_train = np.array(X_train_csp, copy=True)\n",
    "        X_test = np.array(X_test_csp, copy=True)\n",
    "    \n",
    "    \n",
    "    if debug:\n",
    "      print(data_name)\n",
    "      print(\"shape of X_train and Y_train: \" + str(X_train.shape) + \" \" + str(Y_train.shape))\n",
    "      print(\"shape of X_test and Y_test: \" + str(X_test.shape) + \" \" + str(Y_test.shape))\n",
    "\n",
    "    #model = make_pipeline(StandardScaler(), SVC(gamma='auto', random_state=0))\n",
    "    #model = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "    model = make_pipeline(StandardScaler(), AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "    #model = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis())\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_hat = model.predict(X_test)\n",
    "    \n",
    "    accuracy.append(accuracy_score(Y_test, Y_hat))\n",
    "    precision.append(precision_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    recall.append(recall_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    f1.append(f1_score(Y_test, Y_hat, average=\"macro\"))\n",
    "    kappa.append(cohen_kappa_score(Y_test, Y_hat))\n",
    "\n",
    "  if debug:\n",
    "    print(\"accuracy: \" + str(np.mean(accuracy)))\n",
    "    print(\"precision: \" + str(np.mean(precision)))\n",
    "    print(\"recall: \" + str(np.mean(recall)))\n",
    "    print(\"f1: \" + str(np.mean(f1)))\n",
    "    print(\"kappa: \" + str(np.mean(kappa)))\n",
    "\n",
    "  results[data_name][\"accuracy\"] = accuracy\n",
    "  results[data_name][\"precision\"] = precision\n",
    "  results[data_name][\"recall\"] = recall\n",
    "  results[data_name][\"f1\"] = f1\n",
    "  results[data_name][\"kappa\"] = kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24298eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "result_accuracy = []\n",
    "result_precision = []\n",
    "result_recall = []\n",
    "result_f1 = []\n",
    "result_kappa = []\n",
    "for key, value in results.items():\n",
    "  result_accuracy += [value[\"accuracy\"]]\n",
    "  result_precision += [value[\"precision\"]]\n",
    "  result_recall += [value[\"recall\"]]\n",
    "  result_f1 += [value[\"f1\"]]\n",
    "  result_kappa += [value[\"kappa\"]]\n",
    "\n",
    "print(\"accuracy: (mean) \" + str(np.mean(result_accuracy)) + \" (std) \" + str(np.std(result_accuracy)))\n",
    "print(\"precision: (mean) \" + str(np.mean(result_precision)) + \" (std) \" + str(np.std(result_precision)))\n",
    "print(\"recall: (mean) \" + str(np.mean(result_recall)) + \" (std) \" + str(np.std(result_recall)))\n",
    "print(\"f1: (mean) \" + str(np.mean(result_f1)) + \" (std) \" + str(np.std(result_f1)))\n",
    "print(\"kappa: (mean) \" + str(np.mean(result_kappa)) + \" (std) \" + str(np.std(result_kappa)))\n",
    "\n",
    "#np.savez(os.path.join(DIRECTORY_PATH, \"adaboost_boosted.npz\"), results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c63204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_boosted = np.load(os.path.join(DIRECTORY_PATH, \"svm_boosted.npz\"), allow_pickle=True)\n",
    "# print(svm_boosted[\"results\"].item()[\"A01\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349f209",
   "metadata": {},
   "source": [
    "Refer to raw eeg -> source -> bandpass filter -> csp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16eafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time computation\n",
    "events = [\"left\", \"right\"]\n",
    "use_csp = False\n",
    "select_channels = list(range(22))\n",
    "#select_channels = [7, 9, 11]\n",
    "debug = False\n",
    "random_select = False\n",
    "use_mask = True\n",
    "kqv = False\n",
    "model_name = \"eegnet\"\n",
    "warm_up = 10 # initializing memory allocators, and GPU-related initializations \n",
    "\n",
    "data_list = []\n",
    "data_list.append(\"A01\")\n",
    "\n",
    "information = get_inverse_and_forward_information(epochs)\n",
    "print(information)\n",
    "print(information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]].shape)\n",
    "print(information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]].shape)\n",
    "forward_matrix = np.concatenate((information[\"leadfield\"][:, :len(information[\"left_vertices\"])][:, information[\"my_left_points\"]], \n",
    "                          information[\"leadfield\"][:, -len(information[\"right_vertices\"]):][:, information[\"my_right_points\"]]),\n",
    "                          axis = 1)\n",
    "print(forward_matrix.shape)\n",
    "inverse_operator = information[\"inverse_operator\"]\n",
    "my_left_points = information[\"my_left_points\"]\n",
    "my_right_points = information[\"my_right_points\"]\n",
    "info = epochs[\"A01\"][\"left\"].info\n",
    "\n",
    "for data_name in data_list:\n",
    "    \n",
    "  X, Y = [], []\n",
    "  for event in epochs[data_name].keys():\n",
    "    for i in range(len(events)):\n",
    "      if event == events[i]:\n",
    "        if len(X) == 0:\n",
    "          X = epochs[data_name][event].get_data()\n",
    "          Y = np.zeros(len(epochs[data_name][event].get_data())) + i\n",
    "        else:\n",
    "          X = np.append(X, epochs[data_name][event].get_data(), axis=0)\n",
    "          Y = np.append(Y, np.zeros(len(epochs[data_name][event].get_data())) + i, axis=0)\n",
    "\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y) \n",
    "  X_test = np.expand_dims(X[0], axis=0)\n",
    "  Y_test = np.expand_dims(Y[0], axis=0)\n",
    "    \n",
    "  # load pretrained model\n",
    "  model = AutoSelectData(select_channels, forward_matrix, random_select, use_mask, kqv, model_name)\n",
    "  #weight_path = \"D:/forward and inverse results (new)/motor/ml motor/\"+ model_name + \"/models/\" + data_name + \"/\"\n",
    "  weight_path = \"D:/forward and inverse results (new)/motor/ml motor/eegnet 22 channels/models/\" + data_name + \"/\"\n",
    "  counter = 0\n",
    "\n",
    "  for weight_file in os.listdir(weight_path):\n",
    "      if weight_file.split(\"_\")[0] == str(counter):\n",
    "          break\n",
    "  load_weights_file = os.path.join(weight_path, weight_file) + \"/\"\n",
    "  model.load_weights(load_weights_file)\n",
    "  #model.build((None, X_train.shape[1], X_train.shape[2]))\n",
    "  #print(model.summary())\n",
    "  model.trainable = False\n",
    "\n",
    "  #ml_model = make_pipeline(StandardScaler(), SVC(gamma='auto', random_state=0))\n",
    "  #ml_model = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "  ml_model = make_pipeline(StandardScaler(), AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "  #ml_model = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis())\n",
    "\n",
    "  # dummy training\n",
    "  if use_csp:\n",
    "    ml_model.fit(X[:, select_channels,:].reshape(X.shape[0], -1)[:, :36], Y)\n",
    "  else:\n",
    "    ml_model.fit(X[:, select_channels,:].reshape(X.shape[0], -1), Y)\n",
    "  \n",
    "  csp_pipeline = make_pipeline(\n",
    "      CSP(n_components=4, reg='diagonal_fixed', log=True, norm_trace=False, rank='full')\n",
    "  )\n",
    "  csp_pipeline.fit(X, Y)\n",
    "\n",
    "  for i in range(warm_up):\n",
    "    if i == warm_up-1:\n",
    "        start = time.time()\n",
    "    \n",
    "    X_epochs = mne.EpochsArray(X_test, info, verbose=False)\n",
    "    method = \"sLORETA\"\n",
    "    snr = 3.\n",
    "    lambda2 = 1. / snr ** 2\n",
    "    stc_test = apply_inverse_epochs(X_epochs, inverse_operator, lambda2,\n",
    "                                  method=method, pick_ori=\"normal\", verbose=debug)\n",
    "    # slice source activity data\n",
    "    left_hemi_data = []\n",
    "    right_hemi_data = []\n",
    "    for source in stc_test:\n",
    "        left_hemi_data.append(source.data[:len(source.vertices[0])][my_left_points])\n",
    "        right_hemi_data.append(source.data[-len(source.vertices[1]):][my_right_points])\n",
    "    left_hemi_data = np.array(left_hemi_data)\n",
    "    right_hemi_data = np.array(right_hemi_data)\n",
    "    \n",
    "    X_time = np.append(left_hemi_data, right_hemi_data, axis=1)\n",
    "    X_time = model(X_time, training=False).numpy()\n",
    "    \n",
    "    if not use_csp:\n",
    "        X_time = X_time.reshape(X_time.shape[0], -1)\n",
    "    \n",
    "    if use_csp:\n",
    "        lowcut = 4\n",
    "        highcut = 40\n",
    "        interval = 4\n",
    "        filter_bank = list(np.arange(lowcut, highcut+interval, step=interval))\n",
    "        info = epochs[data_name][\"left\"].info\n",
    "        \n",
    "        X_time = mne.EpochsArray(X_time, info, verbose=False)\n",
    "        \n",
    "        X_time_csp = []\n",
    "        \n",
    "        for freq_i in range(len(filter_bank)-1):\n",
    "            low = filter_bank[freq_i]\n",
    "            high = filter_bank[freq_i+1]\n",
    "            #print(\"current frequency: \", low, \" to \", high)\n",
    "\n",
    "            # filter frequency\n",
    "            iir_params = dict(order=5, ftype='butter')\n",
    "            X_time_filter = X_time.copy()\n",
    "            X_time_filter.filter(low, high, method=\"iir\", iir_params=iir_params)\n",
    "            X_time_csp_feature = csp_pipeline.transform(X_time_filter.get_data())\n",
    "            \n",
    "            if len(X_time_csp) == 0:\n",
    "                X_time_csp = X_time_csp_feature\n",
    "            else:\n",
    "                X_time_csp = np.append(X_time_csp, X_time_csp_feature, axis=1)\n",
    "        \n",
    "        X_time = np.array(X_time_csp, copy=True)\n",
    "    \n",
    "    Y_hat = ml_model.predict(X_time)\n",
    "\n",
    "    if i == warm_up-1:\n",
    "        end = time.time()\n",
    "        print(\"time used: \", (end - start)*1000, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a09bf",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(DIRECTORY_PATH, \"ml motor results (22 channels)\")\n",
    "\n",
    "for ml_result in os.listdir(results_path):\n",
    "    print(ml_result.split(\".\")[0])\n",
    "    result = np.load(os.path.join(results_path, ml_result), allow_pickle=True)\n",
    "    subject_mean_accuracy = []\n",
    "    for subject, metrics in result[\"results\"].item().items():\n",
    "        print(subject)\n",
    "        print(np.mean(metrics[\"accuracy\"])*100)\n",
    "        subject_mean_accuracy.append(np.mean(metrics[\"accuracy\"])*100)\n",
    "    print(\"average: \", np.mean(subject_mean_accuracy), \"std: \", np.std(subject_mean_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ca3c7",
   "metadata": {},
   "source": [
    "### Statistical test (subject-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2af59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(DIRECTORY_PATH, \"ml motor results csp (22 channels)\")\n",
    "\n",
    "for ml_result in os.listdir(results_path):\n",
    "    if \"boosted\" in ml_result.split(\".\")[0]:\n",
    "        continue\n",
    "    print(ml_result.split(\".\")[0])\n",
    "    result_original = np.load(os.path.join(results_path, ml_result), allow_pickle=True)\n",
    "    result_boosted = np.load(os.path.join(results_path, ml_result.split(\".\")[0]+\"_boosted.npz\"), allow_pickle=True)\n",
    "    original_accuracy = {}\n",
    "    boosted_accuracy = {}\n",
    "    for subject, metrics in result_original[\"results\"].item().items():\n",
    "        original_accuracy[subject] = np.array(metrics[\"accuracy\"])\n",
    "    for subject, metrics in result_boosted[\"results\"].item().items():\n",
    "        boosted_accuracy[subject] = np.array(metrics[\"accuracy\"])\n",
    "    count = 0\n",
    "    for subject in original_accuracy.keys():\n",
    "        accuracy_mean_std[subject] = {}\n",
    "        mean = np.mean(boosted_accuracy[subject] - original_accuracy[subject])\n",
    "        std = np.std(boosted_accuracy[subject] - original_accuracy[subject])\n",
    "\n",
    "        n = 5\n",
    "        t_test = mean / np.sqrt((std**2)/n)\n",
    "        df = n - 1\n",
    "        if t_test > 0:\n",
    "            p_value = stats.t.sf(np.abs(t_test), df)\n",
    "            if p_value < 0.05:\n",
    "                count += 1\n",
    "                print(subject, mean)\n",
    "        #print(t_test, df, p_value)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def948bb",
   "metadata": {},
   "source": [
    "### Statistical test (method-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(DIRECTORY_PATH, \"ml motor results csp (22 channels)\")\n",
    "\n",
    "for ml_result in os.listdir(results_path):\n",
    "    if \"boosted\" in ml_result.split(\".\")[0]:\n",
    "        continue\n",
    "    print(ml_result.split(\".\")[0])\n",
    "    result_original = np.load(os.path.join(results_path, ml_result), allow_pickle=True)\n",
    "    result_boosted = np.load(os.path.join(results_path, ml_result.split(\".\")[0]+\"_boosted.npz\"), allow_pickle=True)\n",
    "    original_accuracy = {}\n",
    "    boosted_accuracy = {}\n",
    "    accuracy_mean = []\n",
    "    for subject, metrics in result_original[\"results\"].item().items():\n",
    "        original_accuracy[subject] = np.mean(np.array(metrics[\"accuracy\"]))\n",
    "    for subject, metrics in result_boosted[\"results\"].item().items():\n",
    "        boosted_accuracy[subject] = np.mean(np.array(metrics[\"accuracy\"]))\n",
    "    for subject in original_accuracy.keys():\n",
    "        accuracy_mean.append(boosted_accuracy[subject] - original_accuracy[subject])\n",
    "\n",
    "    mean = np.mean(accuracy_mean)\n",
    "    std = np.std(accuracy_mean)\n",
    "    n = 9\n",
    "    t_test = mean / np.sqrt((std**2)/n)\n",
    "    df = n - 1\n",
    "    if t_test > 0:\n",
    "        p_value = stats.t.sf(np.abs(t_test), df)\n",
    "        if p_value < 0.05:\n",
    "            print(mean)\n",
    "    #print(t_test, df, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490130d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
